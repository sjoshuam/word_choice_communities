{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee32860-1ea4-4daa-98fc-291749307fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52c8a0-b7be-4019-878a-4e9ac1ff4895",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b339-098d-4b27-89a0-8c1167b5ac8a",
   "metadata": {},
   "source": [
    "#### HEAD 01 - toggle user settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18e3fdb-3cd1-47ca-94c6-0544e31fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine whether to cache data from some time consuming tasks\n",
    "## selecting True for any option increases run time and memory usage\n",
    "## selecting True for all options fully executes the code from start-to-finish\n",
    "settings = {\n",
    "    'num_parallel_cores': 13,\n",
    "    'full_data_mode': True, ## model all data, not just a sample for code dev\n",
    "    \n",
    "    'collect_data': False, ## toggles twitter api pulls in PULL01-03\n",
    "    \n",
    "    'rebuild_word_data': False, ## rebuild vs cache load for word_data MUNG02\n",
    "    'rebuild_tweet_words': False, ## rebuild vs cache load tweet_words MUNG03\n",
    "    'rebuild_user_token': False, ## rebuild v cache load user_token_tally MUNG04\n",
    "    \n",
    "    'rebuild_logistic_hparams': False, ## rebuild v cache ml training MODE04\n",
    "    'rebuild_bayes_hparams': False, ## rebuild v cache ml training MODE05\n",
    "    'rebuild_forest_hparams': False, ## rebuild v cache ml training MODE06\n",
    "    'rebuild_adaboost_hparams': False, ## rebuild v cache ml training MODE07\n",
    "    \n",
    "    'rebuild_important_features': False, ## rebuild/cache var importance EVAL04\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16158d71-339e-4026-93e9-b270cc45859d",
   "metadata": {},
   "source": [
    "#### HEAD02 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0914f0c2-3000-4136-94e5-630181f3e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from time import sleep\n",
    "from os.path import exists\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18441e2b-97b6-4243-a468-0de4c7e31258",
   "metadata": {},
   "source": [
    "#### HEAD03 - load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f766755-c82c-49a1-ae9a-05155a84f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s8/opt/anaconda3/envs/py310/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## read in roster of handles\n",
    "user_data = pd.read_excel(\"A_Input/twitter_handles.xlsx\")\n",
    "\n",
    "## drop all but a handful of cases if in test mode\n",
    "if not settings['full_data_mode']:\n",
    "    user_data = user_data.sample(125, random_state = 5542)\n",
    "\n",
    "## read in twitter credentials; initialize api connection+\n",
    "twitter_credentials = pd.read_csv('../api_keys/twitter.csv').set_index('item')\n",
    "twitter_credentials = tweepy.OAuth1UserHandler(\n",
    "    consumer_key = twitter_credentials.loc['API Key', 'string'],\n",
    "    consumer_secret = twitter_credentials.loc['API Key Secret', 'string'],\n",
    "    access_token = twitter_credentials.loc['Access Token', 'string'],\n",
    "   access_token_secret = twitter_credentials.loc['Access Token Secret', 'string']\n",
    "    )\n",
    "api = tweepy.API(twitter_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eace4-543f-4ff2-81cc-0eb3cc114c18",
   "metadata": {},
   "source": [
    "#### HEAD04 - create build or cache decision function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96445ddf-b3b5-4672-95d7-cc48563f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build switching function to execute code or cache results\n",
    "def build_or_cache_csv(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        x.to_csv(address, index = False)\n",
    "        return x\n",
    "    else:\n",
    "        return pd.read_csv(address)\n",
    "    \n",
    "def build_or_cache_pickle(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        conn = open(address, 'wb')\n",
    "        pickle.dump(x, conn)\n",
    "        conn.close()\n",
    "        return x\n",
    "    else:\n",
    "        conn = open(address, 'rb')\n",
    "        x = pickle.load(conn)\n",
    "        conn.close()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fd8c1f-1a4e-4dca-a2de-100adffbceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed49a6b-8b72-4edd-84b8-0f815986fc01",
   "metadata": {},
   "source": [
    "## HAND â€“ Gather Twitter handles for test accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67daeb1-100d-4fc1-9c20-f28ed2c1ec0a",
   "metadata": {},
   "source": [
    "#### HAND01 - extract handles from roster URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdda14bb-55b3-41d9-b735-b674367eee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract handles from roster urls\n",
    "user_data['handle'] = user_data.url.str.replace('https://twitter.com/', '',\n",
    "            regex = False).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db8ac6c-b02c-454a-ae37-cf1cc444b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e64b95-7e26-4789-b736-089706334fad",
   "metadata": {},
   "source": [
    "## PULL - Pull Twitter data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9531973-8911-4beb-a48b-1c7989ae2428",
   "metadata": {},
   "source": [
    "#### PULL01 - query API for each roster handle's user_timeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1200e6ca-cf86-412a-bdf2-da760105698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tweet data from api object\n",
    "def refine_tweet_data(x):\n",
    "    tweet_data = list()\n",
    "    for i in range(0, len(x)):\n",
    "        tweet_data.append({\n",
    "            'tweet_id': x[i].id, \n",
    "            'created_at': x[i].created_at, \n",
    "            'lang': x[i].lang,\n",
    "            'full_text': x[i].full_text,\n",
    "            'screen_name': x[i].author.screen_name,\n",
    "            'verified' : x[i].author.verified\n",
    "        })\n",
    "    return pd.DataFrame(tweet_data)\n",
    "\n",
    "## define function to pull user tweet data and apply function to extract tweet data\n",
    "def pull_tweet_data(handles = user_data.handle, a = api):\n",
    "    tweet_data = list()\n",
    "    for i in handles:\n",
    "        try:\n",
    "            user_tweets = a.user_timeline(\n",
    "                screen_name = i, count = 200, tweet_mode = 'extended', \n",
    "                exclude_replies = True, include_rts = False)\n",
    "            tweet_data.append(refine_tweet_data(user_tweets))\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_data = pd.concat(tweet_data)\n",
    "    tweet_data['screen_name'] = tweet_data['screen_name'].str.lower()\n",
    "    return tweet_data\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    tweet_data = pull_tweet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7301b7-3283-45c5-8943-1444d0e8cf27",
   "metadata": {},
   "source": [
    "#### PULL02 - tabulate tweet statistics, divide users into train/tune/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f26d7ac-312e-4445-a22a-0f8bada0d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combile tweet count and verification status at the user level\n",
    "def enhance_user_data(td, ud = user_data):\n",
    "    \n",
    "    def np_size(x): return x.size\n",
    "    \n",
    "    ## calculate tweet summary statistics\n",
    "    td_original = tweet_data\n",
    "    td = td.copy()\n",
    "    verified = td.groupby('screen_name').mean()\n",
    "    tweets = td['screen_name'].value_counts()\n",
    "    td = pd.concat([verified, tweets], axis = 1).reset_index()\n",
    "    td.columns = ['handle', 'verified', 'tweets']\n",
    "    \n",
    "    ## merge statistics into the user_data object\n",
    "    ud = pd.merge(ud, td, on = 'handle', how = 'left')\n",
    "    td_original = td_original.drop(['verified'], axis = 1)\n",
    "    ud = ud.drop(['url'], axis = 1).reset_index(drop = True)\n",
    "    ud = ud.fillna({'tweets': 0}).astype({'tweets': int})\n",
    "    \n",
    "    ## divide users into train, and test subsets\n",
    "    ml_set = pd.Series(['train', 'test'], name = 'ml_set').sample(\n",
    "                n = ud.shape[0], replace = True, weights = [0.8, 0.20],\n",
    "                random_state = 2006)\n",
    "    ud['ml_set'] = ml_set.values\n",
    "    ud.loc[ud.tweets == 0, 'ml_set'] = 'exclude'\n",
    "    \n",
    "    return ud, td_original\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    user_data, tweet_data = enhance_user_data(\n",
    "        tweet_data[['screen_name', 'verified']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6e488-00b0-4257-a07b-598824c2aa18",
   "metadata": {},
   "source": [
    "#### PULL03 - save datasets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb27d14-f5e0-40ef-beac-2f6d369a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save user/tweet datasets to disk as csvs\n",
    "if settings['collect_data']:\n",
    "    user_data.to_csv('B_Process/user_data.csv', index = False)\n",
    "    tweet_data.to_csv('B_Process/tweet_data.csv', index = False)\n",
    "else:\n",
    "    user_data = pd.read_csv('B_Process/user_data.csv')\n",
    "    tweet_data = pd.read_csv('B_Process/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab22ae2d-8e04-4fa8-91de-70498694d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1152f-f55d-4758-a9d1-78e30499f55f",
   "metadata": {},
   "source": [
    "## MUNG - Process Twitter data to model-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fdb93-7acf-43e0-9ae9-3ae2eb6a749e",
   "metadata": {},
   "source": [
    "#### MUNG01 - parse tweet text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3362904b-047d-44ef-9497-12af5a5421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize, remove capitalization, and remove duplicate tokens\n",
    "def nlp_tokenize_tweet(x):\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    x = list(set(x))\n",
    "    return x\n",
    "\n",
    "## execute code\n",
    "tweet_data['tokens'] = tweet_data.full_text.apply(nlp_tokenize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422810c-53d2-4b5e-b9e8-68457e82756b",
   "metadata": {},
   "source": [
    "#### MUNG02 - create word/token level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f7a6af-6786-469b-8ec6-f07fb6d31709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/rmkl6dh17jl6zqhzl0ky4b_w0000gp/T/ipykernel_32298/3614223863.py:8: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(address)\n"
     ]
    }
   ],
   "source": [
    "## create word/token level dataset and identify valid word tokens\n",
    "def make_word_data(td = tweet_data):\n",
    "\n",
    "    ## flatten token lists and count occurances\n",
    "    word_data = list()\n",
    "    for i in td.tokens:\n",
    "        word_data += i\n",
    "    word_data = pd.Series(word_data, name = 'count').value_counts()\n",
    "    word_data = word_data.sort_values(ascending = False)\n",
    "    word_data = pd.DataFrame(word_data)\n",
    "    \n",
    "    ## determine which tokens occur often enough to warrant inclusion\n",
    "    word_data['valid'] = word_data['count'] > max(\n",
    "        word_data['count'].quantile(0.2), 3)\n",
    "    word_data['word'] = word_data.index\n",
    "    \n",
    "    ## determine part of speech for eligible tokens\n",
    "    speech_part = word_data['word'].loc[word_data['valid']].values\n",
    "    speech_part = pos_tag(speech_part)\n",
    "    speech_part = [i[1][0].lower() for i in speech_part]\n",
    "    word_data['pos'] = '.'\n",
    "    word_data.loc[word_data['valid'], 'pos'] = speech_part\n",
    "    \n",
    "    ## lemmatize\n",
    "    WNL = WordNetLemmatizer()\n",
    "    word_data['token'] = None\n",
    "    for i in word_data.word:\n",
    "        if not word_data.loc[i, 'valid']: \n",
    "            break\n",
    "        if word_data.loc[i, 'pos'] in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            try:\n",
    "                word_data.loc[i, 'token'] = WNL.lemmatize(\n",
    "                    word_data.loc[i, 'word'],\n",
    "                    pos = word_data.loc[i, 'pos']\n",
    "                )\n",
    "            except:\n",
    "                word_data.loc[i, 'token'] = word_data.loc[i, 'word']\n",
    "        else:\n",
    "            word_data.loc[i, 'valid'] = False\n",
    "        \n",
    "    return word_data.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "word_data = build_or_cache_csv(\n",
    "    address = 'B_Process/word_data.csv',\n",
    "    function = make_word_data,\n",
    "    build_bool = settings['rebuild_word_data']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419c4d1-dada-48f5-9bcd-252137c407b5",
   "metadata": {},
   "source": [
    "#### MUNG03 - generate a tokens x tweets link database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247437d5-61e4-4143-93b2-cc66edf8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tweet_token_data(td = tweet_data, wd = word_data):\n",
    "    \n",
    "    wd = wd.set_index('word')\n",
    "    \n",
    "    ## replicate tweet ids\n",
    "    n = td.tokens.apply(len).values\n",
    "    tweet_tokens = pd.Series(np.repeat(td.tweet_id.values, n), name = \"tweet_id\")\n",
    "    tweet_tokens = pd.DataFrame(tweet_tokens)\n",
    "    \n",
    "    ## allocate words to the new dataset\n",
    "    words = list()\n",
    "    for i in td.tokens:\n",
    "        words += i\n",
    "    tweet_tokens['words'] = words\n",
    "    \n",
    "    ## convert words to tokens\n",
    "    tweet_tokens['tokens'] = wd.loc[\n",
    "        tweet_tokens.words.values, 'token'].values\n",
    "    tweet_tokens = tweet_tokens.dropna()\n",
    "\n",
    "    return tweet_tokens.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "tweet_words = build_or_cache_csv(\n",
    "    address = 'B_Process/tweet_words.csv',\n",
    "    function = make_tweet_token_data,\n",
    "    build_bool = settings['rebuild_tweet_words']\n",
    "    )\n",
    "tweet_data = tweet_data.drop('tokens', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5cef3-bc88-4f27-9a5e-f76ce06d149c",
   "metadata": {},
   "source": [
    "#### MUNG04 - generate a tokens x users count; drop tokens with only one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bc82f94-94cf-4030-a5d9-b3f5e5bbd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_token_matrix(td = tweet_data, tw = tweet_words, ud = user_data):\n",
    "    \n",
    "    ## count of number of times each user wrote each token\n",
    "    tw = tw.merge(right = td[['screen_name', 'tweet_id']],\n",
    "                  how = 'left', on = 'tweet_id')\n",
    "    tw = tw.drop(['tweet_id', 'words'], axis = 1).groupby('screen_name')\n",
    "    tw = tw.value_counts()\n",
    "    tw.name = 'count'\n",
    "    tw = tw.reset_index().set_index('screen_name')\n",
    "    tw = tw.pivot(columns = 'tokens').fillna(0).astype(int)\n",
    "    tw = tw.droplevel(axis = 1, level = 0)\n",
    "    \n",
    "    ## remove tokens that fewer than 3 or more than 80 percent of users use\n",
    "    valid_usage = (tw > 0).astype(int).sum().values\n",
    "    valid_usage = (valid_usage > 2\n",
    "                  ) & (valid_usage < int(tw.shape[0] * 0.8))\n",
    "    tw = tw.loc[:, valid_usage]\n",
    "    \n",
    "    ## standardize matrix as words per 1,000 tweets\n",
    "    denom = pd.DataFrame({'handle': tw.index}).merge(\n",
    "        ud[['handle', 'tweets']],\n",
    "        how = 'left', on = 'handle'\n",
    "        ).set_index('handle').squeeze().fillna(1)\n",
    "    denom.loc[denom < 1] = 1\n",
    "    tw = (tw.divide(denom, axis = 0) * 1000).astype(int)\n",
    "    tw = tw.reset_index().rename({'level_0':'screen_name'}, axis = 1)\n",
    "    \n",
    "    return tw\n",
    "\n",
    "\n",
    "## execute code\n",
    "user_token_matrix = build_or_cache_csv(\n",
    "    address = 'B_Process/user_token_matrix.csv',\n",
    "    function = make_user_token_matrix,\n",
    "    build_bool = settings['rebuild_user_token']\n",
    "    ).set_index('screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2591e88-3b00-4e6a-a199-bb129120db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0ad36-2fff-4c0f-8b41-a0dba7c3d20a",
   "metadata": {},
   "source": [
    "## HYPE â€“ Train models and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4089e-17c3-4502-b726-889d0abb5668",
   "metadata": {},
   "source": [
    "#### HYPE01 - build x/y train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0bc61b5-b3fd-407e-b340-0f6e9b504f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unpack train and test datasets\n",
    "def split_xy_data(ml_cat):\n",
    "    i = user_data.loc[user_data.ml_set == ml_cat, 'handle'].values\n",
    "    \n",
    "    x = user_token_matrix.loc[i, :]\n",
    "    \n",
    "    y1 = user_data.set_index('handle').loc[i, 'group'] == 'USA House'\n",
    "    y1 = y1.astype(int)\n",
    "    \n",
    "    y2 = user_data.set_index('handle').loc[i, 'party'] == 'Republican'\n",
    "    y2 = y2.astype(int)\n",
    "    \n",
    "    return x, y1, y2\n",
    "\n",
    "## execute code\n",
    "train_x, train_y1, train_y2 = split_xy_data('train')\n",
    "test_x, test_y1, test_y2    = split_xy_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "861ddd9a-b469-45ae-a00c-e169aeae04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete objects to clear up memory in prep for modeling\n",
    "del tweet_words, tweet_data, user_token_matrix, word_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ffc07-371d-4d92-a7d5-909291aca128",
   "metadata": {},
   "source": [
    "#### HYPE02 - build pca versions of x dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f0598e9-3004-4e91-a861-81871cfceb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit pca model in order to simplify and enhance feature matrix\n",
    "model_pca = PCA().fit(train_x)\n",
    "\n",
    "## generate pca transformations of all feature matrices\n",
    "train_x_pca = model_pca.transform(train_x)\n",
    "test_x_pca  = model_pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a98ada54-302b-423a-8707-083129de235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## time trial pca using the GridSearchCV interface (for consistency)\n",
    "pca_time_cost = GridSearchCV(\n",
    "    estimator = PCA(),\n",
    "    param_grid = {'svd_solver': ['auto' for i in range(0, 5)]},\n",
    "    n_jobs = settings['num_parallel_cores'],\n",
    "    scoring = lambda a,b: 1,\n",
    "    refit = False\n",
    "    ).fit(train_x)\n",
    "pca_time_cost = pca_time_cost.cv_results_['mean_fit_time'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350d0f8-56f0-4784-add3-2e2f4bd161c6",
   "metadata": {},
   "source": [
    "#### HYPE03 - formulate generic function for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b888b789-65dc-4d69-8a55-d44d163da978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "## find best hyperparameters\n",
    "def find_hparams():\n",
    "    \n",
    "    model1 = clone(the_model)\n",
    "    model2 = clone(the_model)\n",
    "    model3 = clone(the_model)\n",
    "    model4 = clone(the_model)\n",
    "    model5 = clone(the_model)\n",
    "    model6 = clone(the_model)\n",
    "    \n",
    "    only_s2 = train_y1.astype(bool).values\n",
    "    \n",
    "    model1 = model1.fit(X = train_x, y = train_y1)\n",
    "    model2 = model2.fit(X = train_x.loc[only_s2, :], y = train_y2.loc[only_s2])\n",
    "    model3 = model3.fit(X = train_x_pca, y = train_y1)\n",
    "    model4 = model4.fit(X = train_x_pca[only_s2, :], y = train_y2.loc[only_s2])\n",
    "    model5 = model5.fit(X = train_x, y = train_y2)\n",
    "    model6 = model6.fit(X = train_x_pca, y = train_y2)\n",
    "    \n",
    "    return {'feature_stage1': model1, 'feature_stage2': model2,\n",
    "            'pca_stage1': model3, 'pca_stage2': model4,\n",
    "            'feature_unitary': model5, 'pca_unitary': model6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36dfe9-ab90-466f-937b-551d7c03b217",
   "metadata": {},
   "source": [
    "#### HYPE04 - find best logistic regression model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea55281c-08ee-474d-8a95-34b4e55ee067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = LogisticRegression(penalty = 'l1',\n",
    "                                   class_weight = 'balanced',\n",
    "                                   solver = 'saga',\n",
    "                                   max_iter = 2**10\n",
    "                                  ),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {'C': [2. ** i for i in range(-2, 6)]}\n",
    ")\n",
    "\n",
    "## execute code\n",
    "logistic_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/logistic_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_logistic_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202253d7-59ce-4881-bfdd-50a8762076fc",
   "metadata": {},
   "source": [
    "#### HYPE05 - find best naive bayes model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "221dab0d-02fa-4753-808a-1e313182cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = GaussianNB(),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {'priors': [\n",
    "        (0.5, 0.5), \n",
    "        (1 - train_y1.mean(), train_y1.mean()),\n",
    "        (1 - train_y2.mean(), train_y2.mean()),\n",
    "    ]}\n",
    ")\n",
    "\n",
    "## execute code\n",
    "bayes_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/bayes_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_bayes_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432aa43-4dc0-42d0-8734-8d829c3fec20",
   "metadata": {},
   "source": [
    "#### HYPE06 - find best random forest model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f65fbd-06ab-4b79-af98-c02347bf84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = RandomForestClassifier(class_weight = 'balanced'),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {\n",
    "        'n_estimators': [2**i for i in range(6, 10)],\n",
    "        'min_samples_leaf': [2**i for i in range(2, 5)]\n",
    "    }\n",
    ")\n",
    "\n",
    "## execute code\n",
    "forest_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/forest_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_forest_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638e51f-6dd7-4160-be9f-60e7712b0151",
   "metadata": {},
   "source": [
    "#### HYPE07 - find best AdaBoost model parameters and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c56ff21e-a554-438f-b235-d590a907b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = AdaBoostClassifier(base_estimator = None),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {\n",
    "        'n_estimators':  [int(2**i) for i in range(6, 10)],\n",
    "        'learning_rate': [2**i for i in range(-2, 3)]\n",
    "    }\n",
    ")\n",
    "\n",
    "## execute code\n",
    "adaboost_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/adaboost_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_adaboost_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64189160-c590-484d-b783-02e4a1ba21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550df932-9da0-4c4e-bd8c-bc40e94e4edb",
   "metadata": {},
   "source": [
    "## EVAL - Analyse model performance and interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0d58d-ab83-481b-9438-1cb96d8c1337",
   "metadata": {},
   "source": [
    "#### EVAL01 - compile hyperparameter search statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baab9f46-f1ad-4356-9576-ed6ef8215b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hparam_stats(x):\n",
    "    \n",
    "    n = x.n_features_in_\n",
    "    x = x.cv_results_\n",
    "    \n",
    "    ## extract performance stats\n",
    "    perform_stats_i = ['mean_fit_time', 'mean_score_time', 'mean_test_score',\n",
    "                      'rank_test_score']\n",
    "    perform_stats = [x[i] for i in perform_stats_i]\n",
    "    perform_stats = pd.DataFrame(perform_stats).T\n",
    "    perform_stats.columns = perform_stats_i\n",
    "    \n",
    "    ## extract parameters\n",
    "    params = pd.DataFrame(x['params'])\n",
    "    perform_stats = {'stats': perform_stats, 'params': params}\n",
    "    perform_stats = pd.concat(perform_stats, axis = 1)\n",
    "    \n",
    "    ## count features\n",
    "    perform_stats[('features', 'n')] = n\n",
    "    \n",
    "    \n",
    "    return perform_stats.sort_values(('stats', 'rank_test_score'))\n",
    "\n",
    "def compile_hparam_stats(all_models = {\n",
    "    'logistic': logistic_hparams, 'bayes': bayes_hparams,\n",
    "    'forest': forest_hparams, 'adaboost': adaboost_hparams},\n",
    "    pca_time = pca_time_cost):\n",
    "    \n",
    "    ## make counter and results container\n",
    "    hparam_data = [None for i in range(0, 24)]\n",
    "    k = 0\n",
    "    \n",
    "    ## extract hyperparameter search statistics for all models\n",
    "    for i in all_models.keys():\n",
    "        for j in all_models[i].keys():\n",
    "            hparam_data[k] = extract_hparam_stats(all_models[i][j])\n",
    "            hparam_data[k][('model', 'algorithm')] = i\n",
    "            hparam_data[k][('model', 'variant')] = j\n",
    "            k += 1\n",
    "            \n",
    "    ## compile as a dataframe and return results\n",
    "    hparam_data = pd.concat(hparam_data, axis = 0).reset_index(drop = True)\n",
    "    hparam_data = hparam_data.reindex(sorted(hparam_data.columns), axis = 1)\n",
    "    ## adjust fit times to account for pca model fitting\n",
    "    i = hparam_data[('model', 'variant')].isin(['pca_unitary', 'pca_stage1'])\n",
    "    j = ('stats', 'mean_fit_time')\n",
    "    hparam_data.loc[i, j] = hparam_data.loc[i, j].values + pca_time\n",
    "    \n",
    "    \n",
    "    return hparam_data\n",
    "\n",
    "## execute code\n",
    "hparam_stats = compile_hparam_stats()\n",
    "hparam_stats.to_csv('C_Output/hparam_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e2165-76fc-4f27-ae2c-16a14c11b564",
   "metadata": {},
   "source": [
    "#### EVAL02 - compile best model performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "262d7c03-b71a-47ab-a541-f2409aaa30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_best_model_stats(stats = hparam_stats):\n",
    "    \n",
    "    ## filter existing stats from hparams to populate dataset\n",
    "    stats = stats.drop('params', axis = 1).loc[\n",
    "        stats[('stats', 'rank_test_score')] == 1, ]\n",
    "    stats = stats.drop(('stats', 'rank_test_score'), axis = 1)\n",
    "    stats[('model', 'variant')] = stats[('model', 'variant')].str.replace(\n",
    "        '[0-9]$', '', regex = True)\n",
    "    stats = stats.groupby([('model', 'algorithm'), ('model', 'variant')]).sum()\n",
    "    stats = stats.reset_index()\n",
    "    \n",
    "    ## delete test_scores for multi-stage models\n",
    "    i = stats[('model', 'variant')].isin(['feature_stage', 'pca_stage'])\n",
    "    stats.loc[i, ('stats', 'mean_test_score')] = np.nan\n",
    "    \n",
    "    ## clean up columns\n",
    "    stats[('time', 'run_time')] = stats[('stats', 'mean_fit_time')] +\\\n",
    "        stats[('stats', 'mean_score_time')]\n",
    "    del stats[('stats', 'mean_fit_time')]\n",
    "    del stats[('stats', 'mean_score_time')]\n",
    "    del stats[('stats', 'mean_test_score')]\n",
    "    \n",
    "    ## add empty columns for performance data\n",
    "    stats[('perform', 'precision')] = np.nan\n",
    "    stats[('perform', 'recall')] = np.nan\n",
    "    stats[('perform', 'f1')] = np.nan\n",
    "    stats[('perform', 'f1_train')] = np.nan\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def score_best_model_performance(all_models = {\n",
    "    'logistic': logistic_hparams, 'bayes': bayes_hparams,\n",
    "    'forest': forest_hparams, 'adaboost': adaboost_hparams}):\n",
    "    \n",
    "    ## generate container for best model statistics\n",
    "    i = [('model', 'algorithm'), ('model', 'variant')]\n",
    "    bms = construct_best_model_stats().set_index(i)\n",
    "    \n",
    "    ## iterate through models\n",
    "    for i in bms.index:\n",
    "        \n",
    "        ## select dataset\n",
    "        if i[1] in ['pca_stage', 'pca_unitary']:\n",
    "            trx = train_x_pca\n",
    "            tex = test_x_pca\n",
    "        else:\n",
    "            trx = train_x\n",
    "            tex = test_x\n",
    "            \n",
    "        ## make prediction\n",
    "        train_predict = train_y1 * 0\n",
    "        test_predict  = test_y1 * 0\n",
    "        \n",
    "        if i[1] in ['pca_stage', 'feature_stage']:\n",
    "            stage1 = all_models[i[0]][i[1] + '1'].best_estimator_\n",
    "            stage2 = all_models[i[0]][i[1] + '2'].best_estimator_\n",
    "            try:\n",
    "                train_predict = stage1.predict(trx) * stage2.predict(trx)\n",
    "                test_predict  = stage1.predict(tex) * stage2.predict(tex)\n",
    "            except: pass\n",
    "        else:\n",
    "            stage0 = all_models[i[0]][i[1]].best_estimator_\n",
    "            try:\n",
    "                train_predict = stage0.predict(trx)\n",
    "                test_predict  = stage0.predict(tex)\n",
    "            except: pass\n",
    "        \n",
    "        ## calculate statistics\n",
    "        bms.loc[i, ('perform', 'precision')] = precision_score(\n",
    "            y_true = test_y2, y_pred = test_predict)\n",
    "        bms.loc[i, ('perform', 'recall')] = recall_score(\n",
    "            y_true = test_y2, y_pred = test_predict)\n",
    "        bms.loc[i, ('perform', 'f1')] = f1_score(\n",
    "            y_true = test_y2, y_pred = test_predict)\n",
    "        bms.loc[i, ('perform', 'f1_train')] = f1_score(\n",
    "            y_true = train_y2, y_pred = train_predict)\n",
    "\n",
    "    ## return model\n",
    "    return bms.round(3)\n",
    "    \n",
    "## execute code\n",
    "best_model_stats = score_best_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccf932-9336-4754-a13e-1480e74e5172",
   "metadata": {},
   "source": [
    "#### EVAL03 - calculate comparison performance stats for random guess model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "800cf523-e288-47ec-afac-ba6ef0f5ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_guess_confuse_matrix(y1, y2):\n",
    "    \n",
    "    ## tabulate probability of a positive at random\n",
    "    pos1  = y1.mean()\n",
    "    pos2  = y2.mean()\n",
    "    \n",
    "    ## calculate confusion matrix stats\n",
    "    confusion_matrix = {\n",
    "        'neg': {\n",
    "            'neg': (1 - pos1) * (1 - pos2),\n",
    "            'pos': (1 - pos1) * pos2\n",
    "            },\n",
    "        'pos': {\n",
    "            'neg': pos1 * (1 - pos2),\n",
    "            'pos': pos1 * pos2\n",
    "\n",
    "            }\n",
    "        }\n",
    "    confusion_matrix = pd.DataFrame(confusion_matrix)\n",
    "    \n",
    "    ## check results and return\n",
    "    assert confusion_matrix.sum().sum() == 1\n",
    "    return confusion_matrix\n",
    "\n",
    "def estimate_staged_guess_confuse_matrix(y1p, y1o, y2p, y2o):\n",
    "    \n",
    "    ## generate confusion matrix for each stage\n",
    "    stage1 = estimate_guess_confuse_matrix(y1p, y1o)\n",
    "    stage2 = estimate_guess_confuse_matrix(y2p, y2o)\n",
    "    \n",
    "    ## generate confusion matrix for both stages combined\n",
    "    joint_matrix = stage1 * 0\n",
    "    joint_matrix.loc['pos', 'pos'] = stage1.loc['pos', 'pos'] *\\\n",
    "                                    stage2.loc['pos', 'pos']\n",
    "    joint_matrix.loc['pos', 'neg'] = stage1.loc['pos', 'neg'] *\\\n",
    "                                    stage2.loc['pos', 'neg'] +\\\n",
    "                                    stage1.loc['pos', 'neg'] *\\\n",
    "                                    stage2.loc['pos', 'pos'] +\\\n",
    "                                    stage1.loc['pos', 'pos'] *\\\n",
    "                                    stage2.loc['pos', 'neg']\n",
    "    joint_matrix.loc['neg', 'pos'] = stage1.loc['neg', 'pos'] *\\\n",
    "                                    stage2.loc['neg', 'pos'] +\\\n",
    "                                    stage1.loc['neg', 'pos'] *\\\n",
    "                                    stage2.loc['pos', 'pos'] +\\\n",
    "                                    stage1.loc['pos', 'pos'] *\\\n",
    "                                    stage2.loc['neg', 'pos']\n",
    "    joint_matrix.loc['neg', 'neg'] = 1 - joint_matrix.sum().sum()\n",
    "    \n",
    "    ## return result\n",
    "    return joint_matrix\n",
    "\n",
    "def score_guess_performance(cm, algo_var):\n",
    "    \n",
    "    ## create container object\n",
    "    performance = dict()\n",
    "    performance[('model', 'algorithm')] = algo_var[0]\n",
    "    performance[('model', 'variant')]   = algo_var[1]\n",
    "    performance[('features', 'n')]      = 0\n",
    "    performance[('time', 'run_time')]   = np.nan\n",
    "    \n",
    "    ## calculate precision and recall\n",
    "    performance[('perform', 'precision')] = cm.loc['pos', 'pos'] / (\n",
    "        cm.loc['pos', 'pos'] + cm.loc['pos', 'neg'])\n",
    "    performance[('perform', 'recall')] = cm.loc['pos', 'pos'] / (\n",
    "        cm.loc['pos', 'pos'] + cm.loc['neg', 'pos'])\n",
    "    \n",
    "    ## calculate f1\n",
    "    performance[('perform', 'f1')] = 2 *\\\n",
    "        performance[('perform', 'precision')] *\\\n",
    "        performance[('perform', 'recall')]/\\\n",
    "        (performance[('perform', 'precision')] +\\\n",
    "        performance[('perform', 'recall')])\n",
    "    performance[('perform', 'f1_train')] = None\n",
    "    \n",
    "    ## round statistics\n",
    "    performance[('perform', 'f1')] = performance[(\n",
    "        'perform', 'f1')].round(3)\n",
    "    performance[('perform', 'precision')] = performance[(\n",
    "        'perform', 'precision')].round(3)\n",
    "    performance[('perform', 'recall')] = performance[(\n",
    "        'perform', 'recall')].round(3)\n",
    "    \n",
    "    ## package and return object\n",
    "    performance = pd.DataFrame(pd.Series(performance)).T\n",
    "    performance = performance.set_index(\n",
    "        [('model', 'algorithm'), ('model', 'variant')])\n",
    "    \n",
    "    return performance\n",
    "\n",
    "## execute code - multistage model\n",
    "con_mat = estimate_staged_guess_confuse_matrix(\n",
    "    y1p = train_y1, y1o = test_y1, y2p = train_y2, y2o = test_y2)\n",
    "random_performance = score_guess_performance(con_mat, ('random', 'stage'))\n",
    "\n",
    "con_mat = estimate_staged_guess_confuse_matrix(\n",
    "    y1p = train_y1, y1o = train_y1, y2p = train_y2, y2o = train_y2)\n",
    "random_performance.loc[('random', 'stage'), ('perform', 'f1_train')\n",
    "    ] = score_guess_performance(con_mat, ('random', 'stage')).loc[\n",
    "    ('random', 'stage'), ('perform', 'f1')]\n",
    "\n",
    "## execute code - single stage model\n",
    "con_mat = estimate_guess_confuse_matrix(train_y2, test_y2)\n",
    "x = score_guess_performance(con_mat, ('random', 'unitary'))\n",
    "con_mat = estimate_guess_confuse_matrix(train_y2, train_y2)\n",
    "x.loc[('random', 'unitary'), ('perform', 'f1_train')\n",
    "    ] = score_guess_performance(con_mat, ('random', 'unitary')).loc[\n",
    "    ('random', 'unitary'), ('perform', 'f1')]\n",
    "\n",
    "## package\n",
    "random_performance = pd.concat([random_performance, x], axis = 0)\n",
    "best_model_stats = pd.concat([best_model_stats, random_performance])\n",
    "del con_mat, random_performance, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c68fa1bd-4cf7-42fb-9477-9e94cbbb589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write best_model_stats to disk\n",
    "with open('B_Process/best_model_stats.pickle', 'wb') as conn:\n",
    "    pickle.dump(best_model_stats, conn)\n",
    "best_model_stats.to_csv('C_Output/best_model_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26766d6-eb06-4383-829c-79beac7273b7",
   "metadata": {},
   "source": [
    "print('NOTE: do manual interpretation of best_model_stats to choose models')\n",
    "#### EVAL04 - determine variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9758df8c-4669-4c0a-bf74-6b713bf9f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminder: do manual interpretation of best_model_stats to choose models\n"
     ]
    }
   ],
   "source": [
    "print('Reminder: do manual interpretation of best_model_stats to choose models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ed2407b-a48a-49ae-8285-0dfbdbb8e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine most important feature for best models\n",
    "def find_important_features(hparam):\n",
    "    mod = hparam['feature_unitary'].best_estimator_\n",
    "    important = permutation_importance(\n",
    "        estimator = mod, X = test_x, y = test_y2, scoring = 'f1',\n",
    "        n_jobs = settings['num_parallel_cores'], n_repeats = 5)\n",
    "    return important\n",
    "\n",
    "def loop_through_models(\n",
    "    mod_list = [forest_hparams, bayes_hparams], func = find_important_features):\n",
    "    mod_results = list()\n",
    "    for i in mod_list: mod_results.append(func(i))\n",
    "    return mod_results\n",
    "\n",
    "def convert_to_dict(x):\n",
    "    y = dict()\n",
    "    y['forest'] = x[0]\n",
    "    y['bayes'] = x[1]\n",
    "    return y\n",
    "\n",
    "## execute code\n",
    "important_features = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/important_features.pickle',\n",
    "    function = loop_through_models,\n",
    "    build_bool = settings['rebuild_important_features']\n",
    "    )\n",
    "important_features = convert_to_dict(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bbac191-730b-416b-a097-2ffd6b512327",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compile importance results\n",
    "\n",
    "def analyze_importance(important = important_features):\n",
    "    \n",
    "    ## extract importance scores\n",
    "    for i in important.keys(): important[i] = important[i].importances_mean\n",
    "    important = pd.DataFrame(important)\n",
    "    \n",
    "    ## filter and label\n",
    "    important.index = train_x.columns\n",
    "    important = important.loc[important.max(axis = 1) > 0, :]\n",
    "    important['mean'] = important.mean(axis = 1)\n",
    "    important = important.sort_values('mean', ascending = False).round(3)\n",
    "    \n",
    "    ## tabulate differences between groups in usage of words\n",
    "    all_groups = ((1 - train_y1) * 1)\n",
    "    all_groups = all_groups + (train_y2 * 3)\n",
    "    all_groups = all_groups + (train_y1 * (1 - train_y2) * 2)\n",
    "    x = train_x.copy()\n",
    "    x['group'] = all_groups\n",
    "    x = x.groupby('group').mean().round().astype(int)\n",
    "    x = x.T.loc[important.index, :]\n",
    "    for i in x.columns: x[i] = x[i] == x.max(axis = 1).values\n",
    "    x = x.astype(int)\n",
    "    important = pd.concat([important, x], axis = 1)\n",
    "    important = important.sort_values([1, 2, 3], ascending = False)\n",
    "    \n",
    "    return important\n",
    "\n",
    "## execute code\n",
    "word_importance = analyze_importance()\n",
    "word_importance.to_csv('C_Output/word_importance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866f8ad-2048-4b91-8245-f1df2c76ae6d",
   "metadata": {},
   "source": [
    "#### EVAL07 - extract softmax scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8adfd903-8c89-4032-a007-7ae5fb7ab234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>party</th>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Conservative</th>\n",
       "      <th>Alberta</th>\n",
       "      <td>0.160214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>British Columbia</th>\n",
       "      <td>0.167948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manitoba</th>\n",
       "      <td>0.217902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New Brunswick</th>\n",
       "      <td>0.153643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nova Scotia</th>\n",
       "      <td>0.113046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Republican</th>\n",
       "      <th>Virginia</th>\n",
       "      <td>0.888933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Washington</th>\n",
       "      <td>0.713758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>0.858978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wisconsin</th>\n",
       "      <td>0.859306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>0.808240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   prob\n",
       "party        state                     \n",
       "Conservative Alberta           0.160214\n",
       "             British Columbia  0.167948\n",
       "             Manitoba          0.217902\n",
       "             New Brunswick     0.153643\n",
       "             Nova Scotia       0.113046\n",
       "...                                 ...\n",
       "Republican   Virginia          0.888933\n",
       "             Washington        0.713758\n",
       "             West Virginia     0.858978\n",
       "             Wisconsin         0.859306\n",
       "             Wyoming           0.808240\n",
       "\n",
       "[115 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_y2_prob(x = forest_hparams, ud = user_data.copy()):\n",
    "    \n",
    "    ## calculate probabilities\n",
    "    f = x['feature_unitary'].best_estimator_\n",
    "    prob = np.concatenate(\n",
    "        [f.predict_proba(train_x), f.predict_proba(test_x)], axis = 0)\n",
    "    prob = prob[:, 1]\n",
    "    prob = pd.Series(prob, name = 'prob')\n",
    "    prob = pd.DataFrame(prob)\n",
    "    prob.index = pd.concat([pd.Series(train_x.index), pd.Series(test_x.index)])\n",
    "    prob['y2'] = pd.concat([pd.Series(train_y2), pd.Series(test_y2)])\n",
    "    \n",
    "    ## merge into user object and calculate statistics\n",
    "    ud['state'] = ud['state'].str.strip()\n",
    "    ud = ud.merge(prob, left_on = 'handle', right_index = True)\n",
    "    ud = ud[['party', 'state', 'prob']].groupby(['party', 'state']).mean()\n",
    "    \n",
    "    return ud\n",
    "    \n",
    "    \n",
    "\n",
    "## execute code\n",
    "state_scores = find_y2_prob()\n",
    "state_scores.to_csv('C_Output/state_scores.csv')\n",
    "state_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb186c33-c378-4d7b-ab28-ee608cee5953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8b6cff4-877e-45d0-8c69-107e8defb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9ec1b-cea5-4168-9a5e-d4767a436101",
   "metadata": {},
   "source": [
    "## FOOT - display objects as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fac6f-85bd-44dc-94a3-1bea9b5d4352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f49ff-0685-47bc-879c-486666f17ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
