{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71861cf0-5da0-430b-a572-f5ce2e4875e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29996091-53a7-4b38-8ac4-9a394c1b55c9",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745dca2-b8b8-40f9-a788-5fd29d962eae",
   "metadata": {},
   "source": [
    "#### HEAD01 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b415c32-1f68-45ff-af0b-4b0b35aea61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import standard libraries\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## HEAD\n",
    "import pickle\n",
    "from os import mkdir\n",
    "from os.path import exists\n",
    "\n",
    "## HAND\n",
    "from time import sleep, time\n",
    "import tweepy\n",
    "\n",
    "## MUNG\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## MODE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "## ??\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08abb1-39dc-4bdc-b18c-87e3083fcafb",
   "metadata": {},
   "source": [
    "#### HEAD02 - user settings and setting validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba7f223-6ede-45fd-b3be-c2f67899fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create toggle file for executing code versus loading cached results\n",
    "set_cache = {\n",
    "    'collect_user_data': {'order': 1, 'bool': False},\n",
    "    ## download tweets from Twitter api and make a dataset\n",
    "    'collect_tweet_data': {'order': 2, 'bool': False},\n",
    "    ## build a dataset of words from tweets\n",
    "    'rebuild_word_data': {'order': 3, 'bool': False},\n",
    "    ## build a linking dataset of words to tweets\n",
    "    'rebuild_tweet_words': {'order': 4, 'bool': False},\n",
    "    ## aggregate tweet_words to be a matrix of word usage rates for each user\n",
    "    'rebuild_user_token': {'order': 5, 'bool': False},\n",
    "    ##\n",
    "    'rebuild_model_data': {'order': 6, 'bool': False},\n",
    "    ##\n",
    "    'fill_in_model_stats': {'order': 7, 'bool': True},\n",
    "    }\n",
    "\n",
    "set_cache = pd.DataFrame(set_cache).T.sort_values('order')\n",
    "\n",
    "## create toggles for other user settings\n",
    "set_other = {'num_parallel_cores': 8, 'use_full_roster': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e6a8-11c4-42f0-87d2-c0c064f709e7",
   "metadata": {},
   "source": [
    "#### HEAD03 - ensure directory structure and validate cache settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1116afa3-2891-4fa6-9411-55e034dcba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check that essential directories exist; make directories as needed\n",
    "def ensure_directories_exist(\n",
    "    dir_list = ['A_Input', 'B_Process', 'C_Output', 'B_Process/cache',\n",
    "               'B_Process/models']):\n",
    "    for i in dir_list:\n",
    "        if exists(i): pass\n",
    "        else: mkdir(i)\n",
    "\n",
    "## validate cache settings (ensure that settings are self-consistent)\n",
    "def validate_cache_bools(sc = set_cache):\n",
    "    the_bool = False\n",
    "    for i in sc.index:\n",
    "        if sc.loc[i, 'order'] == 0:\n",
    "            pass\n",
    "        elif sc.loc[i, 'bool'] or the_bool:\n",
    "            the_bool = True\n",
    "            sc.loc[i, 'bool'] = True\n",
    "    return sc\n",
    "        \n",
    "## execute code\n",
    "ensure_directories_exist()\n",
    "set_cache = validate_cache_bools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3970b34-9fc6-40bf-bc7d-ffd1b94ebbf3",
   "metadata": {},
   "source": [
    "#### HEAD04 - create shells for future objects and load twitter credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd399e3b-c1a6-49e0-9c09-8af80bd7b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user_data\n",
    "user_data = dict()\n",
    "\n",
    "## tweet_data\n",
    "tweet_data = dict()\n",
    "\n",
    "## word_data\n",
    "word_data = dict()\n",
    "\n",
    "## user_word_data\n",
    "user_word_data = dict()\n",
    "\n",
    "## twitter_credentials\n",
    "twitter_credentials = pd.read_csv('../api_keys/twitter.csv').set_index('item')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34886bb9-1b13-4647-85b1-b5ec5c09f533",
   "metadata": {},
   "source": [
    "#### HEAD05 - write functions to toggle between executing functions and loading cached results from code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8709dbbd-6198-4afb-a2d1-7d9e8dd0937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## toggle function for regenerating objects versus loading from disk\n",
    "def execute_or_cache(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        conn = open(address, 'wb')\n",
    "        pickle.dump(x, conn)\n",
    "    else:\n",
    "        conn = open(address, 'rb')\n",
    "        x = pickle.load(conn)\n",
    "    conn.close()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7996cb0e-854d-438b-8c09-7b47b007d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5536b8-98d4-4417-ab78-2cc7068666cb",
   "metadata": {},
   "source": [
    "## HAND - Gather Twitter handles for test accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c8f97-a842-487d-919c-7c5c94cfd478",
   "metadata": {},
   "source": [
    "#### HAND01 - intialize twitter connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92be05b0-735d-4fc0-bf8c-91a1e1b8b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_credentials = tweepy.OAuth1UserHandler(\n",
    "    consumer_key = twitter_credentials.loc['API Key', 'string'],\n",
    "    consumer_secret = twitter_credentials.loc['API Key Secret', 'string'],\n",
    "    access_token = twitter_credentials.loc['Access Token', 'string'],\n",
    "   access_token_secret = twitter_credentials.loc['Access Token Secret', 'string']\n",
    "    )\n",
    "twitter_api = tweepy.API(twitter_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa424f30-e76f-48da-8b51-798d76da3fff",
   "metadata": {},
   "source": [
    "#### HAND02 - load user data from file and extract handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8caddb2-a743-4bb5-838c-81209f6759bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_data(addr = 'A_Input/twitter_handles.xlsx', so = set_other):\n",
    "    \n",
    "    ## read user data from disk\n",
    "    ud = pd.read_excel(addr)\n",
    "    \n",
    "    ## sample from data if in test data\n",
    "    if not so['use_full_roster']:\n",
    "        ud = ud.sample(80, random_state = 4431)\n",
    "    \n",
    "    ## extract user handles\n",
    "    ud['handle'] = ud.url.str.replace('https://twitter.com/', '',\n",
    "                                        regex = False).str.strip().str.lower()\n",
    "    \n",
    "    ## return user_data object\n",
    "    return ud\n",
    "\n",
    "## extract handles from roster urls\n",
    "user_data = execute_or_cache(\n",
    "    address = 'B_Process/cache/user_data.pickle',\n",
    "    function = load_user_data,\n",
    "    build_bool = set_cache.loc['collect_user_data', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7214aa7f-ab9e-40d2-add8-7295bb4ca789",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3a469-c015-4124-9c19-eb5530c4a69d",
   "metadata": {},
   "source": [
    "## PULL - Pull Twitter data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375ea76-f855-40ea-8c93-4a26a67388ee",
   "metadata": {},
   "source": [
    "#### PULL01 - query API for tweet data from each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213cd9f7-c951-4afa-9872-ca07f1ebe88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tweet data from api object (helps pull_tweet_data)\n",
    "def refine_tweet_data(x):\n",
    "    tweet_data = list()\n",
    "    for i in range(0, len(x)):\n",
    "        tweet_data.append({\n",
    "            'tweet_id': x[i].id, \n",
    "            'created_at': x[i].created_at, \n",
    "            'lang': x[i].lang,\n",
    "            'full_text': x[i].full_text,\n",
    "            'screen_name': x[i].author.screen_name,\n",
    "            'verified' : x[i].author.verified\n",
    "        })\n",
    "    return pd.DataFrame(tweet_data)\n",
    "\n",
    "## pull data from the api\n",
    "def pull_tweet_data(handles = user_data.handle, a = twitter_api):\n",
    "    tweet_data = list()\n",
    "    for i in handles:\n",
    "        start_time = time()\n",
    "        try:\n",
    "            user_tweets = a.user_timeline(\n",
    "                screen_name = i, count = 200, tweet_mode = 'extended', \n",
    "                exclude_replies = True, include_rts = False)\n",
    "            tweet_data.append(refine_tweet_data(user_tweets))\n",
    "            sleep_time = max(1.01 - (time() - start_time), 0)\n",
    "            sleep(sleep_time)\n",
    "            print('Querying Twitter API:', sep = ', ', end = '')\n",
    "            print(i, sep = ', ', end = '')\n",
    "        except:\n",
    "            pass\n",
    "    tweet_data = pd.concat(tweet_data)\n",
    "    tweet_data['screen_name'] = tweet_data['screen_name'].str.lower()\n",
    "    return tweet_data\n",
    "\n",
    "## execute code\n",
    "tweet_data = execute_or_cache(\n",
    "    address = 'B_Process/cache/tweet_data.pickle',\n",
    "    function = pull_tweet_data,\n",
    "    build_bool = set_cache.loc['collect_tweet_data', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36108c88-c0b5-468b-8633-d8b296c923c5",
   "metadata": {},
   "source": [
    "#### PULL02 - tabulate tweet statistics, divide users into train/tune/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be309202-ea90-4ca4-baea-e86c4c1b5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combile tweet count and verification status at the user level\n",
    "def enhance_user_tweet_data(td = tweet_data, ud = user_data):\n",
    "    \n",
    "    def np_size(x): return x.size\n",
    "    \n",
    "    \n",
    "    ## calculate tweet summary statistics\n",
    "    td_original = td\n",
    "    td = td[['screen_name', 'verified']].copy()\n",
    "    verified = td.groupby('screen_name').mean()\n",
    "    tweets = td['screen_name'].value_counts()\n",
    "    td = pd.concat([verified, tweets], axis = 1).reset_index()\n",
    "    td.columns = ['handle', 'verified', 'tweets']\n",
    "    \n",
    "    ## merge statistics into the user_data object\n",
    "    ud = pd.merge(ud, td, on = 'handle', how = 'left')\n",
    "    td_original = td_original.drop(['verified'], axis = 1)\n",
    "    ud = ud.drop(['url'], axis = 1).reset_index(drop = True)\n",
    "    ud = ud.fillna({'tweets': 0}).astype({'tweets': int})\n",
    "    \n",
    "    ## divide users into train, and test subsets\n",
    "    ml_set = pd.Series(['train', 'test'], name = 'ml_set').sample(\n",
    "                n = ud.shape[0], replace = True, weights = [0.8, 0.2],\n",
    "                random_state = 2006)\n",
    "    ud['ml_set'] = ml_set.values\n",
    "    ud.loc[ud.tweets == 0, 'ml_set'] = 'exclude'\n",
    "    \n",
    "    return ud, td_original\n",
    "\n",
    "## execute code\n",
    "user_data, tweet_data = enhance_user_tweet_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3baa5953-693f-4e92-baa9-dd794cf56689",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e43cd5-6a3b-4d7d-aeda-b7d1476d685e",
   "metadata": {},
   "source": [
    "## MUNG - Process Twitter data to model-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac2c0f-4a75-4ce0-a42c-61c2ed409d4f",
   "metadata": {},
   "source": [
    "#### MUNG01 - tokenize words and generate a word-level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a06fe9-1433-4b6b-86d5-a77fc09577dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize, remove capitalization, and remove duplicate tokens\n",
    "def nlp_tokenize_tweet(x):\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    x = list(set(x))\n",
    "    return x\n",
    "\n",
    "## create word/token level dataset and identify valid word tokens\n",
    "def make_word_data(td = tweet_data):\n",
    "    \n",
    "    ## tokenize tweet data text\n",
    "    td['tokens'] = td.full_text.apply(nlp_tokenize_tweet)\n",
    "\n",
    "    ## flatten token lists and count occurances\n",
    "    word_data = list()\n",
    "    for i in td.tokens:\n",
    "        word_data += i\n",
    "    word_data = pd.Series(word_data, name = 'count').value_counts()\n",
    "    word_data = word_data.sort_values(ascending = False)\n",
    "    word_data = pd.DataFrame(word_data)\n",
    "    \n",
    "    ## determine which tokens occur often enough to warrant inclusion\n",
    "    word_data['valid'] = word_data['count'] > max(\n",
    "        word_data['count'].quantile(0.2), 3)\n",
    "    word_data['word'] = word_data.index\n",
    "    \n",
    "    ## determine part of speech for eligible tokens\n",
    "    speech_part = word_data['word'].loc[word_data['valid']].values\n",
    "    speech_part = pos_tag(speech_part)\n",
    "    speech_part = [i[1][0].lower() for i in speech_part]\n",
    "    word_data['pos'] = '.'\n",
    "    word_data.loc[word_data['valid'], 'pos'] = speech_part\n",
    "    \n",
    "    ## lemmatize\n",
    "    WNL = WordNetLemmatizer()\n",
    "    word_data['token'] = None\n",
    "    for i in word_data.word:\n",
    "        if not word_data.loc[i, 'valid']: \n",
    "            break\n",
    "        if word_data.loc[i, 'pos'] in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            try:\n",
    "                word_data.loc[i, 'token'] = WNL.lemmatize(\n",
    "                    word_data.loc[i, 'word'],\n",
    "                    pos = word_data.loc[i, 'pos']\n",
    "                )\n",
    "            except:\n",
    "                word_data.loc[i, 'token'] = word_data.loc[i, 'word']\n",
    "        else:\n",
    "            word_data.loc[i, 'valid'] = False\n",
    "        \n",
    "    return word_data.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "word_data = execute_or_cache(\n",
    "    address = 'B_Process/cache/word_data.pickle',\n",
    "    function = make_word_data,\n",
    "    build_bool = set_cache.loc['rebuild_word_data', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de7bec-74ca-4843-91ac-4cd50344e3d4",
   "metadata": {},
   "source": [
    "#### MUNG03 - generate a tokens x tweets link database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1218c2d3-e6a0-4f8f-8827-d057afb05acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a linking dataset of words to tweets\n",
    "def make_tweet_token_data(td = tweet_data, wd = word_data):\n",
    "    \n",
    "    wd = wd.set_index('word')\n",
    "    \n",
    "    ## replicate tweet ids\n",
    "    n = td.tokens.apply(len).values\n",
    "    tweet_tokens = pd.Series(np.repeat(td.tweet_id.values, n), name = \"tweet_id\")\n",
    "    tweet_tokens = pd.DataFrame(tweet_tokens)\n",
    "    \n",
    "    ## allocate words to the new dataset\n",
    "    words = list()\n",
    "    for i in td.tokens:\n",
    "        words += i\n",
    "    tweet_tokens['words'] = words\n",
    "    \n",
    "    ## convert words to tokens\n",
    "    tweet_tokens['tokens'] = wd.loc[\n",
    "        tweet_tokens.words.values, 'token'].values\n",
    "    tweet_tokens = tweet_tokens.dropna()\n",
    "\n",
    "    return tweet_tokens.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "tweet_words = execute_or_cache(\n",
    "    address = 'B_Process/cache/tweet_words.pickle',\n",
    "    function = make_tweet_token_data,\n",
    "    build_bool = set_cache.loc['rebuild_tweet_words', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28aae1-7cc4-4c0e-9863-da2eb7335b0c",
   "metadata": {},
   "source": [
    "#### MUNG04 - generate a tokens x users count; drop tokens with only one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d0576bc-23d1-44e4-95d0-bac5f746dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregate tweet_words to be a matrix of word usage rates for each user\n",
    "def make_user_token_matrix(td = tweet_data, tw = tweet_words, ud = user_data):\n",
    "    \n",
    "    ## count of number of times each user wrote each token\n",
    "    tw = tw.merge(right = td[['screen_name', 'tweet_id']],\n",
    "                  how = 'left', on = 'tweet_id')\n",
    "    tw = tw.drop(['tweet_id', 'words'], axis = 1).groupby('screen_name')\n",
    "    tw = tw.value_counts()\n",
    "    tw.name = 'count'\n",
    "    tw = tw.reset_index().set_index('screen_name')\n",
    "    tw = tw.pivot(columns = 'tokens').fillna(0).astype(int)\n",
    "    tw = tw.droplevel(axis = 1, level = 0)\n",
    "    \n",
    "    ## remove tokens that fewer than 3 or more than 90 percent of users use\n",
    "    valid_usage = (tw > 0).astype(int).sum().values\n",
    "    valid_usage = (valid_usage > 2\n",
    "                  ) & (valid_usage < int(tw.shape[0] * 0.9))\n",
    "    tw = tw.loc[:, valid_usage]\n",
    "    \n",
    "    ## standardize matrix as words per 1,000 tweets\n",
    "    denom = pd.DataFrame({'handle': tw.index}).merge(\n",
    "        ud[['handle', 'tweets']],\n",
    "        how = 'left', on = 'handle'\n",
    "        ).set_index('handle').squeeze().fillna(1)\n",
    "    denom.loc[denom < 1] = 1\n",
    "    tw = (tw.divide(denom, axis = 0) * 1000).astype(int)\n",
    "    tw = tw.reset_index().rename({'level_0':'screen_name'}, axis = 1)\n",
    "    \n",
    "    return tw\n",
    "\n",
    "## execute code\n",
    "user_token_matrix = execute_or_cache(\n",
    "    address = 'B_Process/cache/user_token_matrix.pickle',\n",
    "    function = make_user_token_matrix,\n",
    "    build_bool = set_cache.loc['rebuild_user_token', 'bool']\n",
    "    ).set_index('screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238ebf22-bc91-4085-a201-81781a655a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4800e93-b052-4544-bb3a-8da62333d013",
   "metadata": {},
   "source": [
    "## MODE – Train models and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44c208-88da-4fef-82ca-fc641db27133",
   "metadata": {},
   "source": [
    "#### MODE01 - build x/y train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c200c207-61a3-48b5-b50a-90355ed8dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unpack train and test datasets\n",
    "def split_xy_data(ml_cat):\n",
    "    i = user_data.loc[user_data.ml_set == ml_cat, 'handle'].values\n",
    "    \n",
    "    x = user_token_matrix.loc[i, :]\n",
    "    \n",
    "    y1 = user_data.set_index('handle').loc[i, 'group'] == 'USA House'\n",
    "    y1 = y1.astype(int)\n",
    "    \n",
    "    y2 = user_data.set_index('handle').loc[i, 'party'] == 'Republican'\n",
    "    y2 = y2.astype(int)\n",
    "    \n",
    "    return x, y1, y2\n",
    "\n",
    "## train an alternative feature matrix using principle components\n",
    "def make_pca_version(trainx, testx):\n",
    "    model_pca = PCA().fit(trainx)\n",
    "    trainx_pca = pd.DataFrame(model_pca.transform(trainx),\n",
    "                              index = trainx.index)\n",
    "    testx_pca = pd.DataFrame(model_pca.transform(testx),\n",
    "                              index = testx.index)\n",
    "    return trainx_pca, testx_pca\n",
    "\n",
    "## execute code\n",
    "train_x, train_y1, train_y2 = split_xy_data('train')\n",
    "test_x, test_y1, test_y2 = split_xy_data('test')\n",
    "train_x_pca, test_x_pca = make_pca_version(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7af1b018-c0f0-4e93-baaf-39da987ca278",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop tweet_words to free up extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b0de3eb-75e9-421c-86c8-4d8b1edd4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweet_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0751295-c65b-45b3-81d5-05b2daf85ae0",
   "metadata": {},
   "source": [
    "#### MODE02 - formulate model database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efeb2a4b-b65e-4812-874e-52fd6263c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate dataset of all model variations to be tested\n",
    "def make_model_data():\n",
    "    \n",
    "    ## generate model formulations\n",
    "    model_data = dict()\n",
    "    for i in ['LR', 'NB', 'RF', 'AB']:\n",
    "        for j in ['1S', '2S']:\n",
    "            for k in ['FM', 'PC']:\n",
    "                l = i + j + k\n",
    "                model_data[l] = {'model': i, 'stage': j, 'x': k}\n",
    "    model_data = pd.DataFrame(model_data).T\n",
    "    \n",
    "    ## create slots for time and accuracy scores\n",
    "    model_data['precision'] = -1\n",
    "    model_data['recall']    = -1\n",
    "    model_data['f1']        = -1\n",
    "    model_data['time']      = -1\n",
    "    model_data['f1_train']  = -1\n",
    "    \n",
    "    ## generate lists of hyperparameters to test\n",
    "    model_params = {\n",
    "        'LR': {'C': [2. ** i for i in range(-2, 6)]},\n",
    "        'NB': {'priors': [(0.5, 0.5), (1 - train_y1.mean(), train_y1.mean()),\n",
    "                          (1 - train_y2.mean(), train_y2.mean())]},\n",
    "        'RF': {'n_estimators': [2**i for i in range(6, 10)],\n",
    "               'min_samples_leaf': [2**i for i in range(2, 5)]},\n",
    "        'AB':{'n_estimators':  [int(2**i) for i in range(6, 10)],\n",
    "              'learning_rate': [2**i for i in range(-2, 3)]},\n",
    "    }\n",
    "    model_data_params = list()\n",
    "    for i in model_data.index:\n",
    "        model_data_params.append(model_params[model_data.loc[i, 'model']])\n",
    "    model_data['params'] = model_data_params\n",
    "    \n",
    "    ## generate list of model functions to test\n",
    "    model_dict = {\n",
    "        'LR': LogisticRegression(penalty = 'l1', class_weight = 'balanced',\n",
    "                                   solver = 'saga', max_iter = 2**10),\n",
    "        'NB': GaussianNB(),\n",
    "        'RF': RandomForestClassifier(class_weight = 'balanced'),\n",
    "        'AB': AdaBoostClassifier(base_estimator = None),\n",
    "        }\n",
    "    model_list = list()\n",
    "    for i in model_data.index:\n",
    "        model_list.append(model_dict[model_data.loc[i, 'model']])\n",
    "    model_data['function'] = model_list\n",
    "    \n",
    "    ## create slot for GridSearchCV object\n",
    "    model_data['best_params'] = [list() for i in model_data.index]\n",
    "\n",
    "    ## return results\n",
    "    return model_data\n",
    "\n",
    "## execute data\n",
    "model_data = make_model_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b13acf-e25a-4d7e-8776-28fc48d497df",
   "metadata": {},
   "source": [
    "#### MODE03 - fit models and find best parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87ebddde-e5e1-4873-8684-84f2486df3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(md = model_data):\n",
    "    \n",
    "    ## create index to distinguish between stages\n",
    "    only_s2 = train_y1.astype(bool).values\n",
    "    \n",
    "    ## choose the right x\n",
    "    if md.loc[idx, 'x'] == 'FM':\n",
    "        trax = train_x\n",
    "        tesx = test_x\n",
    "    else:\n",
    "        trax = train_x_pca\n",
    "        tesx = test_x_pca\n",
    "        \n",
    "    ## choose the right model\n",
    "    mod = md.loc[idx, 'function']\n",
    "    \n",
    "    ## build cross-validation function\n",
    "    cv_function = GridSearchCV(scoring = 'f1', cv = 5,\n",
    "                               param_grid = md.loc[idx, 'params'],\n",
    "                               n_jobs = set_other['num_parallel_cores'],\n",
    "                               estimator = md.loc[idx, 'function'])\n",
    "    ## train model\n",
    "    hparam_object = list()\n",
    "    if md.loc[idx, 'stage'] == '1S':\n",
    "        hparam_object.append(cv_function.fit(X = trax, y = train_y2))\n",
    "    else:\n",
    "        hparam_object.append(cv_function.fit(X = trax, y = train_y1))\n",
    "        hparam_object.append(cv_function.fit(\n",
    "            X = trax.loc[only_s2, :], y = train_y2.loc[only_s2]))\n",
    "    \n",
    "    ## return results\n",
    "    return hparam_object\n",
    "    \n",
    "## execute code\n",
    "for idx in model_data.index:\n",
    "    execute_or_cache(\n",
    "        address = 'B_Process/models/' + idx + '.pickle',\n",
    "        function = train_model,\n",
    "        build_bool = set_cache.loc['rebuild_model_data', 'bool']\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcb55c-d01e-413d-b541-03226dc06a30",
   "metadata": {},
   "source": [
    "#### MODE04 - extract statistics from model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa0cd291-9500-4a2e-9efb-fe58873995e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s8/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/s8/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/s8/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/s8/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/s8/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/s8/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA run time:  0.8349189758300781\n"
     ]
    }
   ],
   "source": [
    "## evaluate one stage models\n",
    "def evaluate_model(idx, md):\n",
    "    \n",
    "    ## create index to distinguish between stages\n",
    "    only_s2 = train_y1.astype(bool).values\n",
    "    \n",
    "    ## choose the right x\n",
    "    if md.loc[idx, 'x'] == 'FM':\n",
    "        trax = train_x\n",
    "        tesx = test_x\n",
    "    else:\n",
    "        trax = train_x_pca\n",
    "        tesx = test_x_pca\n",
    "        \n",
    "    ## choose the right model\n",
    "    mod = clone(md.loc[idx, 'function'])\n",
    "    \n",
    "    ## fit model and predict test data\n",
    "    if md.loc[idx, 'stage'] == '1S':\n",
    "        run_time = time()\n",
    "        params = mod.get_params()\n",
    "        params.update(md.loc[idx, 'best_params'][0])\n",
    "        mod = clone(mod).set_params(**params).fit(trax, train_y2)\n",
    "        \n",
    "        pred = mod.predict(tesx)\n",
    "        run_time = time() - run_time\n",
    "        pred_train = mod.predict(trax)\n",
    "    else:\n",
    "        run_time = time()\n",
    "        params = mod.get_params()\n",
    "        params.update(md.loc[idx, 'best_params'][0])\n",
    "        mod1 = clone(mod).set_params(**params).fit(trax, train_y1)\n",
    "        \n",
    "        params = mod.get_params()\n",
    "        params.update(md.loc[idx, 'best_params'][1])\n",
    "        mod2 = clone(mod).set_params(**params)\n",
    "        mod2 = mod2.fit(trax.loc[only_s2, :], train_y2.loc[only_s2])\n",
    "        \n",
    "        pred = mod1.predict(tesx) * mod2.predict(tesx)\n",
    "        run_time = time() - run_time\n",
    "        pred_train = mod1.predict(trax) * mod2.predict(trax)\n",
    "        \n",
    "    ## tabulate performance statistics (test data)\n",
    "    md.loc[idx, 'precision'] = precision_score(test_y2, pred)\n",
    "    md.loc[idx, 'recall'] = recall_score(test_y2, pred)\n",
    "    md.loc[idx, 'f1'] = f1_score(test_y2, pred)\n",
    "    md.loc[idx, 'time'] = run_time\n",
    "    md.loc[idx, 'f1_train'] = f1_score(train_y2, pred_train)\n",
    "    \n",
    "    ## return results\n",
    "    return md\n",
    "\n",
    "## extract best model parameters\n",
    "def extract_best_params(md = model_data.copy()):\n",
    "    \n",
    "    ## loop through models\n",
    "    best_params = list()\n",
    "    for i in md.index:\n",
    "        \n",
    "        ## load model\n",
    "        conn = open('B_Process/models/' + i + '.pickle', 'rb')\n",
    "        cv_object = pickle.load(conn)\n",
    "        conn.close()\n",
    "        \n",
    "        ## extract best parameters\n",
    "        for j in range(0, len(cv_object)):\n",
    "            cv_object[j] = cv_object[j].best_params_\n",
    "        best_params.append(cv_object)\n",
    "        \n",
    "    ## return results\n",
    "    md['best_params'] = best_params  \n",
    "    return md\n",
    "\n",
    "## add pca fit time to pca models\n",
    "def add_pca_time(md):\n",
    "    \n",
    "    ## measure pca run time\n",
    "    run_time = time()\n",
    "    pca_model = PCA().fit(train_x)\n",
    "    trax = pca_model.transform(train_x)\n",
    "    tesx = pca_model.transform(test_x)\n",
    "    run_time = time() - run_time\n",
    "    \n",
    "    ## add run time to relevant tables\n",
    "    print('PCA run time: ', run_time)\n",
    "    i = md.x == 'PCA'\n",
    "    md.loc[i, 'time'] = (md.loc[i, 'time'] + run_time) * -1\n",
    "    \n",
    "    ## return results\n",
    "    return md\n",
    "\n",
    "## roll up all functions into a single package function\n",
    "def calculate_model_stats(md = model_data):\n",
    "    md = extract_best_params()\n",
    "    for i in md.index: md = evaluate_model(i, md)\n",
    "    md = add_pca_time(md.copy())\n",
    "    return md\n",
    "\n",
    "## execute code\n",
    "model_data = execute_or_cache(\n",
    "        address = 'B_Process/cache/model_data.pickle',\n",
    "        function = calculate_model_stats,\n",
    "        build_bool = set_cache.loc['fill_in_model_stats', 'bool']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a742b0b-0cdd-4497-bbc3-e49c3dce4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.to_csv('C_Output/model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb2f9aed-f773-4f26-9fd4-6694c7fcbfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>stage</th>\n",
       "      <th>x</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>params</th>\n",
       "      <th>function</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NB1SPC</th>\n",
       "      <td>NB</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.622568</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB2SPC</th>\n",
       "      <td>NB</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>0.696231</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB1SFM</th>\n",
       "      <td>NB</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>0.382650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF1SFM</th>\n",
       "      <td>RF</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>0.988453</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 16, 'n_estimators': 128}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF1SPC</th>\n",
       "      <td>RF</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.568386</td>\n",
       "      <td>0.997701</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 8, 'n_estimators': 128}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB2SFM</th>\n",
       "      <td>NB</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.687221</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF2SPC</th>\n",
       "      <td>RF</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.865156</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 8, 'n_estimators': 128},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF2SFM</th>\n",
       "      <td>RF</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>3.408125</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 4, 'n_estimators': 512},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB1SPC</th>\n",
       "      <td>AB</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>3.585175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.25, 'n_estimators': 128}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR1SPC</th>\n",
       "      <td>LR</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>4.114126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.5}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2SPC</th>\n",
       "      <td>LR</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>6.002437</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.25}, {'C': 0.25}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB2SPC</th>\n",
       "      <td>AB</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>11.174774</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.5, 'n_estimators': 256}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB1SFM</th>\n",
       "      <td>AB</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>51.988796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.25, 'n_estimators': 512}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB2SFM</th>\n",
       "      <td>AB</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>83.603132</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.25, 'n_estimators': 512},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR1SFM</th>\n",
       "      <td>LR</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>98.230987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.25}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2SFM</th>\n",
       "      <td>LR</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>161.457307</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.5}, {'C': 0.5}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model stage   x  precision    recall        f1        time  f1_train  \\\n",
       "NB1SPC    NB    1S  PC   0.500000  1.000000  0.666667    0.013171  0.622568   \n",
       "NB2SPC    NB    2S  PC   0.617647  0.976744  0.756757    0.021205  0.696231   \n",
       "NB1SFM    NB    1S  FM   0.973684  0.860465  0.913580    0.382650  1.000000   \n",
       "RF1SFM    RF    1S  FM   1.000000  0.953488  0.976190    0.511713  0.988453   \n",
       "RF1SPC    RF    1S  PC   1.000000  0.976744  0.988235    0.568386  0.997701   \n",
       "NB2SFM    NB    2S  FM   0.947368  0.837209  0.888889    0.687221  0.901763   \n",
       "RF2SPC    RF    2S  PC   0.893617  0.976744  0.933333    0.865156  0.901763   \n",
       "RF2SFM    RF    2S  FM   0.976744  0.976744  0.976744    3.408125  0.901763   \n",
       "AB1SPC    AB    1S  PC   0.952381  0.930233  0.941176    3.585175  1.000000   \n",
       "LR1SPC    LR    1S  PC   0.954545  0.976744  0.965517    4.114126  1.000000   \n",
       "LR2SPC    LR    2S  PC   0.971429  0.790698  0.871795    6.002437  0.901763   \n",
       "AB2SPC    AB    2S  PC   0.968750  0.720930  0.826667   11.174774  0.901763   \n",
       "AB1SFM    AB    1S  FM   0.976190  0.953488  0.964706   51.988796  1.000000   \n",
       "AB2SFM    AB    2S  FM   0.968750  0.720930  0.826667   83.603132  0.901763   \n",
       "LR1SFM    LR    1S  FM   0.953488  0.953488  0.953488   98.230987  1.000000   \n",
       "LR2SFM    LR    2S  FM   0.970588  0.767442  0.857143  161.457307  0.901763   \n",
       "\n",
       "                                                   params  \\\n",
       "NB1SPC  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "NB2SPC  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "NB1SFM  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "RF1SFM  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "RF1SPC  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "NB2SFM  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "RF2SPC  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "RF2SFM  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "AB1SPC  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "LR1SPC  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "LR2SPC  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "AB2SPC  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "AB1SFM  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "AB2SFM  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "LR1SFM  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "LR2SFM  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "\n",
       "                                                 function  \\\n",
       "NB1SPC                                       GaussianNB()   \n",
       "NB2SPC                                       GaussianNB()   \n",
       "NB1SFM                                       GaussianNB()   \n",
       "RF1SFM    RandomForestClassifier(class_weight='balanced')   \n",
       "RF1SPC    RandomForestClassifier(class_weight='balanced')   \n",
       "NB2SFM                                       GaussianNB()   \n",
       "RF2SPC    RandomForestClassifier(class_weight='balanced')   \n",
       "RF2SFM    RandomForestClassifier(class_weight='balanced')   \n",
       "AB1SPC                               AdaBoostClassifier()   \n",
       "LR1SPC  LogisticRegression(class_weight='balanced', ma...   \n",
       "LR2SPC  LogisticRegression(class_weight='balanced', ma...   \n",
       "AB2SPC                               AdaBoostClassifier()   \n",
       "AB1SFM                               AdaBoostClassifier()   \n",
       "AB2SFM                               AdaBoostClassifier()   \n",
       "LR1SFM  LogisticRegression(class_weight='balanced', ma...   \n",
       "LR2SFM  LogisticRegression(class_weight='balanced', ma...   \n",
       "\n",
       "                                              best_params  \n",
       "NB1SPC                           [{'priors': (0.5, 0.5)}]  \n",
       "NB2SPC   [{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]  \n",
       "NB1SFM                           [{'priors': (0.5, 0.5)}]  \n",
       "RF1SFM    [{'min_samples_leaf': 16, 'n_estimators': 128}]  \n",
       "RF1SPC     [{'min_samples_leaf': 8, 'n_estimators': 128}]  \n",
       "NB2SFM   [{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]  \n",
       "RF2SPC  [{'min_samples_leaf': 8, 'n_estimators': 128},...  \n",
       "RF2SFM  [{'min_samples_leaf': 4, 'n_estimators': 512},...  \n",
       "AB1SPC     [{'learning_rate': 0.25, 'n_estimators': 128}]  \n",
       "LR1SPC                                       [{'C': 0.5}]  \n",
       "LR2SPC                         [{'C': 0.25}, {'C': 0.25}]  \n",
       "AB2SPC  [{'learning_rate': 0.5, 'n_estimators': 256}, ...  \n",
       "AB1SFM     [{'learning_rate': 0.25, 'n_estimators': 512}]  \n",
       "AB2SFM  [{'learning_rate': 0.25, 'n_estimators': 512},...  \n",
       "LR1SFM                                      [{'C': 0.25}]  \n",
       "LR2SFM                           [{'C': 0.5}, {'C': 0.5}]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deeeace1-6556-4e97-a46b-758c71d0c63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>stage</th>\n",
       "      <th>x</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>params</th>\n",
       "      <th>function</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR1SFM</th>\n",
       "      <td>LR</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>98.230987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.25}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR1SPC</th>\n",
       "      <td>LR</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>4.114126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.5}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2SFM</th>\n",
       "      <td>LR</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>161.457307</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.5}, {'C': 0.5}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR2SPC</th>\n",
       "      <td>LR</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>6.002437</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...</td>\n",
       "      <td>LogisticRegression(class_weight='balanced', ma...</td>\n",
       "      <td>[{'C': 0.25}, {'C': 0.25}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB1SFM</th>\n",
       "      <td>NB</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>0.382650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB1SPC</th>\n",
       "      <td>NB</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.622568</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB2SFM</th>\n",
       "      <td>NB</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.687221</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB2SPC</th>\n",
       "      <td>NB</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>0.696231</td>\n",
       "      <td>{'priors': [(0.5, 0.5), (0.43410852713178294, ...</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>[{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF1SFM</th>\n",
       "      <td>RF</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.511713</td>\n",
       "      <td>0.988453</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 16, 'n_estimators': 128}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF1SPC</th>\n",
       "      <td>RF</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.568386</td>\n",
       "      <td>0.997701</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 8, 'n_estimators': 128}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF2SFM</th>\n",
       "      <td>RF</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>3.408125</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 4, 'n_estimators': 512},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF2SPC</th>\n",
       "      <td>RF</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.865156</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'min_sam...</td>\n",
       "      <td>RandomForestClassifier(class_weight='balanced')</td>\n",
       "      <td>[{'min_samples_leaf': 8, 'n_estimators': 128},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB1SFM</th>\n",
       "      <td>AB</td>\n",
       "      <td>1S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>51.988796</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.25, 'n_estimators': 512}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB1SPC</th>\n",
       "      <td>AB</td>\n",
       "      <td>1S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>3.585175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.25, 'n_estimators': 128}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB2SFM</th>\n",
       "      <td>AB</td>\n",
       "      <td>2S</td>\n",
       "      <td>FM</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>83.603132</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.25, 'n_estimators': 512},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB2SPC</th>\n",
       "      <td>AB</td>\n",
       "      <td>2S</td>\n",
       "      <td>PC</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>11.174774</td>\n",
       "      <td>0.901763</td>\n",
       "      <td>{'n_estimators': [64, 128, 256, 512], 'learnin...</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>[{'learning_rate': 0.5, 'n_estimators': 256}, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model stage   x  precision    recall        f1        time  f1_train  \\\n",
       "LR1SFM    LR    1S  FM   0.953488  0.953488  0.953488   98.230987  1.000000   \n",
       "LR1SPC    LR    1S  PC   0.954545  0.976744  0.965517    4.114126  1.000000   \n",
       "LR2SFM    LR    2S  FM   0.970588  0.767442  0.857143  161.457307  0.901763   \n",
       "LR2SPC    LR    2S  PC   0.971429  0.790698  0.871795    6.002437  0.901763   \n",
       "NB1SFM    NB    1S  FM   0.973684  0.860465  0.913580    0.382650  1.000000   \n",
       "NB1SPC    NB    1S  PC   0.500000  1.000000  0.666667    0.013171  0.622568   \n",
       "NB2SFM    NB    2S  FM   0.947368  0.837209  0.888889    0.687221  0.901763   \n",
       "NB2SPC    NB    2S  PC   0.617647  0.976744  0.756757    0.021205  0.696231   \n",
       "RF1SFM    RF    1S  FM   1.000000  0.953488  0.976190    0.511713  0.988453   \n",
       "RF1SPC    RF    1S  PC   1.000000  0.976744  0.988235    0.568386  0.997701   \n",
       "RF2SFM    RF    2S  FM   0.976744  0.976744  0.976744    3.408125  0.901763   \n",
       "RF2SPC    RF    2S  PC   0.893617  0.976744  0.933333    0.865156  0.901763   \n",
       "AB1SFM    AB    1S  FM   0.976190  0.953488  0.964706   51.988796  1.000000   \n",
       "AB1SPC    AB    1S  PC   0.952381  0.930233  0.941176    3.585175  1.000000   \n",
       "AB2SFM    AB    2S  FM   0.968750  0.720930  0.826667   83.603132  0.901763   \n",
       "AB2SPC    AB    2S  PC   0.968750  0.720930  0.826667   11.174774  0.901763   \n",
       "\n",
       "                                                   params  \\\n",
       "LR1SFM  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "LR1SPC  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "LR2SFM  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "LR2SPC  {'C': [0.25, 0.5, 1.0, 2.0, 4.0, 8.0, 16.0, 32...   \n",
       "NB1SFM  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "NB1SPC  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "NB2SFM  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "NB2SPC  {'priors': [(0.5, 0.5), (0.43410852713178294, ...   \n",
       "RF1SFM  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "RF1SPC  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "RF2SFM  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "RF2SPC  {'n_estimators': [64, 128, 256, 512], 'min_sam...   \n",
       "AB1SFM  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "AB1SPC  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "AB2SFM  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "AB2SPC  {'n_estimators': [64, 128, 256, 512], 'learnin...   \n",
       "\n",
       "                                                 function  \\\n",
       "LR1SFM  LogisticRegression(class_weight='balanced', ma...   \n",
       "LR1SPC  LogisticRegression(class_weight='balanced', ma...   \n",
       "LR2SFM  LogisticRegression(class_weight='balanced', ma...   \n",
       "LR2SPC  LogisticRegression(class_weight='balanced', ma...   \n",
       "NB1SFM                                       GaussianNB()   \n",
       "NB1SPC                                       GaussianNB()   \n",
       "NB2SFM                                       GaussianNB()   \n",
       "NB2SPC                                       GaussianNB()   \n",
       "RF1SFM    RandomForestClassifier(class_weight='balanced')   \n",
       "RF1SPC    RandomForestClassifier(class_weight='balanced')   \n",
       "RF2SFM    RandomForestClassifier(class_weight='balanced')   \n",
       "RF2SPC    RandomForestClassifier(class_weight='balanced')   \n",
       "AB1SFM                               AdaBoostClassifier()   \n",
       "AB1SPC                               AdaBoostClassifier()   \n",
       "AB2SFM                               AdaBoostClassifier()   \n",
       "AB2SPC                               AdaBoostClassifier()   \n",
       "\n",
       "                                              best_params  \n",
       "LR1SFM                                      [{'C': 0.25}]  \n",
       "LR1SPC                                       [{'C': 0.5}]  \n",
       "LR2SFM                           [{'C': 0.5}, {'C': 0.5}]  \n",
       "LR2SPC                         [{'C': 0.25}, {'C': 0.25}]  \n",
       "NB1SFM                           [{'priors': (0.5, 0.5)}]  \n",
       "NB1SPC                           [{'priors': (0.5, 0.5)}]  \n",
       "NB2SFM   [{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]  \n",
       "NB2SPC   [{'priors': (0.5, 0.5)}, {'priors': (0.5, 0.5)}]  \n",
       "RF1SFM    [{'min_samples_leaf': 16, 'n_estimators': 128}]  \n",
       "RF1SPC     [{'min_samples_leaf': 8, 'n_estimators': 128}]  \n",
       "RF2SFM  [{'min_samples_leaf': 4, 'n_estimators': 512},...  \n",
       "RF2SPC  [{'min_samples_leaf': 8, 'n_estimators': 128},...  \n",
       "AB1SFM     [{'learning_rate': 0.25, 'n_estimators': 512}]  \n",
       "AB1SPC     [{'learning_rate': 0.25, 'n_estimators': 128}]  \n",
       "AB2SFM  [{'learning_rate': 0.25, 'n_estimators': 512},...  \n",
       "AB2SPC  [{'learning_rate': 0.5, 'n_estimators': 256}, ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO: ADD HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5504d3ef-8bff-44e3-868a-349bd8900191",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d22fc-5df4-4122-af1c-b7a003b158ab",
   "metadata": {},
   "source": [
    "## EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1628f7f6-11ff-4b4c-b95b-429ab156c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9204e-5315-4616-92d2-f0be874745a2",
   "metadata": {},
   "source": [
    "## TOPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59913e1d-08c3-423e-b5fa-d2528ccc4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4baec78-0e62-4780-ac1e-b6cefc3bda42",
   "metadata": {},
   "source": [
    "## FOOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43416ed2-6493-407c-b2ad-a69cd011825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
