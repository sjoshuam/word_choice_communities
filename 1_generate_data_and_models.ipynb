{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71861cf0-5da0-430b-a572-f5ce2e4875e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29996091-53a7-4b38-8ac4-9a394c1b55c9",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745dca2-b8b8-40f9-a788-5fd29d962eae",
   "metadata": {},
   "source": [
    "#### HEAD01 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b415c32-1f68-45ff-af0b-4b0b35aea61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import standard libraries\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## HEAD\n",
    "import pickle\n",
    "from os import mkdir\n",
    "from os.path import exists\n",
    "\n",
    "## HAND\n",
    "from time import sleep, time\n",
    "import tweepy\n",
    "\n",
    "## MUNG\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08abb1-39dc-4bdc-b18c-87e3083fcafb",
   "metadata": {},
   "source": [
    "#### HEAD02 - user settings and setting validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ba7f223-6ede-45fd-b3be-c2f67899fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create toggle file for executing code versus loading cached results\n",
    "set_cache = {\n",
    "    ## do *not* limit the user roster to a small subset (for testing)\n",
    "    'use_full_roster': {'order': 0, 'bool': False},\n",
    "    ## generate user data from file on disk\n",
    "    'collect_user_data': {'order': 1, 'bool': False},\n",
    "    ## download tweets from Twitter api and make a dataset\n",
    "    'collect_tweet_data': {'order': 2, 'bool': False},\n",
    "    ## build a dataset of words from tweets\n",
    "    'rebuild_word_data': {'order': 3, 'bool': False},\n",
    "    ## build a linking dataset of words to tweets\n",
    "    'rebuild_tweet_words': {'order': 4, 'bool': False},\n",
    "    ## aggregate tweet_words to be a matrix of word usage rates for each user\n",
    "    'rebuild_user_token': {'order': 5, 'bool': False},\n",
    "    ##\n",
    "    'rebuild_model_data': {'order': 6, 'bool': True},\n",
    "    }\n",
    "\n",
    "set_cache = pd.DataFrame(set_cache).T.sort_values('order')\n",
    "\n",
    "## create toggles for other user settings\n",
    "set_other = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e6a8-11c4-42f0-87d2-c0c064f709e7",
   "metadata": {},
   "source": [
    "#### HEAD03 - ensure directory structure and validate cache settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1116afa3-2891-4fa6-9411-55e034dcba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check that essential directories exist; make directories as needed\n",
    "def ensure_directories_exist(\n",
    "    dir_list = ['A_Input', 'B_Process', 'C_Output', 'B_Process/cache']):\n",
    "    for i in dir_list:\n",
    "        if exists(i): pass\n",
    "        else: mkdir(i)\n",
    "\n",
    "## validate cache settings (ensure that settings are self-consistent)\n",
    "def validate_cache_bools(sc = set_cache):\n",
    "    the_bool = False\n",
    "    for i in sc.index:\n",
    "        if sc.loc[i, 'order'] == 0:\n",
    "            pass\n",
    "        elif sc.loc[i, 'bool'] or the_bool:\n",
    "            the_bool = True\n",
    "            sc.loc[i, 'bool'] = True\n",
    "    return sc\n",
    "        \n",
    "## execute code\n",
    "ensure_directories_exist()\n",
    "set_cache = validate_cache_bools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3970b34-9fc6-40bf-bc7d-ffd1b94ebbf3",
   "metadata": {},
   "source": [
    "#### HEAD04 - create shells for future objects and load twitter credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd399e3b-c1a6-49e0-9c09-8af80bd7b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user_data\n",
    "user_data = dict()\n",
    "\n",
    "## tweet_data\n",
    "tweet_data = dict()\n",
    "\n",
    "## word_data\n",
    "word_data = dict()\n",
    "\n",
    "## user_word_data\n",
    "user_word_data = dict()\n",
    "\n",
    "## twitter_credentials\n",
    "twitter_credentials = pd.read_csv('../api_keys/twitter.csv').set_index('item')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34886bb9-1b13-4647-85b1-b5ec5c09f533",
   "metadata": {},
   "source": [
    "#### HEAD05 - write functions to toggle between executing functions and loading cached results from code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8709dbbd-6198-4afb-a2d1-7d9e8dd0937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## toggle function for regenerating objects versus loading from disk\n",
    "def execute_or_cache(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        conn = open(address, 'wb')\n",
    "        pickle.dump(x, conn)\n",
    "    else:\n",
    "        conn = open(address, 'rb')\n",
    "        x = pickle.load(conn)\n",
    "    conn.close()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7996cb0e-854d-438b-8c09-7b47b007d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5536b8-98d4-4417-ab78-2cc7068666cb",
   "metadata": {},
   "source": [
    "## HAND - Gather Twitter handles for test accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c8f97-a842-487d-919c-7c5c94cfd478",
   "metadata": {},
   "source": [
    "#### HAND01 - intialize twitter connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92be05b0-735d-4fc0-bf8c-91a1e1b8b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_credentials = tweepy.OAuth1UserHandler(\n",
    "    consumer_key = twitter_credentials.loc['API Key', 'string'],\n",
    "    consumer_secret = twitter_credentials.loc['API Key Secret', 'string'],\n",
    "    access_token = twitter_credentials.loc['Access Token', 'string'],\n",
    "   access_token_secret = twitter_credentials.loc['Access Token Secret', 'string']\n",
    "    )\n",
    "twitter_api = tweepy.API(twitter_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa424f30-e76f-48da-8b51-798d76da3fff",
   "metadata": {},
   "source": [
    "#### HAND02 - load user data from file and extract handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8caddb2-a743-4bb5-838c-81209f6759bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_data(addr = 'A_Input/twitter_handles.xlsx', sc = set_cache):\n",
    "    \n",
    "    ## read user data from disk\n",
    "    ud = pd.read_excel(addr)\n",
    "    \n",
    "    ## sample from data if in test data\n",
    "    if not sc.loc['use_full_roster', 'bool']:\n",
    "        ud = ud.sample(125, random_state = 4431)\n",
    "    \n",
    "    ## extract user handles\n",
    "    ud['handle'] = ud.url.str.replace('https://twitter.com/', '',\n",
    "                                        regex = False).str.strip().str.lower()\n",
    "    \n",
    "    ## return user_data object\n",
    "    return ud\n",
    "\n",
    "## extract handles from roster urls\n",
    "user_data = execute_or_cache(\n",
    "    address = 'B_Process/cache/user_data.pickle',\n",
    "    function = load_user_data,\n",
    "    build_bool = set_cache.loc['collect_user_data', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7214aa7f-ab9e-40d2-add8-7295bb4ca789",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3a469-c015-4124-9c19-eb5530c4a69d",
   "metadata": {},
   "source": [
    "## PULL - Pull Twitter data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375ea76-f855-40ea-8c93-4a26a67388ee",
   "metadata": {},
   "source": [
    "#### PULL01 - query API for tweet data from each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213cd9f7-c951-4afa-9872-ca07f1ebe88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tweet data from api object (helps pull_tweet_data)\n",
    "def refine_tweet_data(x):\n",
    "    tweet_data = list()\n",
    "    for i in range(0, len(x)):\n",
    "        tweet_data.append({\n",
    "            'tweet_id': x[i].id, \n",
    "            'created_at': x[i].created_at, \n",
    "            'lang': x[i].lang,\n",
    "            'full_text': x[i].full_text,\n",
    "            'screen_name': x[i].author.screen_name,\n",
    "            'verified' : x[i].author.verified\n",
    "        })\n",
    "    return pd.DataFrame(tweet_data)\n",
    "\n",
    "## pull data from the api\n",
    "def pull_tweet_data(handles = user_data.handle, a = twitter_api):\n",
    "    tweet_data = list()\n",
    "    for i in handles:\n",
    "        start_time = time()\n",
    "        try:\n",
    "            user_tweets = a.user_timeline(\n",
    "                screen_name = i, count = 200, tweet_mode = 'extended', \n",
    "                exclude_replies = True, include_rts = False)\n",
    "            tweet_data.append(refine_tweet_data(user_tweets))\n",
    "            sleep_time = max(1 - (time() - start_time), 0)\n",
    "            sleep(sleep_time)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_data = pd.concat(tweet_data)\n",
    "    tweet_data['screen_name'] = tweet_data['screen_name'].str.lower()\n",
    "    return tweet_data\n",
    "\n",
    "## execute code\n",
    "tweet_data = execute_or_cache(\n",
    "    address = 'B_Process/cache/tweet_data.pickle',\n",
    "    function = pull_tweet_data,\n",
    "    build_bool = set_cache.loc['collect_tweet_data', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36108c88-c0b5-468b-8633-d8b296c923c5",
   "metadata": {},
   "source": [
    "#### PULL02 - tabulate tweet statistics, divide users into train/tune/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be309202-ea90-4ca4-baea-e86c4c1b5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combile tweet count and verification status at the user level\n",
    "def enhance_user_tweet_data(td = tweet_data, ud = user_data):\n",
    "    \n",
    "    def np_size(x): return x.size\n",
    "    \n",
    "    \n",
    "    ## calculate tweet summary statistics\n",
    "    td_original = td\n",
    "    td = td[['screen_name', 'verified']].copy()\n",
    "    verified = td.groupby('screen_name').mean()\n",
    "    tweets = td['screen_name'].value_counts()\n",
    "    td = pd.concat([verified, tweets], axis = 1).reset_index()\n",
    "    td.columns = ['handle', 'verified', 'tweets']\n",
    "    \n",
    "    ## merge statistics into the user_data object\n",
    "    ud = pd.merge(ud, td, on = 'handle', how = 'left')\n",
    "    td_original = td_original.drop(['verified'], axis = 1)\n",
    "    ud = ud.drop(['url'], axis = 1).reset_index(drop = True)\n",
    "    ud = ud.fillna({'tweets': 0}).astype({'tweets': int})\n",
    "    \n",
    "    ## divide users into train, and test subsets\n",
    "    ml_set = pd.Series(['train', 'test'], name = 'ml_set').sample(\n",
    "                n = ud.shape[0], replace = True, weights = [0.8, 0.2],\n",
    "                random_state = 2006)\n",
    "    ud['ml_set'] = ml_set.values\n",
    "    ud.loc[ud.tweets == 0, 'ml_set'] = 'exclude'\n",
    "    \n",
    "    return ud, td_original\n",
    "\n",
    "## execute code\n",
    "user_data, tweet_data = enhance_user_tweet_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3baa5953-693f-4e92-baa9-dd794cf56689",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e43cd5-6a3b-4d7d-aeda-b7d1476d685e",
   "metadata": {},
   "source": [
    "## MUNG - Process Twitter data to model-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac2c0f-4a75-4ce0-a42c-61c2ed409d4f",
   "metadata": {},
   "source": [
    "#### MUNG01 - tokenize words and generate a word-level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a06fe9-1433-4b6b-86d5-a77fc09577dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize, remove capitalization, and remove duplicate tokens\n",
    "def nlp_tokenize_tweet(x):\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    x = list(set(x))\n",
    "    return x\n",
    "\n",
    "## create word/token level dataset and identify valid word tokens\n",
    "def make_word_data(td = tweet_data):\n",
    "    \n",
    "    ## tokenize tweet data text\n",
    "    td['tokens'] = td.full_text.apply(nlp_tokenize_tweet)\n",
    "\n",
    "    ## flatten token lists and count occurances\n",
    "    word_data = list()\n",
    "    for i in td.tokens:\n",
    "        word_data += i\n",
    "    word_data = pd.Series(word_data, name = 'count').value_counts()\n",
    "    word_data = word_data.sort_values(ascending = False)\n",
    "    word_data = pd.DataFrame(word_data)\n",
    "    \n",
    "    ## determine which tokens occur often enough to warrant inclusion\n",
    "    word_data['valid'] = word_data['count'] > max(\n",
    "        word_data['count'].quantile(0.2), 3)\n",
    "    word_data['word'] = word_data.index\n",
    "    \n",
    "    ## determine part of speech for eligible tokens\n",
    "    speech_part = word_data['word'].loc[word_data['valid']].values\n",
    "    speech_part = pos_tag(speech_part)\n",
    "    speech_part = [i[1][0].lower() for i in speech_part]\n",
    "    word_data['pos'] = '.'\n",
    "    word_data.loc[word_data['valid'], 'pos'] = speech_part\n",
    "    \n",
    "    ## lemmatize\n",
    "    WNL = WordNetLemmatizer()\n",
    "    word_data['token'] = None\n",
    "    for i in word_data.word:\n",
    "        if not word_data.loc[i, 'valid']: \n",
    "            break\n",
    "        if word_data.loc[i, 'pos'] in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            try:\n",
    "                word_data.loc[i, 'token'] = WNL.lemmatize(\n",
    "                    word_data.loc[i, 'word'],\n",
    "                    pos = word_data.loc[i, 'pos']\n",
    "                )\n",
    "            except:\n",
    "                word_data.loc[i, 'token'] = word_data.loc[i, 'word']\n",
    "        else:\n",
    "            word_data.loc[i, 'valid'] = False\n",
    "        \n",
    "    return word_data.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "word_data = execute_or_cache(\n",
    "    address = 'B_Process/cache/word_data.pickle',\n",
    "    function = make_word_data,\n",
    "    build_bool = set_cache.loc['rebuild_word_data', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de7bec-74ca-4843-91ac-4cd50344e3d4",
   "metadata": {},
   "source": [
    "#### MUNG03 - generate a tokens x tweets link database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1218c2d3-e6a0-4f8f-8827-d057afb05acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a linking dataset of words to tweets\n",
    "def make_tweet_token_data(td = tweet_data, wd = word_data):\n",
    "    \n",
    "    wd = wd.set_index('word')\n",
    "    \n",
    "    ## replicate tweet ids\n",
    "    n = td.tokens.apply(len).values\n",
    "    tweet_tokens = pd.Series(np.repeat(td.tweet_id.values, n), name = \"tweet_id\")\n",
    "    tweet_tokens = pd.DataFrame(tweet_tokens)\n",
    "    \n",
    "    ## allocate words to the new dataset\n",
    "    words = list()\n",
    "    for i in td.tokens:\n",
    "        words += i\n",
    "    tweet_tokens['words'] = words\n",
    "    \n",
    "    ## convert words to tokens\n",
    "    tweet_tokens['tokens'] = wd.loc[\n",
    "        tweet_tokens.words.values, 'token'].values\n",
    "    tweet_tokens = tweet_tokens.dropna()\n",
    "\n",
    "    return tweet_tokens.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "tweet_words = execute_or_cache(\n",
    "    address = 'B_Process/cache/tweet_words.pickle',\n",
    "    function = make_tweet_token_data,\n",
    "    build_bool = set_cache.loc['rebuild_tweet_words', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28aae1-7cc4-4c0e-9863-da2eb7335b0c",
   "metadata": {},
   "source": [
    "#### MUNG04 - generate a tokens x users count; drop tokens with only one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d0576bc-23d1-44e4-95d0-bac5f746dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregate tweet_words to be a matrix of word usage rates for each user\n",
    "def make_user_token_matrix(td = tweet_data, tw = tweet_words, ud = user_data):\n",
    "    \n",
    "    ## count of number of times each user wrote each token\n",
    "    tw = tw.merge(right = td[['screen_name', 'tweet_id']],\n",
    "                  how = 'left', on = 'tweet_id')\n",
    "    tw = tw.drop(['tweet_id', 'words'], axis = 1).groupby('screen_name')\n",
    "    tw = tw.value_counts()\n",
    "    tw.name = 'count'\n",
    "    tw = tw.reset_index().set_index('screen_name')\n",
    "    tw = tw.pivot(columns = 'tokens').fillna(0).astype(int)\n",
    "    tw = tw.droplevel(axis = 1, level = 0)\n",
    "    \n",
    "    ## remove tokens that fewer than 3 or more than 90 percent of users use\n",
    "    valid_usage = (tw > 0).astype(int).sum().values\n",
    "    valid_usage = (valid_usage > 2\n",
    "                  ) & (valid_usage < int(tw.shape[0] * 0.9))\n",
    "    tw = tw.loc[:, valid_usage]\n",
    "    \n",
    "    ## standardize matrix as words per 1,000 tweets\n",
    "    denom = pd.DataFrame({'handle': tw.index}).merge(\n",
    "        ud[['handle', 'tweets']],\n",
    "        how = 'left', on = 'handle'\n",
    "        ).set_index('handle').squeeze().fillna(1)\n",
    "    denom.loc[denom < 1] = 1\n",
    "    tw = (tw.divide(denom, axis = 0) * 1000).astype(int)\n",
    "    tw = tw.reset_index().rename({'level_0':'screen_name'}, axis = 1)\n",
    "    \n",
    "    return tw\n",
    "\n",
    "## execute code\n",
    "user_token_matrix = execute_or_cache(\n",
    "    address = 'B_Process/cache/user_token_matrix.pickle',\n",
    "    function = make_user_token_matrix,\n",
    "    build_bool = set_cache.loc['rebuild_user_token', 'bool']\n",
    "    ).set_index('screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238ebf22-bc91-4085-a201-81781a655a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4800e93-b052-4544-bb3a-8da62333d013",
   "metadata": {},
   "source": [
    "## MODE â€“ Train models and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44c208-88da-4fef-82ca-fc641db27133",
   "metadata": {},
   "source": [
    "#### MODE01 - build x/y train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c200c207-61a3-48b5-b50a-90355ed8dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unpack train and test datasets\n",
    "def split_xy_data(ml_cat):\n",
    "    i = user_data.loc[user_data.ml_set == ml_cat, 'handle'].values\n",
    "    \n",
    "    x = user_token_matrix.loc[i, :]\n",
    "    \n",
    "    y1 = user_data.set_index('handle').loc[i, 'group'] == 'USA House'\n",
    "    y1 = y1.astype(int)\n",
    "    \n",
    "    y2 = user_data.set_index('handle').loc[i, 'party'] == 'Republican'\n",
    "    y2 = y2.astype(int)\n",
    "    \n",
    "    return x, y1, y2\n",
    "\n",
    "## execute code\n",
    "train_x, train_y1, train_y2 = split_xy_data('train')\n",
    "test_x, test_y1, test_y2    = split_xy_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7af1b018-c0f0-4e93-baaf-39da987ca278",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop tweet_words to free up extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b0de3eb-75e9-421c-86c8-4d8b1edd4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweet_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0751295-c65b-45b3-81d5-05b2daf85ae0",
   "metadata": {},
   "source": [
    "#### MODE02 - formulate model database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efeb2a4b-b65e-4812-874e-52fd6263c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate dataset of all model variations to be tested\n",
    "def make_model_data():\n",
    "    \n",
    "    ## generate model formulations\n",
    "    model_data = dict()\n",
    "    for i in ['LR', 'NB', 'RF', 'AB']:\n",
    "        for j in ['1S', '2S']:\n",
    "            for k in ['FM', 'PC']:\n",
    "                l = i + j + k\n",
    "                model_data[l] = {'model': i, 'stage': j, 'x': k}\n",
    "    model_data = pd.DataFrame(model_data).T\n",
    "    \n",
    "    ## create slots for time and accuracy scores\n",
    "    model_data['precision'] = -1\n",
    "    model_data['recall']    = -1\n",
    "    model_data['f1']        = -1\n",
    "    model_data['time']      = -1\n",
    "    \n",
    "    ## generate lists of hyperparameters to test\n",
    "    model_params = {\n",
    "        'LR': {'C': [2. ** i for i in range(-2, 6)]},\n",
    "        'NB': {'priors': [(0.5, 0.5), (1 - train_y1.mean(), train_y1.mean()),\n",
    "                          (1 - train_y2.mean(), train_y2.mean())]},\n",
    "        'RF': {'n_estimators': [2**i for i in range(6, 10)],\n",
    "               'min_samples_leaf': [2**i for i in range(2, 5)]},\n",
    "        'AB':{'n_estimators':  [int(2**i) for i in range(6, 10)],\n",
    "              'learning_rate': [2**i for i in range(-2, 3)]},\n",
    "    }\n",
    "    model_data_params = list()\n",
    "    for i in model_data.index:\n",
    "        model_data_params.append(model_params[model_data.loc[i, 'model']])\n",
    "    model_data['params'] = model_data_params\n",
    "    \n",
    "    ## create slot for GridSearchCV object\n",
    "    model_data['object'] = [list() for i in model_data.index]\n",
    "    \n",
    "    ## return results\n",
    "    return model_data\n",
    "\n",
    "## execute data\n",
    "model_data = make_model_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b13acf-e25a-4d7e-8776-28fc48d497df",
   "metadata": {},
   "source": [
    "#### MODE03 - fit models and find best parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6f82d1c-2b8d-48f4-a876-1d9d35d6a34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model in progress: LR1SFM\n",
      "Model in progress: LR1SPC\n",
      "Model in progress: NB1SFM\n",
      "Model in progress: NB1SPC\n",
      "Model in progress: RF1SFM\n",
      "Model in progress: RF1SPC\n",
      "Model in progress: AB1SFM\n",
      "Model in progress: AB1SPC\n",
      "Model in progress: LR2SFM\n",
      "Model in progress: LR2SPC\n",
      "Model in progress: NB2SFM\n",
      "Model in progress: NB2SPC\n",
      "Model in progress: RF2SFM\n",
      "Model in progress: RF2SPC\n",
      "Model in progress: AB2SFM\n",
      "Model in progress: AB2SPC\n"
     ]
    }
   ],
   "source": [
    "def fit_1s_models(md, x, y1, y2):\n",
    "    \n",
    "    ## iterate through all one stage models\n",
    "    for i in md.index:\n",
    "        if md.loc[i, 'stage'] == '2S': continue\n",
    "        print('Model in progress: ' + i)\n",
    "        \n",
    "        ## FM v PC feature matrix\n",
    "        if md.loc[i, 'x'] == 'PC': pass\n",
    "        else:pass\n",
    "        \n",
    "        ## fit model\n",
    "        \n",
    "        ## extract statistics (precision, recall, f1, time elapsed)\n",
    "        \n",
    "    ## express object\n",
    "    return md\n",
    "        \n",
    "def fit_2s_models(md, x, y1, y2):\n",
    "    \n",
    "    ## iterate through all one stage models\n",
    "    for i in md.index:\n",
    "        if md.loc[i, 'stage'] == '1S': continue\n",
    "        print('Model in progress: ' + i)\n",
    "        \n",
    "        ## FM v PC feature matrix\n",
    "        if md.loc[i, 'x'] == 'PC': pass\n",
    "        else: pass\n",
    "        \n",
    "        ## fit stage 1 model\n",
    "        \n",
    "        ## fit stage 2 model\n",
    "        \n",
    "        ## extract statistics (precision, recall, f1, time elapsed)\n",
    "        \n",
    "    ## express object\n",
    "    return md\n",
    "\n",
    "def fit_all_models(md = model_data, x = train_x, y1 = train_y1, y2 = train_y2):\n",
    "    md = fit_1s_models(md = md, x = x, y1 = y1, y2 = y2)\n",
    "    md = fit_2s_models(md = md, x = x, y1 = y1, y2 = y2)\n",
    "    return md\n",
    "\n",
    "## execute data\n",
    "model_data = execute_or_cache(\n",
    "    address = 'B_Process/cache/model_data.pickle',\n",
    "    function = fit_all_models,\n",
    "    build_bool = set_cache.loc['rebuild_model_data', 'bool']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5504d3ef-8bff-44e3-868a-349bd8900191",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d22fc-5df4-4122-af1c-b7a003b158ab",
   "metadata": {},
   "source": [
    "## EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1628f7f6-11ff-4b4c-b95b-429ab156c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c9204e-5315-4616-92d2-f0be874745a2",
   "metadata": {},
   "source": [
    "## TOPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59913e1d-08c3-423e-b5fa-d2528ccc4023",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4baec78-0e62-4780-ac1e-b6cefc3bda42",
   "metadata": {},
   "source": [
    "## FOOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43416ed2-6493-407c-b2ad-a69cd011825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
