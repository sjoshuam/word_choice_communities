{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee32860-1ea4-4daa-98fc-291749307fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52c8a0-b7be-4019-878a-4e9ac1ff4895",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b339-098d-4b27-89a0-8c1167b5ac8a",
   "metadata": {},
   "source": [
    "#### HEAD 01 - toggle user settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18e3fdb-3cd1-47ca-94c6-0544e31fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine whether to cache data from some time consuming tasks\n",
    "## selecting True for any option increases run time and memory usage\n",
    "## selecting True for all options fully executes the code from start-to-finish\n",
    "settings = {\n",
    "    'num_parallel_cores': 13,\n",
    "    'full_data_mode': True, ## model all data, not just a sample for code dev\n",
    "    \n",
    "    'collect_data': True, ## toggles twitter api pulls in PULL01-03\n",
    "    \n",
    "    'rebuild_word_data': True, ## rebuild vs cache load for word_data MUNG02\n",
    "    'rebuild_tweet_words': True, ## rebuild vs cache load tweet_words MUNG03\n",
    "    'rebuild_user_token': True, ## rebuild v cache load user_token_tally MUNG04\n",
    "    \n",
    "    'rebuild_logistic_hparams': True, ## rebuild v cache ml training MODE04\n",
    "    'rebuild_bayes_hparams': True, ## rebuild v cache ml training MODE05\n",
    "    'rebuild_forest_hparams': True, ## rebuild v cache ml training MODE06\n",
    "    'rebuild_adaboost_hparams': True ## rebuild v cache ml training MODE07\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16158d71-339e-4026-93e9-b270cc45859d",
   "metadata": {},
   "source": [
    "#### HEAD02 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0914f0c2-3000-4136-94e5-630181f3e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from time import sleep\n",
    "from os.path import exists\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18441e2b-97b6-4243-a468-0de4c7e31258",
   "metadata": {},
   "source": [
    "#### HEAD03 - load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f766755-c82c-49a1-ae9a-05155a84f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s8/opt/anaconda3/envs/py310/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## read in roster of handles\n",
    "user_data = pd.read_excel(\"A_Input/twitter_handles.xlsx\")\n",
    "\n",
    "## drop all but a handful of cases if in test mode\n",
    "if not settings['full_data_mode']:\n",
    "    user_data = user_data.sample(125, random_state = 5542)\n",
    "\n",
    "## read in twitter credentials; initialize api connection+\n",
    "twitter_credentials = pd.read_csv('../api_keys/twitter.csv').set_index('item')\n",
    "twitter_credentials = tweepy.OAuth1UserHandler(\n",
    "    consumer_key = twitter_credentials.loc['API Key', 'string'],\n",
    "    consumer_secret = twitter_credentials.loc['API Key Secret', 'string'],\n",
    "    access_token = twitter_credentials.loc['Access Token', 'string'],\n",
    "   access_token_secret = twitter_credentials.loc['Access Token Secret', 'string']\n",
    "    )\n",
    "api = tweepy.API(twitter_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eace4-543f-4ff2-81cc-0eb3cc114c18",
   "metadata": {},
   "source": [
    "#### HEAD04 - create build or cache decision function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96445ddf-b3b5-4672-95d7-cc48563f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build switching function to execute code or cache results\n",
    "def build_or_cache_csv(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        x.to_csv(address, index = False)\n",
    "        return x\n",
    "    else:\n",
    "        return pd.read_csv(address)\n",
    "    \n",
    "def build_or_cache_pickle(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        conn = open(address, 'wb')\n",
    "        pickle.dump(x, conn)\n",
    "        conn.close()\n",
    "        return x\n",
    "    else:\n",
    "        conn = open(address, 'rb')\n",
    "        x = pickle.load(conn)\n",
    "        conn.close()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fd8c1f-1a4e-4dca-a2de-100adffbceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed49a6b-8b72-4edd-84b8-0f815986fc01",
   "metadata": {},
   "source": [
    "## HAND â€“ Gather Twitter handles for test accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67daeb1-100d-4fc1-9c20-f28ed2c1ec0a",
   "metadata": {},
   "source": [
    "#### HAND01 - extract handles from roster URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdda14bb-55b3-41d9-b735-b674367eee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract handles from roster urls\n",
    "user_data['handle'] = user_data.url.str.replace('https://twitter.com/', '',\n",
    "            regex = False).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db8ac6c-b02c-454a-ae37-cf1cc444b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e64b95-7e26-4789-b736-089706334fad",
   "metadata": {},
   "source": [
    "## PULL - Pull Twitter data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9531973-8911-4beb-a48b-1c7989ae2428",
   "metadata": {},
   "source": [
    "#### PULL01 - query API for each roster handle's user_timeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1200e6ca-cf86-412a-bdf2-da760105698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tweet data from api object\n",
    "def refine_tweet_data(x):\n",
    "    tweet_data = list()\n",
    "    for i in range(0, len(x)):\n",
    "        tweet_data.append({\n",
    "            'tweet_id': x[i].id, \n",
    "            'created_at': x[i].created_at, \n",
    "            'lang': x[i].lang,\n",
    "            'full_text': x[i].full_text,\n",
    "            'screen_name': x[i].author.screen_name,\n",
    "            'verified' : x[i].author.verified\n",
    "        })\n",
    "    return pd.DataFrame(tweet_data)\n",
    "\n",
    "## define function to pull user tweet data and apply function to extract tweet data\n",
    "def pull_tweet_data(handles = user_data.handle, a = api):\n",
    "    tweet_data = list()\n",
    "    for i in handles:\n",
    "        try:\n",
    "            user_tweets = a.user_timeline(\n",
    "                screen_name = i, count = 200, tweet_mode = 'extended', \n",
    "                exclude_replies = True, include_rts = False)\n",
    "            tweet_data.append(refine_tweet_data(user_tweets))\n",
    "            sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_data = pd.concat(tweet_data)\n",
    "    tweet_data['screen_name'] = tweet_data['screen_name'].str.lower()\n",
    "    return tweet_data\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    tweet_data = pull_tweet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7301b7-3283-45c5-8943-1444d0e8cf27",
   "metadata": {},
   "source": [
    "#### PULL02 - tabulate tweet statistics, divide users into train/tune/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f26d7ac-312e-4445-a22a-0f8bada0d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combile tweet count and verification status at the user level\n",
    "def enhance_user_data(td, ud = user_data):\n",
    "    \n",
    "    def np_size(x): return x.size\n",
    "    \n",
    "    ## calculate tweet summary statistics\n",
    "    td_original = tweet_data\n",
    "    td = td.copy()\n",
    "    verified = td.groupby('screen_name').mean()\n",
    "    tweets = td['screen_name'].value_counts()\n",
    "    td = pd.concat([verified, tweets], axis = 1).reset_index()\n",
    "    td.columns = ['handle', 'verified', 'tweets']\n",
    "    \n",
    "    ## merge statistics into the user_data object\n",
    "    ud = pd.merge(ud, td, on = 'handle', how = 'left')\n",
    "    td_original = td_original.drop(['verified'], axis = 1)\n",
    "    ud = ud.drop(['url'], axis = 1).reset_index(drop = True)\n",
    "    ud = ud.fillna({'tweets': 0}).astype({'tweets': int})\n",
    "    \n",
    "    ## divide users into train, and test subsets\n",
    "    ml_set = pd.Series(['train', 'test'], name = 'ml_set').sample(\n",
    "                n = ud.shape[0], replace = True, weights = [0.8, 0.20],\n",
    "                random_state = 2006)\n",
    "    ud['ml_set'] = ml_set.values\n",
    "    ud.loc[ud.tweets == 0, 'ml_set'] = 'exclude'\n",
    "    \n",
    "    return ud, td_original\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    user_data, tweet_data = enhance_user_data(\n",
    "        tweet_data[['screen_name', 'verified']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6e488-00b0-4257-a07b-598824c2aa18",
   "metadata": {},
   "source": [
    "#### PULL03 - save datasets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb27d14-f5e0-40ef-beac-2f6d369a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save user/tweet datasets to disk as csvs\n",
    "if settings['collect_data']:\n",
    "    user_data.to_csv('B_Process/user_data.csv', index = False)\n",
    "    tweet_data.to_csv('B_Process/tweet_data.csv', index = False)\n",
    "else:\n",
    "    user_data = pd.read_csv('B_Process/user_data.csv')\n",
    "    tweet_data = pd.read_csv('B_Process/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab22ae2d-8e04-4fa8-91de-70498694d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1152f-f55d-4758-a9d1-78e30499f55f",
   "metadata": {},
   "source": [
    "## MUNG - Process Twitter data to model-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fdb93-7acf-43e0-9ae9-3ae2eb6a749e",
   "metadata": {},
   "source": [
    "#### MUNG01 - parse tweet text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3362904b-047d-44ef-9497-12af5a5421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize, remove capitalization, and remove duplicate tokens\n",
    "def nlp_tokenize_tweet(x):\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    x = list(set(x))\n",
    "    return x\n",
    "\n",
    "## execute code\n",
    "tweet_data['tokens'] = tweet_data.full_text.apply(nlp_tokenize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422810c-53d2-4b5e-b9e8-68457e82756b",
   "metadata": {},
   "source": [
    "#### MUNG02 - create word/token level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f7a6af-6786-469b-8ec6-f07fb6d31709",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create word/token level dataset and identify valid word tokens\n",
    "def make_word_data(td = tweet_data):\n",
    "\n",
    "    ## flatten token lists and count occurances\n",
    "    word_data = list()\n",
    "    for i in td.tokens:\n",
    "        word_data += i\n",
    "    word_data = pd.Series(word_data, name = 'count').value_counts()\n",
    "    word_data = word_data.sort_values(ascending = False)\n",
    "    word_data = pd.DataFrame(word_data)\n",
    "    \n",
    "    ## determine which tokens occur often enough to warrant inclusion\n",
    "    word_data['valid'] = word_data['count'] > max(\n",
    "        word_data['count'].quantile(0.2), 3)\n",
    "    word_data['word'] = word_data.index\n",
    "    \n",
    "    ## determine part of speech for eligible tokens\n",
    "    speech_part = word_data['word'].loc[word_data['valid']].values\n",
    "    speech_part = pos_tag(speech_part)\n",
    "    speech_part = [i[1][0].lower() for i in speech_part]\n",
    "    word_data['pos'] = '.'\n",
    "    word_data.loc[word_data['valid'], 'pos'] = speech_part\n",
    "    \n",
    "    ## lemmatize\n",
    "    WNL = WordNetLemmatizer()\n",
    "    word_data['token'] = None\n",
    "    for i in word_data.word:\n",
    "        if not word_data.loc[i, 'valid']: \n",
    "            break\n",
    "        if word_data.loc[i, 'pos'] in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            try:\n",
    "                word_data.loc[i, 'token'] = WNL.lemmatize(\n",
    "                    word_data.loc[i, 'word'],\n",
    "                    pos = word_data.loc[i, 'pos']\n",
    "                )\n",
    "            except:\n",
    "                word_data.loc[i, 'token'] = word_data.loc[i, 'word']\n",
    "        else:\n",
    "            word_data.loc[i, 'valid'] = False\n",
    "        \n",
    "    return word_data.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "word_data = build_or_cache_csv(\n",
    "    address = 'B_Process/word_data.csv',\n",
    "    function = make_word_data,\n",
    "    build_bool = settings['rebuild_word_data']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419c4d1-dada-48f5-9bcd-252137c407b5",
   "metadata": {},
   "source": [
    "#### MUNG03 - generate a tokens x tweets link database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247437d5-61e4-4143-93b2-cc66edf8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tweet_token_data(td = tweet_data, wd = word_data):\n",
    "    \n",
    "    wd = wd.set_index('word')\n",
    "    \n",
    "    ## replicate tweet ids\n",
    "    n = td.tokens.apply(len).values\n",
    "    tweet_tokens = pd.Series(np.repeat(td.tweet_id.values, n), name = \"tweet_id\")\n",
    "    tweet_tokens = pd.DataFrame(tweet_tokens)\n",
    "    \n",
    "    ## allocate words to the new dataset\n",
    "    words = list()\n",
    "    for i in td.tokens:\n",
    "        words += i\n",
    "    tweet_tokens['words'] = words\n",
    "    \n",
    "    ## convert words to tokens\n",
    "    tweet_tokens['tokens'] = wd.loc[\n",
    "        tweet_tokens.words.values, 'token'].values\n",
    "    tweet_tokens = tweet_tokens.dropna()\n",
    "\n",
    "    return tweet_tokens.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "tweet_words = build_or_cache_csv(\n",
    "    address = 'B_Process/tweet_words.csv',\n",
    "    function = make_tweet_token_data,\n",
    "    build_bool = settings['rebuild_tweet_words']\n",
    "    )\n",
    "tweet_data = tweet_data.drop('tokens', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5cef3-bc88-4f27-9a5e-f76ce06d149c",
   "metadata": {},
   "source": [
    "#### MUNG04 - generate a tokens x users count; drop tokens with only one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bc82f94-94cf-4030-a5d9-b3f5e5bbd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_token_matrix(td = tweet_data, tw = tweet_words, ud = user_data):\n",
    "    \n",
    "    ## count of number of times each user wrote each token\n",
    "    tw = tw.merge(right = td[['screen_name', 'tweet_id']],\n",
    "                  how = 'left', on = 'tweet_id')\n",
    "    tw = tw.drop(['tweet_id', 'words'], axis = 1).groupby('screen_name')\n",
    "    tw = tw.value_counts()\n",
    "    tw.name = 'count'\n",
    "    tw = tw.reset_index().set_index('screen_name')\n",
    "    tw = tw.pivot(columns = 'tokens').fillna(0).astype(int)\n",
    "    tw = tw.droplevel(axis = 1, level = 0)\n",
    "    \n",
    "    ## remove tokens that fewer than 3 or more than 80 percent of users use\n",
    "    valid_usage = (tw > 0).astype(int).sum().values\n",
    "    valid_usage = (valid_usage > 2\n",
    "                  ) & (valid_usage < int(tw.shape[0] * 0.8))\n",
    "    tw = tw.loc[:, valid_usage]\n",
    "    \n",
    "    ## standardize matrix as words per 1,000 tweets\n",
    "    denom = pd.DataFrame({'handle': tw.index}).merge(\n",
    "        ud[['handle', 'tweets']],\n",
    "        how = 'left', on = 'handle'\n",
    "        ).set_index('handle').squeeze().fillna(1)\n",
    "    denom.loc[denom < 1] = 1\n",
    "    tw = (tw.divide(denom, axis = 0) * 1000).astype(int)\n",
    "    tw = tw.reset_index().rename({'level_0':'screen_name'}, axis = 1)\n",
    "    \n",
    "    return tw\n",
    "\n",
    "\n",
    "## execute code\n",
    "user_token_matrix = build_or_cache_csv(\n",
    "    address = 'B_Process/user_token_matrix.csv',\n",
    "    function = make_user_token_matrix,\n",
    "    build_bool = settings['rebuild_user_token']\n",
    "    ).set_index('screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2591e88-3b00-4e6a-a199-bb129120db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0ad36-2fff-4c0f-8b41-a0dba7c3d20a",
   "metadata": {},
   "source": [
    "## HYPE â€“ Train models and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4089e-17c3-4502-b726-889d0abb5668",
   "metadata": {},
   "source": [
    "#### HYPE01 - build x/y train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0bc61b5-b3fd-407e-b340-0f6e9b504f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unpack train and test datasets\n",
    "def split_xy_data(ml_cat):\n",
    "    i = user_data.loc[user_data.ml_set == ml_cat, 'handle'].values\n",
    "    \n",
    "    x = user_token_matrix.loc[i, :]\n",
    "    \n",
    "    y1 = user_data.set_index('handle').loc[i, 'group'] == 'USA House'\n",
    "    y1 = y1.astype(int)\n",
    "    \n",
    "    y2 = user_data.set_index('handle').loc[i, 'party'] == 'Republican'\n",
    "    y2 = y2.astype(int)\n",
    "    \n",
    "    return x, y1, y2\n",
    "\n",
    "## execute code\n",
    "train_x, train_y1, train_y2 = split_xy_data('train')\n",
    "test_x, test_y1, test_y2    = split_xy_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "861ddd9a-b469-45ae-a00c-e169aeae04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete objects to clear up memory in prep for modeling\n",
    "del tweet_words, tweet_data, user_token_matrix, word_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ffc07-371d-4d92-a7d5-909291aca128",
   "metadata": {},
   "source": [
    "#### HYPE02 - build pca versions of x dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f0598e9-3004-4e91-a861-81871cfceb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit pca model in order to simplify and enhance feature matrix\n",
    "model_pca = PCA().fit(train_x)\n",
    "\n",
    "## generate pca transformations of all feature matrices\n",
    "train_x_pca = model_pca.transform(train_x)\n",
    "test_x_pca  = model_pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350d0f8-56f0-4784-add3-2e2f4bd161c6",
   "metadata": {},
   "source": [
    "#### HYPE03 - formulate generic function for hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b888b789-65dc-4d69-8a55-d44d163da978",
   "metadata": {},
   "outputs": [],
   "source": [
    "## find best hyperparameters\n",
    "def find_hparams():\n",
    "    model1 = the_model.fit(X = train_x, y = train_y1)\n",
    "    model2 = the_model.fit(X = train_x, y = train_y2, sample_weight = train_y1)\n",
    "    model3 = the_model.fit(X = train_x_pca, y = train_y1)\n",
    "    model4 = the_model.fit(\n",
    "        X = train_x_pca, y = train_y2, sample_weight = train_y1)\n",
    "    return {'feature_stage1': model1, 'feature_stage2': model2,\n",
    "            'pca_stage1': model3, 'pca_stage2': model4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36dfe9-ab90-466f-937b-551d7c03b217",
   "metadata": {},
   "source": [
    "#### HYPE04 - find best logistic regression model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea55281c-08ee-474d-8a95-34b4e55ee067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = LogisticRegression(penalty = 'l1',\n",
    "                                   class_weight = 'balanced',\n",
    "                                   solver = 'saga',\n",
    "                                   max_iter = 2**10\n",
    "                                  ),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {'C': [2. ** i for i in range(-2, 6)]}\n",
    ")\n",
    "\n",
    "## execute code\n",
    "logistic_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/logistic_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_logistic_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202253d7-59ce-4881-bfdd-50a8762076fc",
   "metadata": {},
   "source": [
    "#### HYPE05 - find best naive bayes model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "221dab0d-02fa-4753-808a-1e313182cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = GaussianNB(),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {'priors': [\n",
    "        (0.5, 0.5), \n",
    "        (1 - train_y1.mean(), train_y1.mean()),\n",
    "        (1 - train_y2.mean(), train_y2.mean()),\n",
    "    ]}\n",
    ")\n",
    "\n",
    "## execute code\n",
    "bayes_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/bayes_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_bayes_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c432aa43-4dc0-42d0-8734-8d829c3fec20",
   "metadata": {},
   "source": [
    "#### HYPE06 - find best random forest model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f65fbd-06ab-4b79-af98-c02347bf84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = RandomForestClassifier(class_weight = 'balanced'),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {\n",
    "        'n_estimators': [2**i for i in range(6, 10)],\n",
    "        'min_samples_leaf': [2**i for i in range(2, 5)]\n",
    "    }\n",
    ")\n",
    "\n",
    "## execute code\n",
    "forest_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/forest_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_forest_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638e51f-6dd7-4160-be9f-60e7712b0151",
   "metadata": {},
   "source": [
    "#### HYPE07 - find best AdaBoost model parameters and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c56ff21e-a554-438f-b235-d590a907b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## formulate model\n",
    "the_model = GridSearchCV( \n",
    "    estimator = AdaBoostClassifier(base_estimator = None),\n",
    "    scoring = 'f1',\n",
    "    cv = 5, \n",
    "    n_jobs = settings['num_parallel_cores'], \n",
    "    param_grid = {\n",
    "        'n_estimators':  [int(2**i) for i in range(6, 10)],\n",
    "        'learning_rate': [2**i for i in range(-2, 3)]\n",
    "    }\n",
    ")\n",
    "\n",
    "## execute code\n",
    "adaboost_hparams = build_or_cache_pickle(\n",
    "    address = 'B_Process/models/adaboost_hparams.pickle',\n",
    "    function = find_hparams,\n",
    "    build_bool = settings['rebuild_adaboost_hparams']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64189160-c590-484d-b783-02e4a1ba21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550df932-9da0-4c4e-bd8c-bc40e94e4edb",
   "metadata": {},
   "source": [
    "## EVAL - Analyse model performance and interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0d58d-ab83-481b-9438-1cb96d8c1337",
   "metadata": {},
   "source": [
    "#### EVAL01 - compile hyperparameter search statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8d8b6-6647-4e6d-8cb9-471dbe8ef024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720e2165-76fc-4f27-ae2c-16a14c11b564",
   "metadata": {},
   "source": [
    "#### EVAL02 - compile best model performance statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d7c03-b71a-47ab-a541-f2409aaa30d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cccf932-9336-4754-a13e-1480e74e5172",
   "metadata": {},
   "source": [
    "#### EVAL03 - calculate comparison performance stats for random guess model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800cf523-e288-47ec-afac-ba6ef0f5ce38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d26766d6-eb06-4383-829c-79beac7273b7",
   "metadata": {},
   "source": [
    "#### EVAL04 - determine variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2407b-a48a-49ae-8285-0dfbdbb8e176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76c8e72e-fab2-4102-ac95-d77b46bac024",
   "metadata": {},
   "source": [
    "#### EVAL05 - score tweets based on variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476758b-f934-4c23-9b2a-8564b3fce5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0866f8ad-2048-4b91-8245-f1df2c76ae6d",
   "metadata": {},
   "source": [
    "#### EVAL06 - extract softmax scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfd903-8c89-4032-a007-7ae5fb7ab234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6cff4-877e-45d0-8c69-107e8defb4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f9ec1b-cea5-4168-9a5e-d4767a436101",
   "metadata": {},
   "source": [
    "## FOOT - display objects as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82de73-7c50-478e-acf7-d9f61b650d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f5a67-38da-4aa4-95dd-3fef0f035e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
