{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc52c8a0-b7be-4019-878a-4e9ac1ff4895",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b339-098d-4b27-89a0-8c1167b5ac8a",
   "metadata": {},
   "source": [
    "#### HEAD 01 - toggle user settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18e3fdb-3cd1-47ca-94c6-0544e31fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine whether to cache data from some time consuming tasks\n",
    "settings = {\n",
    "    'test_mode': False, ## when true, only a small of data is collected\n",
    "    'collect_data': False, ## toggles twitter api pulls in PULL01-03\n",
    "    'rebuild_word_data': True, ## rebuild vs cache load for word_data MUNG02\n",
    "    'rebuild_tweet_words': True, ## rebuild vs cache load tweet_words MUNG03\n",
    "    'rebuild_user_token': True ## rebuild vs cache load user_token_tally MUNG04\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16158d71-339e-4026-93e9-b270cc45859d",
   "metadata": {},
   "source": [
    "#### HEAD02 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0914f0c2-3000-4136-94e5-630181f3e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########==========\n",
    "import tweepy\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from os.path import exists\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18441e2b-97b6-4243-a468-0de4c7e31258",
   "metadata": {},
   "source": [
    "#### HEAD03 - load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f766755-c82c-49a1-ae9a-05155a84f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s8/opt/anaconda3/envs/py310/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## read in roster of handles\n",
    "user_data = pd.read_excel(\"A_Input/twitter_handles.xlsx\")\n",
    "\n",
    "## drop all but a handful of cases if in test mode\n",
    "if settings['test_mode']:\n",
    "    user_data = user_data.sample(100, random_state = 5542)\n",
    "\n",
    "## read in twitter credentials; initialize api connection+\n",
    "twitter_credentials = pd.read_csv('../api_keys/twitter.csv').set_index('item')\n",
    "twitter_credentials = tweepy.OAuth1UserHandler(\n",
    "    consumer_key = twitter_credentials.loc['API Key', 'string'],\n",
    "    consumer_secret = twitter_credentials.loc['API Key Secret', 'string'],\n",
    "    access_token = twitter_credentials.loc['Access Token', 'string'],\n",
    "   access_token_secret = twitter_credentials.loc['Access Token Secret', 'string']\n",
    "    )\n",
    "api = tweepy.API(twitter_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eace4-543f-4ff2-81cc-0eb3cc114c18",
   "metadata": {},
   "source": [
    "#### HEAD04 - create build or cache decision function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96445ddf-b3b5-4672-95d7-cc48563f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build switching function to execute code or cache results\n",
    "def build_or_cache(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        x.to_csv(address, index = False)\n",
    "        return x\n",
    "    else:\n",
    "        return pd.read_csv(address)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed49a6b-8b72-4edd-84b8-0f815986fc01",
   "metadata": {},
   "source": [
    "## HAND – Gather Twitter handles for test accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67daeb1-100d-4fc1-9c20-f28ed2c1ec0a",
   "metadata": {},
   "source": [
    "#### HAND01 - extract handles from roster URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdda14bb-55b3-41d9-b735-b674367eee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract handles from roster urls\n",
    "user_data['handle'] = user_data.url.str.replace('https://twitter.com/', '',\n",
    "            regex = False).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e64b95-7e26-4789-b736-089706334fad",
   "metadata": {},
   "source": [
    "## PULL - Pull Twitter data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9531973-8911-4beb-a48b-1c7989ae2428",
   "metadata": {},
   "source": [
    "#### PULL01 - query API for each roster handle's user_timeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1200e6ca-cf86-412a-bdf2-da760105698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tweet data from api object\n",
    "def refine_tweet_data(x):\n",
    "    tweet_data = list()\n",
    "    for i in range(0, len(x)):\n",
    "        tweet_data.append({\n",
    "            'tweet_id': x[i].id, 'created_at': x[i].created_at, 'lang': x[i].lang,\n",
    "            'full_text': x[i].full_text,\n",
    "            'screen_name': x[i].author.screen_name,\n",
    "            'verified' : x[i].author.verified\n",
    "        })\n",
    "    return pd.DataFrame(tweet_data)\n",
    "\n",
    "## define function to pull user tweet data and apply function to extract tweet data\n",
    "def pull_tweet_data(handles = user_data.handle, a = api):\n",
    "    tweet_data = list()\n",
    "    for i in handles:\n",
    "        try:\n",
    "            user_tweets = a.user_timeline(\n",
    "                screen_name = i, count = 200, tweet_mode = 'extended', \n",
    "                exclude_replies = True, include_rts = False)\n",
    "            tweet_data.append(refine_tweet_data(user_tweets))\n",
    "            sleep(0.5)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_data = pd.concat(tweet_data)\n",
    "    tweet_data['screen_name'] = tweet_data['screen_name'].str.lower()\n",
    "    return tweet_data\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    tweet_data = pull_tweet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7301b7-3283-45c5-8943-1444d0e8cf27",
   "metadata": {},
   "source": [
    "#### PULL02 - tabulate tweet statistics, divide users into train/tune/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f26d7ac-312e-4445-a22a-0f8bada0d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combile tweet count and verification status at the user level\n",
    "def enhance_user_data(td, ud = user_data):\n",
    "    \n",
    "    def np_size(x): return x.size\n",
    "    \n",
    "    ## calculate tweet summary statistics\n",
    "    td_original = tweet_data\n",
    "    td = td.copy()\n",
    "    verified = td.groupby('screen_name').mean()\n",
    "    tweets = td['screen_name'].value_counts()\n",
    "    td = pd.concat([verified, tweets], axis = 1).reset_index()\n",
    "    td.columns = ['handle', 'verified', 'tweets']\n",
    "    \n",
    "    ## merge statistics into the user_data object\n",
    "    ud = pd.merge(ud, td, on = 'handle', how = 'left')\n",
    "    td_original = td_original.drop(['verified'], axis = 1)\n",
    "    ud = ud.drop(['url'], axis = 1).reset_index(drop = True)\n",
    "    ud = ud.fillna({'tweets': 0}).astype({'tweets': int})\n",
    "    \n",
    "    ## divide users into train, tune, and test subsets\n",
    "    ml_set = pd.Series(['train', 'tune', 'test'], name = 'ml_set').sample(\n",
    "                n = ud.shape[0], replace = True, weights = [0.7, 0.15, 0.15],\n",
    "                random_state = 2006)\n",
    "    ud['ml_set'] = ml_set.values\n",
    "    ud.loc[ud.tweets == 0, 'ml_set'] = 'exclude'\n",
    "    \n",
    "    return ud, td_original\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    user_data, tweet_data = enhance_user_data(\n",
    "        tweet_data[['screen_name', 'verified']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6e488-00b0-4257-a07b-598824c2aa18",
   "metadata": {},
   "source": [
    "#### PULL03 - save datasets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb27d14-f5e0-40ef-beac-2f6d369a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save user/tweet datasets to disk as csvs\n",
    "if settings['collect_data']:\n",
    "    user_data.to_csv('B_Process/user_data.csv', index = False)\n",
    "    tweet_data.to_csv('B_Process/tweet_data.csv', index = False)\n",
    "else:\n",
    "    user_data = pd.read_csv('B_Process/user_data.csv')\n",
    "    tweet_data = pd.read_csv('B_Process/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1152f-f55d-4758-a9d1-78e30499f55f",
   "metadata": {},
   "source": [
    "## MUNG - Process Twitter data to model-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fdb93-7acf-43e0-9ae9-3ae2eb6a749e",
   "metadata": {},
   "source": [
    "#### MUNG01 - parse tweet text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3362904b-047d-44ef-9497-12af5a5421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize, remove capitalization, and remove duplicate tokens\n",
    "def nlp_tokenize_tweet(x):\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    x = list(set(x))\n",
    "    return x\n",
    "\n",
    "## execute code\n",
    "tweet_data['tokens'] = tweet_data.full_text.apply(nlp_tokenize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422810c-53d2-4b5e-b9e8-68457e82756b",
   "metadata": {},
   "source": [
    "#### MUNG02 - create word/token level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f7a6af-6786-469b-8ec6-f07fb6d31709",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create word/token level dataset and identify valid word tokens\n",
    "def make_word_data(td = tweet_data):\n",
    "\n",
    "    ## flatten token lists and count occurances\n",
    "    word_data = list()\n",
    "    for i in td.tokens:\n",
    "        word_data += i\n",
    "    word_data = pd.Series(word_data, name = 'count').value_counts()\n",
    "    word_data = word_data.sort_values(ascending = False)\n",
    "    word_data = pd.DataFrame(word_data)\n",
    "    \n",
    "    ## determine which tokens occur often enough to warrant inclusion\n",
    "    word_data['valid'] = word_data['count'] > max(\n",
    "        word_data['count'].quantile(0.2), 3)\n",
    "    word_data['word'] = word_data.index\n",
    "    \n",
    "    ## determine part of speech for eligible tokens\n",
    "    speech_part = word_data['word'].loc[word_data['valid']].values\n",
    "    speech_part = pos_tag(speech_part)\n",
    "    speech_part = [i[1][0].lower() for i in speech_part]\n",
    "    word_data['pos'] = '.'\n",
    "    word_data.loc[word_data['valid'], 'pos'] = speech_part\n",
    "    \n",
    "    ## lemmatize\n",
    "    WNL = WordNetLemmatizer()\n",
    "    word_data['token'] = None\n",
    "    for i in word_data.word:\n",
    "        if not word_data.loc[i, 'valid']: \n",
    "            break\n",
    "        if word_data.loc[i, 'pos'] in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            try:\n",
    "                word_data.loc[i, 'token'] = WNL.lemmatize(\n",
    "                    word_data.loc[i, 'word'],\n",
    "                    pos = word_data.loc[i, 'pos']\n",
    "                )\n",
    "            except:\n",
    "                word_data.loc[i, 'token'] = word_data.loc[i, 'word']\n",
    "        else:\n",
    "            word_data.loc[i, 'valid'] = False\n",
    "        \n",
    "    return word_data.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "word_data = build_or_cache(\n",
    "    address = 'B_Process/word_data.csv',\n",
    "    function = make_word_data,\n",
    "    build_bool = settings['rebuild_word_data']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419c4d1-dada-48f5-9bcd-252137c407b5",
   "metadata": {},
   "source": [
    "#### MUNG03 - generate a tokens x tweets link database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247437d5-61e4-4143-93b2-cc66edf8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tweet_token_data(td = tweet_data, wd = word_data):\n",
    "    \n",
    "    wd = wd.set_index('word')\n",
    "    \n",
    "    ## replicate tweet ids\n",
    "    n = td.tokens.apply(len).values\n",
    "    tweet_tokens = pd.Series(np.repeat(td.tweet_id.values, n), name = \"tweet_id\")\n",
    "    tweet_tokens = pd.DataFrame(tweet_tokens)\n",
    "    \n",
    "    ## allocate words to the new dataset\n",
    "    words = list()\n",
    "    for i in td.tokens:\n",
    "        words += i\n",
    "    tweet_tokens['words'] = words\n",
    "    \n",
    "    ## convert words to tokens\n",
    "    tweet_tokens['tokens'] = wd.loc[\n",
    "        tweet_tokens.words.values, 'token'].values\n",
    "    tweet_tokens = tweet_tokens.dropna()\n",
    "\n",
    "    return tweet_tokens.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "tweet_words = build_or_cache(\n",
    "    address = 'B_Process/tweet_words.csv',\n",
    "    function = make_tweet_token_data,\n",
    "    build_bool = settings['rebuild_tweet_words']\n",
    "    )\n",
    "tweet_data = tweet_data.drop('tokens', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5cef3-bc88-4f27-9a5e-f76ce06d149c",
   "metadata": {},
   "source": [
    "#### MUNG04 - generate a tokens x users count; drop tokens with only one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc82f94-94cf-4030-a5d9-b3f5e5bbd945",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['screen_name'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m user_token_matrix \u001b[38;5;241m=\u001b[39m build_or_cache(\n\u001b[1;32m     32\u001b[0m     address \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB_Process/user_token_matrix.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m     function \u001b[38;5;241m=\u001b[39m make_user_token_matrix,\n\u001b[1;32m     34\u001b[0m     build_bool \u001b[38;5;241m=\u001b[39m settings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrebuild_user_token\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m~\u001b[39msettings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrebuild_user_token\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 37\u001b[0m     user_token_matrix \u001b[38;5;241m=\u001b[39m \u001b[43muser_token_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscreen_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py310/lib/python3.10/site-packages/pandas/core/frame.py:5494\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5491\u001b[0m                 missing\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[1;32m   5493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 5494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are in the columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   5497\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['screen_name'] are in the columns\""
     ]
    }
   ],
   "source": [
    "def make_user_token_matrix(td = tweet_data, tw = tweet_words, ud = user_data):\n",
    "    \n",
    "    ## count of number of times each user wrote each token\n",
    "    tw = tw.merge(right = td[['screen_name', 'tweet_id']],\n",
    "                  how = 'left', on = 'tweet_id')\n",
    "    tw = tw.drop(['tweet_id', 'words'], axis = 1).groupby('screen_name')\n",
    "    tw = tw.value_counts()\n",
    "    tw.name = 'count'\n",
    "    tw = tw.reset_index().set_index('screen_name')\n",
    "    tw = tw.pivot(columns = 'tokens').fillna(0).astype(int)\n",
    "    tw = tw.droplevel(axis = 1, level = 0)\n",
    "    \n",
    "    ## remove tokens that fewer than 3 or more than 80 percent of users use\n",
    "    valid_usage = (tw > 0).astype(int).sum().values\n",
    "    valid_usage = (valid_usage > 2\n",
    "                  ) & (valid_usage < int(tw.shape[0] * 0.8))\n",
    "    tw = tw.loc[:, valid_usage]\n",
    "    \n",
    "    ## standardize matrix as words per 1,000 tweets\n",
    "    denom = pd.DataFrame({'handle': tw.index}).merge(\n",
    "        ud[['handle', 'tweets']],\n",
    "        how = 'left', on = 'handle'\n",
    "        ).set_index('handle').squeeze().fillna(1)\n",
    "    denom.loc[denom < 1] = 1\n",
    "    tw = (tw.divide(denom, axis = 0) * 1000).astype(int)\n",
    "    \n",
    "    return tw.reset_index()\n",
    "\n",
    "\n",
    "## execute code\n",
    "user_token_matrix = build_or_cache(\n",
    "    address = 'B_Process/user_token_matrix.csv',\n",
    "    function = make_user_token_matrix,\n",
    "    build_bool = settings['rebuild_user_token']\n",
    "    ).set_index('screen_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689cb1ec-896a-406e-a048-f036bfa81e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1552727930408734720</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1552727930408734720</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1552727930408734720</td>\n",
       "      <td>https</td>\n",
       "      <td>http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1552727930408734720</td>\n",
       "      <td>photos</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1552727930408734720</td>\n",
       "      <td>@</td>\n",
       "      <td>@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960509</th>\n",
       "      <td>1506387676655759365</td>\n",
       "      <td>family</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960510</th>\n",
       "      <td>1506387676655759365</td>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960511</th>\n",
       "      <td>1506387676655759365</td>\n",
       "      <td>alaskans</td>\n",
       "      <td>alaskans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960512</th>\n",
       "      <td>1506387676655759365</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960513</th>\n",
       "      <td>1506387676655759365</td>\n",
       "      <td>many</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2960514 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tweet_id     words    tokens\n",
       "0        1552727930408734720       the       the\n",
       "1        1552727930408734720   african   african\n",
       "2        1552727930408734720     https      http\n",
       "3        1552727930408734720    photos     photo\n",
       "4        1552727930408734720         @         @\n",
       "...                      ...       ...       ...\n",
       "2960509  1506387676655759365    family    family\n",
       "2960510  1506387676655759365      from      from\n",
       "2960511  1506387676655759365  alaskans  alaskans\n",
       "2960512  1506387676655759365        to        to\n",
       "2960513  1506387676655759365      many      many\n",
       "\n",
       "[2960514 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TODO: fix missing screen_name issue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0ad36-2fff-4c0f-8b41-a0dba7c3d20a",
   "metadata": {},
   "source": [
    "## TRAI – Train models and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bfc08-9fa5-45b5-9136-f5668b1c00e1",
   "metadata": {},
   "source": [
    "##### reminders\n",
    "##### - standardize data\n",
    "##### - write a two-stage meta-function\n",
    "##### - do automatic feature selection for regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db026d-6e08-43d1-9c7e-7d49128c501f",
   "metadata": {},
   "source": [
    "#### TRAI00 - Unpack train, tune, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54825b0-26cd-4d7a-81eb-fb77f4c5b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unpack train, tune, test datasets\n",
    "def split_xy_data(ml_cat):\n",
    "    i = user_data.loc[user_data.ml_set == ml_cat, 'handle'].values\n",
    "    \n",
    "    x = user_token_matrix.loc[i, :]\n",
    "    \n",
    "    y1 = user_data.set_index('handle').loc[i, 'group'] == 'USA House'\n",
    "    y1 = y1.astype(int)\n",
    "    \n",
    "    y2 = user_data.set_index('handle').loc[i, 'party'] == 'Republican'\n",
    "    y2 = y2.astype(int)\n",
    "    \n",
    "    return x, y1, y2\n",
    "\n",
    "## execute code\n",
    "train_x, train_y1, train_y2 = split_xy_data('train')\n",
    "tune_x, tune_y1, tune_y2 = split_xy_data('tune')\n",
    "test_x, test_y1, test_y2 = split_xy_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e904b6-a715-42d7-92c3-1457065b62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete objects to clear up memory\n",
    "del tweet_words, tweet_data, user_token_matrix, word_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fe729-41ac-4f57-9eb5-a9d62004211a",
   "metadata": {},
   "source": [
    "#### TRAI01 - generate PCA simplification of features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526bae5-a1f2-4daf-b566-ef88d384aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit pca model in order to simplify and enhance feature matrix\n",
    "model_pca = PCA().fit(train_x)\n",
    "\n",
    "## generate elbow curve statistics for number of components\n",
    "def pca_elbow(mod = model_pca, fig_name = None):\n",
    "    \n",
    "    ## calculate statistics\n",
    "    quality = mod.singular_values_\n",
    "    quality = (np.cumsum(quality) / np.sum(quality)).round(3)\n",
    "    n = list(range(1, len(quality) + 1))\n",
    "    elbow_stats = pd.DataFrame({'n': n, 'quality': quality})\n",
    "    \n",
    "    ## output figure\n",
    "    print('TODO: finish building section')\n",
    "    \n",
    "    ## find elbow\n",
    "    \n",
    "    return elbow_stats\n",
    "\n",
    "pca_elbow().to_csv('elbow.csv')\n",
    "\n",
    "## choose number of components\n",
    "print('TODO: finishing building PCA section')\n",
    "\n",
    "## generate pca transformations of all feature matrices\n",
    "train_x_pca = model_pca.transform(train_x)\n",
    "tune_x_pca  = model_pca.transform(tune_x)\n",
    "test_x_pca  = model_pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84d1f6-9a64-49aa-a13a-64af0205b4df",
   "metadata": {},
   "source": [
    "#### TRAI02 - [random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed92b5-d95e-4a6e-b55f-473d2f2b36d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64aee504-f3a0-413f-bccf-3be12edf1ceb",
   "metadata": {},
   "source": [
    "#### TRAI03 - [logistic regression (features, PCA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7468d-f865-46f4-8480-2da9ef95b120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d47e02f2-7778-461f-adeb-87ebdc540a84",
   "metadata": {},
   "source": [
    "#### TRAI04 - [Naive bayes (features, pca)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d7a47-0667-4350-a6ac-210c14574408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3cb043-f8c8-4e20-8545-d4db3470adda",
   "metadata": {},
   "source": [
    "#### TRAI05 - [random forest (features, pca)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bdbdf-eea9-4313-b563-e553ed7d01f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2778f074-563c-4f0d-ba09-38f3e935ee6d",
   "metadata": {},
   "source": [
    "#### TRAI06 - [adaboost (features, pca)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "338b430f-a2d6-4836-8984-93d56be79ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>party</th>\n",
       "      <th>member</th>\n",
       "      <th>state</th>\n",
       "      <th>handle</th>\n",
       "      <th>verified</th>\n",
       "      <th>tweets</th>\n",
       "      <th>ml_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA House</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Adams, Alma</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>repadams</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA House</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Aderholt, Robert</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>robert_aderholt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA House</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Aguilar, Pete</td>\n",
       "      <td>California</td>\n",
       "      <td>reppeteaguilar</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA House</td>\n",
       "      <td>Republican</td>\n",
       "      <td>Allen, Rick</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>reprickallen</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USA House</td>\n",
       "      <td>Democratic</td>\n",
       "      <td>Allred, Colin</td>\n",
       "      <td>Texas</td>\n",
       "      <td>repcolinallred</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170</td>\n",
       "      <td>tune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>CAN House</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Williamson, John</td>\n",
       "      <td>New Brunswick</td>\n",
       "      <td>johnwilliamson_</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127</td>\n",
       "      <td>tune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>CAN House</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Yip, Jean</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>jeanyip3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>CAN House</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Zahid, Salma</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>salmazahid15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>CAN House</td>\n",
       "      <td>NDP</td>\n",
       "      <td>Zarrillo, Bonita</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>bonitazarrillo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>CAN House</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Zimmer, Bob</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>bobzimmermp</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>796 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         group         party            member             state  \\\n",
       "0    USA House    Democratic       Adams, Alma    North Carolina   \n",
       "1    USA House    Republican  Aderholt, Robert           Alabama   \n",
       "2    USA House    Democratic     Aguilar, Pete        California   \n",
       "3    USA House    Republican       Allen, Rick           Georgia   \n",
       "4    USA House    Democratic     Allred, Colin             Texas   \n",
       "..         ...           ...               ...               ...   \n",
       "791  CAN House  Conservative  Williamson, John     New Brunswick   \n",
       "792  CAN House       Liberal         Yip, Jean           Ontario   \n",
       "793  CAN House       Liberal      Zahid, Salma           Ontario   \n",
       "794  CAN House           NDP  Zarrillo, Bonita  British Columbia   \n",
       "795  CAN House  Conservative       Zimmer, Bob  British Columbia   \n",
       "\n",
       "              handle  verified  tweets ml_set  \n",
       "0           repadams       1.0      87  train  \n",
       "1    robert_aderholt       1.0     154   test  \n",
       "2     reppeteaguilar       1.0     133  train  \n",
       "3       reprickallen       1.0     150  train  \n",
       "4     repcolinallred       1.0     170   tune  \n",
       "..               ...       ...     ...    ...  \n",
       "791  johnwilliamson_       1.0     127   tune  \n",
       "792         jeanyip3       1.0     170  train  \n",
       "793     salmazahid15       1.0     135  train  \n",
       "794   bonitazarrillo       1.0      32  train  \n",
       "795      bobzimmermp       1.0      78  train  \n",
       "\n",
       "[796 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e42c2cff-5e9c-46ac-84c5-367e8570889a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train      551\n",
       "tune       133\n",
       "test       101\n",
       "exclude     11\n",
       "Name: ml_set, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data.ml_set.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64189160-c590-484d-b783-02e4a1ba21cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
