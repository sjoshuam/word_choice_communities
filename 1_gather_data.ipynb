{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc52c8a0-b7be-4019-878a-4e9ac1ff4895",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b339-098d-4b27-89a0-8c1167b5ac8a",
   "metadata": {},
   "source": [
    "#### HEAD 01 - toggle user settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18e3fdb-3cd1-47ca-94c6-0544e31fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine whether to cache data from some time consuming tasks\n",
    "settings = {\n",
    "    'test_mode': False, ## when true, only a small of data is collected\n",
    "    'collect_data': False, ## toggles twitter api pulls in PULL01-03\n",
    "    'rebuild_word_data': False, ## rebuild vs cache load for word_data MUNG02\n",
    "    'rebuild_tweet_words': False, ## rebuild vs cache load tweet_words MUNG03\n",
    "    'rebuild_user_token': False ## rebuild vs cache load user_token_tally MUNG04\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16158d71-339e-4026-93e9-b270cc45859d",
   "metadata": {},
   "source": [
    "#### HEAD02 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0914f0c2-3000-4136-94e5-630181f3e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########==========\n",
    "import tweepy\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from os.path import exists\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18441e2b-97b6-4243-a468-0de4c7e31258",
   "metadata": {},
   "source": [
    "#### HEAD03 - load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f766755-c82c-49a1-ae9a-05155a84f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s8/opt/anaconda3/envs/py310/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## read in roster of handles\n",
    "user_data = pd.read_excel(\"A_Input/twitter_handles.xlsx\")\n",
    "\n",
    "## drop all but a handful of cases if in test mode\n",
    "if settings['test_mode']:\n",
    "    user_data = user_data.sample(100, random_state = 5542)\n",
    "\n",
    "## read in twitter credentials; initialize api connection+\n",
    "twitter_credentials = pd.read_csv('../api_keys/twitter.csv').set_index('item')\n",
    "twitter_credentials = tweepy.OAuth1UserHandler(\n",
    "    consumer_key = twitter_credentials.loc['API Key', 'string'],\n",
    "    consumer_secret = twitter_credentials.loc['API Key Secret', 'string'],\n",
    "    access_token = twitter_credentials.loc['Access Token', 'string'],\n",
    "   access_token_secret = twitter_credentials.loc['Access Token Secret', 'string']\n",
    "    )\n",
    "api = tweepy.API(twitter_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eace4-543f-4ff2-81cc-0eb3cc114c18",
   "metadata": {},
   "source": [
    "#### HEAD04 - create build or cache decision function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96445ddf-b3b5-4672-95d7-cc48563f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build switching function to execute code or cache results\n",
    "def build_or_cache(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        x.to_csv(address, index = False)\n",
    "        return x\n",
    "    else:\n",
    "        return pd.read_csv(address)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed49a6b-8b72-4edd-84b8-0f815986fc01",
   "metadata": {},
   "source": [
    "## HAND â€“ Gather Twitter handles for test accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67daeb1-100d-4fc1-9c20-f28ed2c1ec0a",
   "metadata": {},
   "source": [
    "#### HAND01 - extract handles from roster URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdda14bb-55b3-41d9-b735-b674367eee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract handles from roster urls\n",
    "user_data['handle'] = user_data.url.str.replace('https://twitter.com/', '',\n",
    "            regex = False).str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e64b95-7e26-4789-b736-089706334fad",
   "metadata": {},
   "source": [
    "## PULL - Pull Twitter data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9531973-8911-4beb-a48b-1c7989ae2428",
   "metadata": {},
   "source": [
    "#### PULL01 - query API for each roster handle's user_timeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1200e6ca-cf86-412a-bdf2-da760105698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tweet data from api object\n",
    "def refine_tweet_data(x):\n",
    "    tweet_data = list()\n",
    "    for i in range(0, len(x)):\n",
    "        tweet_data.append({\n",
    "            'tweet_id': x[i].id, 'created_at': x[i].created_at, 'lang': x[i].lang,\n",
    "            'full_text': x[i].full_text,\n",
    "            'screen_name': x[i].author.screen_name,\n",
    "            'verified' : x[i].author.verified\n",
    "        })\n",
    "    return pd.DataFrame(tweet_data)\n",
    "\n",
    "## define function to pull user tweet data and apply function to extract tweet data\n",
    "def pull_tweet_data(handles = user_data.handle, a = api):\n",
    "    tweet_data = list()\n",
    "    for i in handles:\n",
    "        try:\n",
    "            user_tweets = a.user_timeline(\n",
    "                screen_name = i, count = 200, tweet_mode = 'extended', \n",
    "                exclude_replies = True, include_rts = False)\n",
    "            tweet_data.append(refine_tweet_data(user_tweets))\n",
    "            sleep(0.5)\n",
    "        except:\n",
    "            pass\n",
    "    tweet_data = pd.concat(tweet_data)\n",
    "    tweet_data['screen_name'] = tweet_data['screen_name'].str.lower()\n",
    "    return tweet_data\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    tweet_data = pull_tweet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7301b7-3283-45c5-8943-1444d0e8cf27",
   "metadata": {},
   "source": [
    "#### PULL02 - tabulate tweet statistics, divide users into train/tune/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f26d7ac-312e-4445-a22a-0f8bada0d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combile tweet count and verification status at the user level\n",
    "def enhance_user_data(td, ud = user_data):\n",
    "    \n",
    "    def np_size(x): return x.size\n",
    "    \n",
    "    ## calculate tweet summary statistics\n",
    "    td_original = tweet_data\n",
    "    td = td.copy()\n",
    "    verified = td.groupby('screen_name').mean()\n",
    "    tweets = td['screen_name'].value_counts()\n",
    "    td = pd.concat([verified, tweets], axis = 1).reset_index()\n",
    "    td.columns = ['handle', 'verified', 'tweets']\n",
    "    \n",
    "    ## merge statistics into the user_data object\n",
    "    ud = pd.merge(ud, td, on = 'handle', how = 'left')\n",
    "    td_original = td_original.drop(['verified'], axis = 1)\n",
    "    ud = ud.drop(['url'], axis = 1).reset_index(drop = True)\n",
    "    ud = ud.fillna({'tweets': 0}).astype({'tweets': int})\n",
    "    \n",
    "    ## divide users into train, tune, and test subsets\n",
    "    ml_set = pd.Series(['train', 'tune', 'test'], name = 'ml_set').sample(\n",
    "                n = ud.shape[0], replace = True, weights = [0.7, 0.15, 0.15],\n",
    "                random_state = 2006)\n",
    "    ud['ml_set'] = ml_set.values\n",
    "    ud.loc[ud.tweets == 0, 'ml_set'] = 'exclude'\n",
    "    \n",
    "    return ud, td_original\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    user_data, tweet_data = enhance_user_data(\n",
    "        tweet_data[['screen_name', 'verified']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6e488-00b0-4257-a07b-598824c2aa18",
   "metadata": {},
   "source": [
    "#### PULL03 - save datasets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb27d14-f5e0-40ef-beac-2f6d369a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save user/tweet datasets to disk as csvs\n",
    "if settings['collect_data']:\n",
    "    user_data.to_csv('B_Process/user_data.csv', index = False)\n",
    "    tweet_data.to_csv('B_Process/tweet_data.csv', index = False)\n",
    "else:\n",
    "    user_data = pd.read_csv('B_Process/user_data.csv')\n",
    "    tweet_data = pd.read_csv('B_Process/tweet_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1152f-f55d-4758-a9d1-78e30499f55f",
   "metadata": {},
   "source": [
    "## MUNG - Process Twitter data to model-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fdb93-7acf-43e0-9ae9-3ae2eb6a749e",
   "metadata": {},
   "source": [
    "#### MUNG01 - parse tweet text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3362904b-047d-44ef-9497-12af5a5421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize, remove capitalization, and remove duplicate tokens\n",
    "def nlp_tokenize_tweet(x):\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    x = list(set(x))\n",
    "    return x\n",
    "\n",
    "## execute code\n",
    "tweet_data['tokens'] = tweet_data.full_text.apply(nlp_tokenize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422810c-53d2-4b5e-b9e8-68457e82756b",
   "metadata": {},
   "source": [
    "#### MUNG02 - create word/token level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f7a6af-6786-469b-8ec6-f07fb6d31709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/rmkl6dh17jl6zqhzl0ky4b_w0000gp/T/ipykernel_9166/3376109838.py:8: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(address)\n"
     ]
    }
   ],
   "source": [
    "## create word/token level dataset and identify valid word tokens\n",
    "def make_word_data(td = tweet_data):\n",
    "\n",
    "    ## flatten token lists and count occurances\n",
    "    word_data = list()\n",
    "    for i in td.tokens:\n",
    "        word_data += i\n",
    "    word_data = pd.Series(word_data, name = 'count').value_counts()\n",
    "    word_data = word_data.sort_values(ascending = False)\n",
    "    word_data = pd.DataFrame(word_data)\n",
    "    \n",
    "    ## determine which tokens occur often enough to warrant inclusion\n",
    "    word_data['valid'] = word_data['count'] > max(\n",
    "        word_data['count'].quantile(0.2), 3)\n",
    "    word_data['word'] = word_data.index\n",
    "    \n",
    "    ## determine part of speech for eligible tokens\n",
    "    speech_part = word_data['word'].loc[word_data['valid']].values\n",
    "    speech_part = pos_tag(speech_part)\n",
    "    speech_part = [i[1][0].lower() for i in speech_part]\n",
    "    word_data['pos'] = '.'\n",
    "    word_data.loc[word_data['valid'], 'pos'] = speech_part\n",
    "    \n",
    "    ## lemmatize\n",
    "    WNL = WordNetLemmatizer()\n",
    "    word_data['token'] = None\n",
    "    for i in word_data.word:\n",
    "        if not word_data.loc[i, 'valid']: \n",
    "            break\n",
    "        if word_data.loc[i, 'pos'] in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            try:\n",
    "                word_data.loc[i, 'token'] = WNL.lemmatize(\n",
    "                    word_data.loc[i, 'word'],\n",
    "                    pos = word_data.loc[i, 'pos']\n",
    "                )\n",
    "            except:\n",
    "                word_data.loc[i, 'token'] = word_data.loc[i, 'word']\n",
    "        else:\n",
    "            word_data.loc[i, 'valid'] = False\n",
    "        \n",
    "    return word_data.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "word_data = build_or_cache(\n",
    "    address = 'B_Process/word_data.csv',\n",
    "    function = make_word_data,\n",
    "    build_bool = settings['rebuild_word_data']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419c4d1-dada-48f5-9bcd-252137c407b5",
   "metadata": {},
   "source": [
    "#### MUNG03 - generate a tokens x tweets link database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247437d5-61e4-4143-93b2-cc66edf8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tweet_token_data(td = tweet_data, wd = word_data):\n",
    "    \n",
    "    wd = wd.set_index('word')\n",
    "    \n",
    "    ## replicate tweet ids\n",
    "    n = td.tokens.apply(len).values\n",
    "    tweet_tokens = pd.Series(np.repeat(td.tweet_id.values, n), name = \"tweet_id\")\n",
    "    tweet_tokens = pd.DataFrame(tweet_tokens)\n",
    "    \n",
    "    ## allocate words to the new dataset\n",
    "    words = list()\n",
    "    for i in td.tokens:\n",
    "        words += i\n",
    "    tweet_tokens['words'] = words\n",
    "    \n",
    "    ## convert words to tokens\n",
    "    tweet_tokens['tokens'] = wd.loc[\n",
    "        tweet_tokens.words.values, 'token'].values\n",
    "    tweet_tokens = tweet_tokens.dropna()\n",
    "\n",
    "    return tweet_tokens.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "tweet_words = build_or_cache(\n",
    "    address = 'B_Process/tweet_words.csv',\n",
    "    function = make_tweet_token_data,\n",
    "    build_bool = settings['rebuild_tweet_words']\n",
    "    )\n",
    "tweet_data = tweet_data.drop('tokens', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5cef3-bc88-4f27-9a5e-f76ce06d149c",
   "metadata": {},
   "source": [
    "#### MUNG04 - generate a tokens x users count; drop tokens with only one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc82f94-94cf-4030-a5d9-b3f5e5bbd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_token_matrix(td = tweet_data, tw = tweet_words, ud = user_data):\n",
    "    \n",
    "    ## count of number of times each user wrote each token\n",
    "    tw = tw.merge(right = td[['screen_name', 'tweet_id']],\n",
    "                  how = 'left', on = 'tweet_id')\n",
    "    tw = tw.drop(['tweet_id', 'words'], axis = 1).groupby('screen_name')\n",
    "    tw = tw.value_counts()\n",
    "    tw.name = 'count'\n",
    "    tw = tw.reset_index().set_index('screen_name')\n",
    "    tw = tw.pivot(columns = 'tokens').fillna(0).astype(int)\n",
    "    tw = tw.droplevel(axis = 1, level = 0)\n",
    "    \n",
    "    ## remove tokens that fewer than 3 or more than 80 percent of users use\n",
    "    valid_usage = (tw > 0).astype(int).sum().values\n",
    "    valid_usage = (valid_usage > 2\n",
    "                  ) & (valid_usage < int(tw.shape[0] * 0.8))\n",
    "    tw = tw.loc[:, valid_usage]\n",
    "    \n",
    "    ## standardize matrix as words per 1,000 tweets\n",
    "    denom = pd.DataFrame({'handle': tw.index}).merge(\n",
    "        ud[['handle', 'tweets']],\n",
    "        how = 'left', on = 'handle'\n",
    "        ).set_index('handle').squeeze().fillna(1)\n",
    "    denom.loc[denom < 1] = 1\n",
    "    tw = (tw.divide(denom, axis = 0) * 1000).astype(int)\n",
    "    tw = tw.reset_index().rename({'level_0':'screen_name'}, axis = 1)\n",
    "    \n",
    "    return tw\n",
    "\n",
    "\n",
    "## execute code\n",
    "user_token_matrix = build_or_cache(\n",
    "    address = 'B_Process/user_token_matrix.csv',\n",
    "    function = make_user_token_matrix,\n",
    "    build_bool = settings['rebuild_user_token']\n",
    "    ).set_index('screen_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0ad36-2fff-4c0f-8b41-a0dba7c3d20a",
   "metadata": {},
   "source": [
    "## TRAI â€“ Train models and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db026d-6e08-43d1-9c7e-7d49128c501f",
   "metadata": {},
   "source": [
    "#### TRAI00 - Unpack train, tune, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c54825b0-26cd-4d7a-81eb-fb77f4c5b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unpack train, tune, test datasets\n",
    "def split_xy_data(ml_cat):\n",
    "    i = user_data.loc[user_data.ml_set == ml_cat, 'handle'].values\n",
    "    \n",
    "    x = user_token_matrix.loc[i, :]\n",
    "    \n",
    "    y1 = user_data.set_index('handle').loc[i, 'group'] == 'USA House'\n",
    "    y1 = y1.astype(int)\n",
    "    \n",
    "    y2 = user_data.set_index('handle').loc[i, 'party'] == 'Republican'\n",
    "    y2 = y2.astype(int)\n",
    "    \n",
    "    return x, y1, y2\n",
    "\n",
    "## execute code\n",
    "train_x, train_y1, train_y2 = split_xy_data('train')\n",
    "tune_x, tune_y1, tune_y2    = split_xy_data('tune')\n",
    "test_x, test_y1, test_y2    = split_xy_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e904b6-a715-42d7-92c3-1457065b62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete objects to clear up memory in prep for modeling\n",
    "del tweet_words, tweet_data, user_token_matrix, word_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468fe729-41ac-4f57-9eb5-a9d62004211a",
   "metadata": {},
   "source": [
    "#### TRAI01 - generate PCA simplification of features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c526bae5-a1f2-4daf-b566-ef88d384aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit pca model in order to simplify and enhance feature matrix\n",
    "model_pca = PCA().fit(train_x)\n",
    "\n",
    "## generate pca transformations of all feature matrices\n",
    "train_x_pca = model_pca.transform(train_x)\n",
    "tune_x_pca  = model_pca.transform(tune_x)\n",
    "test_x_pca  = model_pca.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84d1f6-9a64-49aa-a13a-64af0205b4df",
   "metadata": {},
   "source": [
    "#### TRAI02 - estimate model performance at random chance (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ed92b5-d95e-4a6e-b55f-473d2f2b36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## estimate the confusion matrix that would result by random chance\n",
    "   ## if we predicted labels based on frequently in the training set.\n",
    "   ## This function predicts results for a single stage.\n",
    "def estimate_random_confusion_matrix(y1, y2):\n",
    "    \n",
    "    ## tabulate probability of a positive at random\n",
    "    pos1  = y1.mean()\n",
    "    pos2  = y2.mean()\n",
    "    \n",
    "    ## calculate confusion matrix stats\n",
    "    confusion_matrix = {\n",
    "        'neg': {\n",
    "            'neg': (1 - pos1) * (1 - pos2),\n",
    "            'pos': (1 - pos1) * pos2\n",
    "            },\n",
    "        'pos': {\n",
    "            'neg': pos1 * (1 - pos2),\n",
    "            'pos': pos1 * pos2\n",
    "\n",
    "            }\n",
    "        }\n",
    "    confusion_matrix = pd.DataFrame(confusion_matrix)\n",
    "    \n",
    "    ## check results and return\n",
    "    assert confusion_matrix.sum().sum() == 1\n",
    "    return confusion_matrix\n",
    "\n",
    "## generate the joint confusion matrix for a multi-stage random chance model\n",
    "def estimate_multistage_random_confusion_matrix(y1p, y1o, y2p, y2o, name):\n",
    "    \n",
    "    ## generate confusion matrix for each stage\n",
    "    stage1 = estimate_random_confusion_matrix(y1p, y1o)\n",
    "    stage2 = estimate_random_confusion_matrix(y2p, y2o)\n",
    "    \n",
    "    ## generate confusion matrix for both stages combined\n",
    "    joint_matrix = stage1 * 0\n",
    "    joint_matrix.loc['pos', 'pos'] = stage1.loc['pos', 'pos'] *\\\n",
    "                                    stage2.loc['pos', 'pos']\n",
    "    joint_matrix.loc['pos', 'neg'] = stage1.loc['pos', 'neg'] *\\\n",
    "                                    stage2.loc['pos', 'neg'] +\\\n",
    "                                    stage1.loc['pos', 'neg'] *\\\n",
    "                                    stage2.loc['pos', 'pos'] +\\\n",
    "                                    stage1.loc['pos', 'pos'] *\\\n",
    "                                    stage2.loc['pos', 'neg']\n",
    "    joint_matrix.loc['neg', 'pos'] = stage1.loc['neg', 'pos'] *\\\n",
    "                                    stage2.loc['neg', 'pos'] +\\\n",
    "                                    stage1.loc['neg', 'pos'] *\\\n",
    "                                    stage2.loc['pos', 'pos'] +\\\n",
    "                                    stage1.loc['pos', 'pos'] *\\\n",
    "                                    stage2.loc['neg', 'pos']\n",
    "    joint_matrix.loc['neg', 'neg'] = 1 - joint_matrix.sum().sum()\n",
    "    \n",
    "    ## calculate precision and recall\n",
    "    performance = dict()\n",
    "    performance['Precision'] = joint_matrix.loc['pos', 'pos'] / (\n",
    "        joint_matrix.loc['pos', 'pos'] + joint_matrix.loc['pos', 'neg'])\n",
    "    performance['Recall'] = joint_matrix.loc['pos', 'pos'] / (\n",
    "        joint_matrix.loc['pos', 'pos'] + joint_matrix.loc['neg', 'pos'])\n",
    "    performance = pd.DataFrame({name: performance}).T.round(3)\n",
    "    \n",
    "    return performance\n",
    "\n",
    "## execute code (and create performance statistics container object)\n",
    "performance_stats = list()\n",
    "performance_stats.append(estimate_multistage_random_confusion_matrix(\n",
    "    train_y1, train_y1, train_y2, train_y2, ('random', 'train')))\n",
    "performance_stats.append(estimate_multistage_random_confusion_matrix(\n",
    "    train_y1, test_y1, train_y2, test_y2, ('random', 'test')))\n",
    "performance_stats = pd.concat(performance_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aee504-f3a0-413f-bccf-3be12edf1ceb",
   "metadata": {},
   "source": [
    "#### TRAI03 - [two-stage logistic regression (features, PCA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7468d-f865-46f4-8480-2da9ef95b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit logistic regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e02f2-7778-461f-adeb-87ebdc540a84",
   "metadata": {},
   "source": [
    "#### TRAI04 - [two-stage Naive bayes (features, pca)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d7a47-0667-4350-a6ac-210c14574408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3cb043-f8c8-4e20-8545-d4db3470adda",
   "metadata": {},
   "source": [
    "#### TRAI05 - [two-stage random forest (features, pca)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bdbdf-eea9-4313-b563-e553ed7d01f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2778f074-563c-4f0d-ba09-38f3e935ee6d",
   "metadata": {},
   "source": [
    "#### TRAI06 - [two-stage adaboost (features, pca)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64189160-c590-484d-b783-02e4a1ba21cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9f9ec1b-cea5-4168-9a5e-d4767a436101",
   "metadata": {},
   "source": [
    "## FOOT - display useful statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e42c2cff-5e9c-46ac-84c5-367e8570889a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train      551\n",
       "tune       133\n",
       "test       101\n",
       "exclude     11\n",
       "Name: ml_set, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data.ml_set.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d973309-e9bd-459c-a036-6bd65ddbf3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">random</th>\n",
       "      <th>train</th>\n",
       "      <td>0.192</td>\n",
       "      <td>0.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.192</td>\n",
       "      <td>0.178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall\n",
       "random train      0.192   0.192\n",
       "       test       0.192   0.178"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
