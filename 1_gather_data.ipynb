{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc52c8a0-b7be-4019-878a-4e9ac1ff4895",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b339-098d-4b27-89a0-8c1167b5ac8a",
   "metadata": {},
   "source": [
    "#### HEAD 01 - toggle user settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18e3fdb-3cd1-47ca-94c6-0544e31fe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine whether to cache data from some time consuming tasks\n",
    "settings = {\n",
    "    'test_mode': True, ## when true, only a small of data is collected\n",
    "    'collect_data': False, ## toggles twitter api pulls in PULL01-03\n",
    "    'rebuild_word_data': False, ## rebuild vs cache load for word_data MUNG02\n",
    "    'rebuild_tweet_words': False, ## rebuild vs cache load tweet_words MUNG03\n",
    "    'rebuild_user_token': False ## rebuild vs cache load user_token_tally MUNG04\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16158d71-339e-4026-93e9-b270cc45859d",
   "metadata": {},
   "source": [
    "#### HEAD02 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0914f0c2-3000-4136-94e5-630181f3e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########==========\n",
    "import tweepy\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from os.path import exists\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18441e2b-97b6-4243-a468-0de4c7e31258",
   "metadata": {},
   "source": [
    "#### HEAD03 - load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f766755-c82c-49a1-ae9a-05155a84f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s8/opt/anaconda3/envs/py310/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## read in roster of handles\n",
    "user_data = pd.read_excel(\"A_Input/twitter_handles.xlsx\")\n",
    "\n",
    "## drop all but a handful of cases if in test mode\n",
    "if settings['test_mode']:\n",
    "    user_data = user_data.sample(int(70 / 0.7), random_state = 5542)\n",
    "\n",
    "## read in twitter credentials; initialize api connection+\n",
    "twitter_credentials = pd.read_csv('../api_keys/twitter.csv').set_index('item')\n",
    "twitter_credentials = tweepy.OAuth1UserHandler(\n",
    "    consumer_key = twitter_credentials.loc['API Key', 'string'],\n",
    "    consumer_secret = twitter_credentials.loc['API Key Secret', 'string'],\n",
    "    access_token = twitter_credentials.loc['Access Token', 'string'],\n",
    "   access_token_secret = twitter_credentials.loc['Access Token Secret', 'string']\n",
    "    )\n",
    "api = tweepy.API(twitter_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eace4-543f-4ff2-81cc-0eb3cc114c18",
   "metadata": {},
   "source": [
    "#### HEAD04 - create build or cache decision function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96445ddf-b3b5-4672-95d7-cc48563f54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build switching function to execute code or cache results\n",
    "def build_or_cache(address, function, build_bool):\n",
    "    if build_bool or not exists(address):\n",
    "        x = function()\n",
    "        x.to_csv(address)\n",
    "        return x\n",
    "    else:\n",
    "        return pd.read_csv(address)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed49a6b-8b72-4edd-84b8-0f815986fc01",
   "metadata": {},
   "source": [
    "## HAND â€“ Gather Twitter handles for test accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67daeb1-100d-4fc1-9c20-f28ed2c1ec0a",
   "metadata": {},
   "source": [
    "#### HAND01 - extract handles from roster URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdda14bb-55b3-41d9-b735-b674367eee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract handles from roster urls\n",
    "user_data['handle'] = user_data.url.str.replace('https://twitter.com/', '',\n",
    "            regex = False).str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e64b95-7e26-4789-b736-089706334fad",
   "metadata": {},
   "source": [
    "## PULL - Pull Twitter data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9531973-8911-4beb-a48b-1c7989ae2428",
   "metadata": {},
   "source": [
    "#### PULL01 - query API for each roster handle's user_timeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1200e6ca-cf86-412a-bdf2-da760105698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract tweet data from api object\n",
    "def refine_tweet_data(x):\n",
    "    tweet_data = list()\n",
    "    for i in range(0, len(x)):\n",
    "        tweet_data.append({\n",
    "            'tweet_id': x[i].id, 'created_at': x[i].created_at, 'lang': x[i].lang,\n",
    "            'full_text': x[i].full_text,\n",
    "            'screen_name': x[i].author.screen_name, 'verified': x[i].author.verified\n",
    "        })\n",
    "    return pd.DataFrame(tweet_data)\n",
    "\n",
    "## define function to pull user tweet data and apply function to extract tweet data\n",
    "def pull_tweet_data(handles = user_data.handle, a = api):\n",
    "    tweet_data = list()\n",
    "    for i in handles:\n",
    "        try:\n",
    "            user_tweets = a.user_timeline(\n",
    "                screen_name = i, count = 200, tweet_mode = 'extended', \n",
    "                exclude_replies = True, include_rts = False)\n",
    "            tweet_data.append(refine_tweet_data(user_tweets))\n",
    "            sleep(0.1)\n",
    "        except:\n",
    "            pass\n",
    "    return pd.concat(tweet_data)\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    tweet_data = pull_tweet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7301b7-3283-45c5-8943-1444d0e8cf27",
   "metadata": {},
   "source": [
    "#### PULL02 - tabulate tweet statistics, divide users into train/tune/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f26d7ac-312e-4445-a22a-0f8bada0d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combile tweet count and verification status at the user level\n",
    "def enhance_user_data(td, ud = user_data):\n",
    "    \n",
    "    ## calculate tweet summary statistics\n",
    "    td_original = tweet_data\n",
    "    td = td.copy().groupby('screen_name').agg({np.mean, len})\n",
    "    td.columns = td.columns.droplevel(0)\n",
    "    td = td.rename({'len': 'tweets', 'mean': 'verified'}, axis = 1)\n",
    "    td = td.fillna({'tweets': 0, 'verified': 0})\n",
    "    \n",
    "    ## merge statistics into the user_data object\n",
    "    ud = pd.merge(ud, td, left_on = 'handle', right_on = 'screen_name', \n",
    "                  how = 'left')\n",
    "    td_original = td_original.drop(['verified'], axis = 1)\n",
    "    ud = ud.drop(['url'], axis = 1).reset_index(drop = True)\n",
    "    \n",
    "    ## divide users into train, tune, and test subsets\n",
    "    ml_set = pd.Series(['train', 'tune', 'test'], name = 'ml_set').sample(\n",
    "                n = ud.shape[0], replace = True, weights = [0.7, 0.15, 0.15],\n",
    "                random_state = 2006)\n",
    "    ud['ml_set'] = ml_set.values\n",
    "    ud.loc[ud.tweets.isnull(), 'ml_set'] = 'exclude'\n",
    "    \n",
    "    return ud, td_original\n",
    "\n",
    "## execute code\n",
    "if settings['collect_data']:\n",
    "    user_data, tweet_data = enhance_user_data(\n",
    "        tweet_data[['screen_name', 'verified']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6e488-00b0-4257-a07b-598824c2aa18",
   "metadata": {},
   "source": [
    "#### PULL03 - save datasets to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eb27d14-f5e0-40ef-beac-2f6d369a9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save user/tweet datasets to disk as csvs\n",
    "if settings['collect_data']:\n",
    "    user_data.to_csv('B_Process/user_data.csv.gz')\n",
    "    tweet_data.to_csv('B_Process/tweet_data.csv.gz')\n",
    "else:\n",
    "    user_data = pd.read_csv('B_Process/user_data.csv.gz')\n",
    "    tweet_data = pd.read_csv('B_Process/tweet_data.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1152f-f55d-4758-a9d1-78e30499f55f",
   "metadata": {},
   "source": [
    "## MUNG - Process Twitter data to model-ready format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fdb93-7acf-43e0-9ae9-3ae2eb6a749e",
   "metadata": {},
   "source": [
    "#### MUNG01 - parse tweet text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3362904b-047d-44ef-9497-12af5a5421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize and remove capitalization\n",
    "def nlp_tokenize_tweet(x):\n",
    "    x = x.lower()\n",
    "    x = word_tokenize(x)\n",
    "    return x\n",
    "\n",
    "## execute code\n",
    "tweet_data['tokens'] = tweet_data.full_text.apply(nlp_tokenize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422810c-53d2-4b5e-b9e8-68457e82756b",
   "metadata": {},
   "source": [
    "#### MUNG02 - create word/token level dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f7a6af-6786-469b-8ec6-f07fb6d31709",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create word/token level dataset and identify valid word tokens\n",
    "def make_word_data(td = tweet_data):\n",
    "\n",
    "    ## flatten token lists and count occurances\n",
    "    word_data = list()\n",
    "    for i in td.tokens:\n",
    "        word_data += i\n",
    "    word_data = pd.Series(word_data, name = 'count').value_counts()\n",
    "    word_data = word_data.sort_values(ascending = False)\n",
    "    word_data = pd.DataFrame(word_data)\n",
    "    \n",
    "    ## determine which tokens occur often enough to warrant inclusion\n",
    "    word_data['valid'] = word_data['count'] > max(\n",
    "        word_data['count'].quantile(0.2), 3)\n",
    "    word_data['word'] = word_data.index\n",
    "    \n",
    "    ## determine part of speech for eligible tokens\n",
    "    speech_part = word_data['word'].loc[word_data['valid']].values\n",
    "    speech_part = pos_tag(speech_part)\n",
    "    speech_part = [i[1][0].lower() for i in speech_part]\n",
    "    word_data['pos'] = '.'\n",
    "    word_data.loc[word_data['valid'], 'pos'] = speech_part\n",
    "    \n",
    "    ## lemmatize\n",
    "    WNL = WordNetLemmatizer()\n",
    "    word_data['token'] = None\n",
    "    for i in word_data.word:\n",
    "        if not word_data.loc[i, 'valid']: \n",
    "            break\n",
    "        if word_data.loc[i, 'pos'] in 'abcdefghijklmnopqrstuvwxyz':\n",
    "            try:\n",
    "                word_data.loc[i, 'token'] = WNL.lemmatize(\n",
    "                    word_data.loc[i, 'word'],\n",
    "                    pos = word_data.loc[i, 'pos']\n",
    "                )\n",
    "            except:\n",
    "                word_data.loc[i, 'token'] = word_data.loc[i, 'word']\n",
    "        else:\n",
    "            word_data.loc[i, 'valid'] = False\n",
    "        \n",
    "    return word_data.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "word_data = build_or_cache(\n",
    "    address = 'B_Process/word_data.csv.gz',\n",
    "    function = make_word_data,\n",
    "    build_bool = settings['rebuild_word_data']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9419c4d1-dada-48f5-9bcd-252137c407b5",
   "metadata": {},
   "source": [
    "#### MUNG03 - generate a tokens x tweets link database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247437d5-61e4-4143-93b2-cc66edf8b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tweet_token_data(td = tweet_data, wd = word_data):\n",
    "    \n",
    "    wd = wd.set_index('word')\n",
    "    \n",
    "    ## replicate tweet ids\n",
    "    n = td.tokens.apply(len).values\n",
    "    tweet_tokens = pd.Series(np.repeat(td.tweet_id.values, n), name = \"tweet_id\")\n",
    "    tweet_tokens = pd.DataFrame(tweet_tokens)\n",
    "    \n",
    "    ## allocate words to the new dataset\n",
    "    words = list()\n",
    "    for i in td.tokens:\n",
    "        words += i\n",
    "    tweet_tokens['words'] = words\n",
    "    \n",
    "    ## convert words to tokens\n",
    "    tweet_tokens['tokens'] = wd.loc[\n",
    "        tweet_tokens.words.values, 'token'].values\n",
    "    tweet_tokens = tweet_tokens.dropna()\n",
    "\n",
    "    return tweet_tokens.reset_index(drop = True)\n",
    "\n",
    "## execute code\n",
    "tweet_words = build_or_cache(\n",
    "    address = 'B_Process/tweet_words.csv.gz',\n",
    "    function = make_tweet_token_data,\n",
    "    build_bool = settings['rebuild_tweet_words']\n",
    "    )\n",
    "tweet_data = tweet_data.drop('tokens', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5cef3-bc88-4f27-9a5e-f76ce06d149c",
   "metadata": {},
   "source": [
    "#### MUNG04 - generate a tokens x users count; drop tokens with only one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc82f94-94cf-4030-a5d9-b3f5e5bbd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_token_matrix(td = tweet_data, tw = tweet_words):\n",
    "    \n",
    "    ## count of number of times each user wrote each token\n",
    "    tw = tw.merge(right = td[['screen_name', 'tweet_id']],\n",
    "                  how = 'left', on = 'tweet_id')\n",
    "    tw = tw.drop(['tweet_id', 'words'], axis = 1).groupby('screen_name')\n",
    "    tw = tw.value_counts()\n",
    "    tw.name = 'count'\n",
    "    tw = tw.reset_index().set_index('screen_name')\n",
    "    tw = tw.pivot(columns = 'tokens').fillna(0).astype(int)\n",
    "    tw = tw.droplevel(axis = 1, level = 0)\n",
    "    \n",
    "    ## remove tokens that fewer than 3 or more than 80 percent of users use\n",
    "    valid_usage = (tw > 0).astype(int).sum().values\n",
    "    valid_usage = (valid_usage > 2\n",
    "                  ) & (valid_usage < int(tw.shape[0] * 0.8))\n",
    "    \n",
    "    return tw.loc[:, valid_usage]\n",
    "\n",
    "\n",
    "## execute code\n",
    "user_token_matrix = build_or_cache(\n",
    "    address = 'B_Process/user_token_tally.csv.gz',\n",
    "    function = make_user_token_matrix,\n",
    "    build_bool = settings['rebuild_user_token']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0ad36-2fff-4c0f-8b41-a0dba7c3d20a",
   "metadata": {},
   "source": [
    "## TRAI â€“ Train models and tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bfc08-9fa5-45b5-9136-f5668b1c00e1",
   "metadata": {},
   "source": [
    "## reminders\n",
    "## - standardize data\n",
    "## - write a two-stage meta-function\n",
    "## - do automatic feature selection for regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f84d1f6-9a64-49aa-a13a-64af0205b4df",
   "metadata": {},
   "source": [
    "#### TRAI01 - [random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed92b5-d95e-4a6e-b55f-473d2f2b36d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64aee504-f3a0-413f-bccf-3be12edf1ceb",
   "metadata": {},
   "source": [
    "#### TRAI02 - [logistic regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7468d-f865-46f4-8480-2da9ef95b120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f6b9a52-2932-499e-bdb6-bb27a3a41d4c",
   "metadata": {},
   "source": [
    "#### TRAI03 - [logistic regression + PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847d7e4-e25f-48bf-8742-ed87f8f8bf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d47e02f2-7778-461f-adeb-87ebdc540a84",
   "metadata": {},
   "source": [
    "#### TRAI04 - [Naive bayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a414c61-8e82-4773-8c8b-080bff29e16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98a7906d-3379-4177-84f6-a3d65d7caa94",
   "metadata": {},
   "source": [
    "#### TRAI05 - [Naive bayes + PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d7a47-0667-4350-a6ac-210c14574408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3cb043-f8c8-4e20-8545-d4db3470adda",
   "metadata": {},
   "source": [
    "#### TRAI06 - [random forest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa969040-d44b-47a5-b189-adc30c2a82c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9bfdf00-e980-4eb0-90cd-6f5074e478d8",
   "metadata": {},
   "source": [
    "#### TRAI07 - [random forest + PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bdbdf-eea9-4313-b563-e553ed7d01f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2778f074-563c-4f0d-ba09-38f3e935ee6d",
   "metadata": {},
   "source": [
    "#### TRAI08 - [adaboost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b430f-a2d6-4836-8984-93d56be79ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "341048ff-53b9-40cc-a2d2-bdc5a6dcfbca",
   "metadata": {},
   "source": [
    "#### TRAI09 - [adaboost + PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285cca8-a673-4acb-a063-9647930ed5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
