{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d958f87-9f13-4306-8234-424d6df9573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########==========##########==========##########==========##########=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea0ce9-f17a-457d-bc44-17d39995134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TODO: PPTX13 code is SLOPPY quick code; will need to be rewritten ASAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92aed8-f41d-402b-8002-f5ee52ca77f9",
   "metadata": {},
   "source": [
    "## HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd2b0c-986e-49b7-92ab-3f4d942d6c67",
   "metadata": {},
   "source": [
    "#### HEAD01 - load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7cefb-9319-430f-8685-4bc08fc90159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib\n",
    "\n",
    "def define_style():\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Quicksand']\n",
    "    plt.rcParams['font.weight'] = 'medium'\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    plt.rcParams['ytick.labelsize'] = 12\n",
    "    plt.rcParams['xtick.labelsize'] = 12\n",
    "    plt.rcParams['axes.labelsize'] = 12\n",
    "    \n",
    "define_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17a0c7-a857-4ed8-895a-36b70eb81f65",
   "metadata": {},
   "source": [
    "#### HEAD02 - load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd573792-10b0-4f5b-a9dc-bf8153259868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(addr):\n",
    "    conn = open(addr, 'rb')\n",
    "    the_object = pickle.load(conn)\n",
    "    conn.close()\n",
    "    return the_object\n",
    "\n",
    "handle_data = pd.read_excel('A_Input/twitter_handles.xlsx')\n",
    "user_data = pd.read_csv('B_Process/user_data.csv')\n",
    "tweet_data = pd.read_csv('B_Process/tweet_data.csv')\n",
    "best_model_stats = load_pickle('B_Process/best_model_stats.pickle')\n",
    "\n",
    "\n",
    "## load hparam files\n",
    "logistic_hparams = load_pickle('B_Process/models/logistic_hparams.pickle')\n",
    "bayes_hparams    = load_pickle('B_Process/models/bayes_hparams.pickle')\n",
    "forest_hparams   = load_pickle('B_Process/models/forest_hparams.pickle')\n",
    "adaboost_hparams = load_pickle('B_Process/models/adaboost_hparams.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e52eb-cc8d-4996-9833-7fbbf34bda07",
   "metadata": {},
   "source": [
    "## PPTX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f247839-2e9c-4bae-b621-e7ae1828ae18",
   "metadata": {},
   "source": [
    "#### PPTX07 - Data Sources and Collection (Slide 07) - Calculate data collection statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf10bec-a01a-4a66-82d8-019c34a7e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate data collection statistics\n",
    "def calculate_collection_stats(\n",
    "    hd = handle_data, ud = user_data, td = tweet_data):\n",
    "    \n",
    "    ## define useful indices\n",
    "    congress_idx = ['USA House', 'USA Senate']\n",
    "    \n",
    "    ## calculate handle_data statistics\n",
    "    hd_stats = {\n",
    "        'Total': hd.shape[0],\n",
    "        'Congress': hd.group.value_counts().loc[congress_idx].sum(),\n",
    "        'Republican': hd.party.value_counts().loc['Republican']\n",
    "    }\n",
    "    \n",
    "    ## calculate user statistics\n",
    "    include_idx = ud.ml_set != 'exclude'\n",
    "    ud_stats = {\n",
    "        'Total': ud.loc[include_idx].shape[0],\n",
    "        'Congress': ud.group.loc[include_idx].value_counts().loc[\n",
    "            congress_idx].sum(),\n",
    "        'Republican': ud.party.loc[include_idx].value_counts().loc['Republican']\n",
    "    }\n",
    "    \n",
    "    ##calcualte tweet statistics\n",
    "    td_stats = {\n",
    "        'Total': td.shape[0],\n",
    "        'Congress': td.merge(ud, how = 'left', right_on = 'handle',\n",
    "            left_on = 'screen_name').group.value_counts().loc[congress_idx].sum(),\n",
    "        'Republican': td.merge(ud, how = 'left', right_on = 'handle',\n",
    "            left_on = 'screen_name').party.value_counts().loc['Republican'].sum()\n",
    "    }\n",
    "    \n",
    "    ## compile statistics\n",
    "    col_stats = {'Handles': hd_stats, 'Users': ud_stats, 'Tweets': td_stats}\n",
    "    \n",
    "    return pd.DataFrame(col_stats).T\n",
    "\n",
    "\n",
    "calculate_collection_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dda7dd-6857-4e1c-b7c3-fed3b080f2b0",
   "metadata": {},
   "source": [
    "#### PPTX12 - Model Performance (Slide 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e40cc-625a-4da9-94b9-615d09acbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = {\n",
    "    'AzureDark': '#021F33',\n",
    "    'BrownDark': '#804419',\n",
    "    'AzureFaded': '#466F91',\n",
    "    'BrownFaded': '#916846',\n",
    "    'AzureBg': '#CAD9E6',\n",
    "    'BrownBg': '#E6D6CA',\n",
    "    'AzureGrid': '#B4C1CC',\n",
    "    'Trans': '#11223300'\n",
    "    }\n",
    "cp['AzureBgTrans'] = cp['AzureBg'] + '40'\n",
    "\n",
    "## prepare data\n",
    "best_model_stats[('time', 'run_time')] = (best_model_stats[('time', 'run_time')\n",
    "        ]).fillna(min(best_model_stats[('time', 'run_time')].values))\n",
    "best_model_stats = best_model_stats.reset_index()\n",
    "\n",
    "## drop staged random guess model\n",
    "i = best_model_stats[('model','variant')] != 'stage'\n",
    "best_model_stats = best_model_stats.loc[i]\n",
    "\n",
    "## configure plotting parameters to represent different model\n",
    "best_model_stats[('label', 'model')] = best_model_stats[(\n",
    "    'model', 'algorithm')].replace({'bayes':'NB', 'forest':'RF',\n",
    "    'logistic':'LR', 'adaboost':'AB', 'random':'RG'})\n",
    "\n",
    "bm_col = {\n",
    "    'feature_unitary': cp['BrownDark'],\n",
    "    'feature_stage': cp['AzureDark'],\n",
    "    'pca_unitary': cp['BrownFaded'],\n",
    "    'pca_stage': cp['AzureFaded'],\n",
    "    'unitary': cp['BrownDark'],\n",
    "    'stage': cp['AzureDark'],\n",
    "}\n",
    "\n",
    "best_model_bbox = {\n",
    "    'feature_unitary': dict(boxstyle = 'round', linestyle = '-',\n",
    "            facecolor = cp['AzureBg'],\n",
    "            edgecolor = bm_col['feature_unitary'] ),\n",
    "    'feature_stage': dict(boxstyle = 'round', linestyle = '-',\n",
    "            facecolor = cp['AzureBg'],\n",
    "            edgecolor = bm_col['feature_stage']),\n",
    "    'pca_unitary': dict(boxstyle = 'round', linestyle = ':',\n",
    "            facecolor = cp['Trans'],\n",
    "            edgecolor = bm_col['pca_unitary']),\n",
    "    'pca_stage': dict(boxstyle = 'round', linestyle = ':',\n",
    "            facecolor = cp['Trans'],\n",
    "            edgecolor = bm_col['pca_stage']),\n",
    "    'unitary': dict(boxstyle = 'round', linestyle = '-',\n",
    "            facecolor = cp['Trans'],\n",
    "            edgecolor = bm_col['unitary']),\n",
    "    'stage': dict(boxstyle = 'round', linestyle = '-',\n",
    "            facecolor = cp['Trans'],\n",
    "                  edgecolor = bm_col['stage']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac287e-744b-43de-9ea7-307511a8eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate f1 x negative runtime performance plot\n",
    "plt.scatter(\n",
    "    x = best_model_stats[('time', 'run_time')],\n",
    "    y = best_model_stats[('perform', 'f1')],\n",
    "    c = '#FFFFFF00'\n",
    ")\n",
    "\n",
    "best_model = ['feature_unitary', 'unitary']\n",
    "\n",
    "## render non-best model variants\n",
    "for i in best_model_stats.index:\n",
    "    if best_model_stats.loc[i, ('model', 'variant')] not in best_model:\n",
    "        plt.text(\n",
    "            x = best_model_stats.loc[i, ('time', 'run_time')],\n",
    "            y = best_model_stats.loc[i, ('perform', 'f1')],\n",
    "            s = best_model_stats.loc[i, ('label', 'model')],\n",
    "            bbox = best_model_bbox[best_model_stats.loc[i, ('model', 'variant')]],\n",
    "            c = bm_col[best_model_stats.loc[i, ('model', 'variant')]]\n",
    "    )\n",
    "\n",
    "## render best model variant\n",
    "for i in best_model_stats.index:\n",
    "    if best_model_stats.loc[i, ('model', 'variant')] in best_model:\n",
    "        plt.text(\n",
    "            x = best_model_stats.loc[i, ('time', 'run_time')],\n",
    "            y = best_model_stats.loc[i, ('perform', 'f1')],\n",
    "            s = best_model_stats.loc[i, ('label', 'model')],\n",
    "            bbox = best_model_bbox[best_model_stats.loc[i, ('model', 'variant')]],\n",
    "            c = bm_col[best_model_stats.loc[i, ('model', 'variant')]]\n",
    "    )\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "        \n",
    "the_handles = [\n",
    "    mpatches.Patch(edgecolor =  bm_col['feature_unitary'],\n",
    "        label='One-Stage, Features', facecolor = cp['AzureBgTrans'], \n",
    "                   linestyle = '-'),\n",
    "    mpatches.Patch(edgecolor = bm_col['pca_unitary'], label='One-Stage, PCA',\n",
    "        facecolor = cp['AzureBgTrans'], linestyle = ':'),\n",
    "    mpatches.Patch(edgecolor = bm_col['feature_stage'],\n",
    "                   label='Two-Stage, Features',\n",
    "        facecolor = cp['AzureBgTrans'], linestyle = '-'),\n",
    "    mpatches.Patch(edgecolor=bm_col['pca_stage'], label='Two-Stage, PCA',\n",
    "        facecolor = cp['AzureBgTrans'], linestyle = ':')\n",
    "]\n",
    "\n",
    "plt.legend(handles = the_handles,\n",
    "           facecolor = cp['AzureBg'], edgecolor = '#00000000')\n",
    "\n",
    "\n",
    "def set_plot_parameters():\n",
    "    matplotlib.rcParams['figure.edgecolor'] = cp['AzureDark']\n",
    "    matplotlib.rcParams['axes.edgecolor'] = cp['AzureDark']\n",
    "    matplotlib.rcParams['axes.labelcolor'] = cp['AzureDark']\n",
    "    plt.gcf().set_size_inches(4.5, 4.5)\n",
    "    plt.gcf().set_facecolor(cp['AzureBg'])\n",
    "    plt.gca().set_facecolor(cp['AzureBg'])\n",
    "    plt.gca().tick_params(axis='x', colors = cp['AzureDark'])\n",
    "    plt.gca().tick_params(axis='y', colors = cp['AzureDark'])\n",
    "\n",
    "## plot parameters\n",
    "\n",
    "\n",
    "set_plot_parameters()\n",
    "plt.gca().set_xlabel('Seconds of runtime (logged) • Model is fast ---->')\n",
    "plt.gca().set_ylabel('F1 score on test dataset • Model is accurate ---->')\n",
    "plt.gca().set_xlim((0.90, 2**11))\n",
    "plt.gca().set_ylim(0.3, 1.005)\n",
    "plt.gca().set_xscale('log', base = 2)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.grid(color = cp['AzureGrid'])\n",
    "\n",
    "plt.gcf().savefig('C_Output/model_performance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff957e-5f3c-4c8e-8ec8-4c1779892453",
   "metadata": {},
   "source": [
    "#### PPTX11 - Hyperparameter Testing (Slide 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985def4d-b35f-4e6b-8705-5a75f922dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract scoring information from hyperparameter search objects\n",
    "def extract_test_scores(hparams, pca = False):\n",
    "    \n",
    "    ## extract scores\n",
    "    result = dict()\n",
    "    for i in hparams.keys():\n",
    "        if pca:\n",
    "            if i[0:3] == \"pca\":\n",
    "                result[i] = hparams[i].cv_results_[\"mean_test_score\"]\n",
    "        else:\n",
    "            if i[0:3] != \"pca\":\n",
    "                result[i] = hparams[i].cv_results_[\"mean_test_score\"]\n",
    "            \n",
    "    ## package and return\n",
    "    result = pd.DataFrame(result)\n",
    "    return result.values.reshape(-1, 1)\n",
    "\n",
    "def compile_test_scores(hparam_dict = {\n",
    "        'LR': logistic_hparams,\n",
    "        'NB': bayes_hparams,\n",
    "        'RF': forest_hparams,\n",
    "        'AB': adaboost_hparams,\n",
    "        }, get_pca = False):\n",
    "    result = dict()\n",
    "    max_length = 0\n",
    "    for i in hparam_dict.keys():\n",
    "        result[i] = pd.DataFrame(\n",
    "            extract_test_scores(hparam_dict[i], pca = get_pca))\n",
    "        result[i] = result[i].squeeze().sample(n = int(1e5), replace = True,\n",
    "                                     random_state = 4561)\n",
    "        result[i] = result[i].reset_index(drop = True)\n",
    "        \n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "## execute code\n",
    "hparams = compile_test_scores(get_pca = False)\n",
    "hparams_pca = compile_test_scores(get_pca = True)\n",
    "hparams = pd.concat([hparams, hparams_pca], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1794932-1172-4bd6-9062-1dc4ea616b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate boxplots to represent spread of accuracy scores given parameters\n",
    "\n",
    "set_plot_parameters()\n",
    "the_box = plt.boxplot(hparams, labels = hparams.columns, \n",
    "            patch_artist=True,\n",
    "            boxprops = dict(\n",
    "                edgecolor = cp[\"AzureDark\"], facecolor = cp['AzureBg']),\n",
    "            whiskerprops = dict(color = cp[\"AzureDark\"]),\n",
    "            capprops = dict(color = cp[\"AzureDark\"]),\n",
    "            flierprops = dict(color = cp[\"AzureDark\"]),\n",
    "            medianprops = dict(color = cp[\"AzureDark\"]),\n",
    "            showfliers = False,\n",
    "            whis = (10, 90)\n",
    "            \n",
    "            )\n",
    "plt.grid(color = cp['AzureGrid'])\n",
    "plt.gca().set_ylim(0.3, 1)\n",
    "plt.gca().set_ylabel('F1 score on test dataset • Model is accurate ---->')\n",
    "plt.gca().set_xlabel(loc = 'left',\n",
    "    xlabel = ' ' * 11 +'Features' + ' ' * 13 + 'Principal Components')\n",
    "plt.gcf().savefig('C_Output/hyperparameter_performance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84bf4b-bdff-46b8-a581-ffd2b17e7742",
   "metadata": {},
   "source": [
    "## FOOT - display objects as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151c784-afd9-46f5-b2b2-686e824ee90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9f027-0200-49e3-b8cb-ea9166c37f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9c3d2-f6ac-44ec-8e95-f8e18938094b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249bed5a-5b77-49ae-89dd-3eff836bf24e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749c6d2-236a-4f14-8203-ef3c59d68173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e3dc1-fd47-4eb1-831b-5292a26807e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3da47-c671-4e44-93d0-6055c8c59863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a420097-c340-403d-b1f7-9ef21921482f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb64fbd-9ac2-4efa-92a5-61ed0bd82d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5ae31-e439-4f11-bad0-baa9819c0e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
